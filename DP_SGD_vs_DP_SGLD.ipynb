{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DP_SGD_vs_DP_SGLD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/littlekii/DPBBP/blob/main/DP_SGD_vs_DP_SGLD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "id": "7igpx9-cMguz",
        "outputId": "ccb24aa7-94a6-49d5-d706-c788a3f3408b"
      },
      "source": [
        "!pip install opacus\n",
        "!git clone https://github.com/JavierAntoran/Bayesian-Neural-Networks.git\n",
        "!git clone https://github.com/knamdar/data.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opacus\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/c4/36f07b54413e607f07858d5b668379991948f0de9979185a895fee9c2208/opacus-0.13.0-py3-none-any.whl (102kB)\n",
            "\r\u001b[K     |███▏                            | 10kB 17.3MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 20kB 18.9MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 30kB 21.9MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 40kB 24.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 51kB 24.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 61kB 25.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 71kB 24.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 81kB 23.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 92kB 24.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.7/dist-packages (from opacus) (0.9.1+cu101)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from opacus) (1.19.5)\n",
            "Collecting requests>=2.25.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl (61kB)\n",
            "\r\u001b[K     |█████▍                          | 10kB 26.9MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 20kB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 30kB 35.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 40kB 37.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 51kB 38.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from opacus) (1.8.1+cu101)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.7/dist-packages (from opacus) (1.4.1)\n",
            "Requirement already satisfied: tqdm>=4.40 in /usr/local/lib/python3.7/dist-packages (from opacus) (4.41.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.9->opacus) (7.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->opacus) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->opacus) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->opacus) (2020.12.5)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->opacus) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3->opacus) (3.7.4.3)\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: requests, opacus\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "Successfully installed opacus-0.13.0 requests-2.25.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "requests"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Bayesian-Neural-Networks'...\n",
            "remote: Enumerating objects: 369, done.\u001b[K\n",
            "remote: Total 369 (delta 0), reused 0 (delta 0), pack-reused 369\u001b[K\n",
            "Receiving objects: 100% (369/369), 15.98 MiB | 36.52 MiB/s, done.\n",
            "Resolving deltas: 100% (191/191), done.\n",
            "Cloning into 'data'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 16 (delta 2), reused 16 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (16/16), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoT8O9JdTvGX"
      },
      "source": [
        "%matplotlib inline\n",
        "from __future__ import print_function, division\n",
        "import numpy as np\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "import torch.utils.data as data\n",
        "from torch.optim import Optimizer\n",
        "from opacus import PrivacyEngine\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import collections\n",
        "import h5py, sys\n",
        "import gzip\n",
        "import os\n",
        "import math\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zssOjs3sTyQ6"
      },
      "source": [
        "def mkdir(paths):\n",
        "    if not isinstance(paths, (list, tuple)):\n",
        "        paths = [paths]\n",
        "    for path in paths:\n",
        "        if not os.path.isdir(path):\n",
        "            os.makedirs(path)\n",
        "\n",
        "\n",
        "suffixes = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']\n",
        "def humansize(nbytes):\n",
        "    i = 0\n",
        "    while nbytes >= 1024 and i < len(suffixes)-1:\n",
        "        nbytes /= 1024.\n",
        "        i += 1\n",
        "    f = ('%.2f' % nbytes)\n",
        "    return '%s%s' % (f, suffixes[i])\n",
        "\n",
        "def to_variable(var=(), cuda=True, volatile=False):\n",
        "    out = []\n",
        "    for v in var:\n",
        "        if isinstance(v, np.ndarray):\n",
        "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
        "\n",
        "        if not v.is_cuda and cuda:\n",
        "            v = v.cuda()\n",
        "\n",
        "        if not isinstance(v, Variable):\n",
        "            v = Variable(v, volatile=volatile)\n",
        "\n",
        "        out.append(v)\n",
        "    return out\n",
        "  \n",
        "def cprint(color, text, **kwargs):\n",
        "    if color[0] == '*':\n",
        "        pre_code = '1;'\n",
        "        color = color[1:]\n",
        "    else:\n",
        "        pre_code = ''\n",
        "    code = {\n",
        "        'a': '30',\n",
        "        'r': '31',\n",
        "        'g': '32',\n",
        "        'y': '33',\n",
        "        'b': '34',\n",
        "        'p': '35',\n",
        "        'c': '36',\n",
        "        'w': '37'\n",
        "    }\n",
        "    print(\"\\x1b[%s%sm%s\\x1b[0m\" % (pre_code, code[color], text), **kwargs)\n",
        "    sys.stdout.flush()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0JWDj67T1hp"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "class BaseNet(object):\n",
        "    def __init__(self):\n",
        "        cprint('c', '\\nNet:')\n",
        "\n",
        "    def get_nb_parameters(self):\n",
        "        return np.sum(p.numel() for p in self.model.parameters())\n",
        "\n",
        "    def set_mode_train(self, train=True):\n",
        "        if train:\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "\n",
        "    def update_lr(self, epoch, gamma=0.99):\n",
        "        self.epoch += 1\n",
        "        if self.schedule is not None:\n",
        "            if len(self.schedule) == 0 or epoch in self.schedule:\n",
        "                self.lr *= gamma\n",
        "                print('learning rate: %f  (%d)\\n' % self.lr, epoch)\n",
        "                for param_group in self.optimizer.param_groups:\n",
        "                    param_group['lr'] = self.lr\n",
        "\n",
        "    def save(self, filename):\n",
        "        cprint('c', 'Writting %s\\n' % filename)\n",
        "        torch.save({\n",
        "            'epoch': self.epoch,\n",
        "            'lr': self.lr,\n",
        "            'model': self.model,\n",
        "            'optimizer': self.optimizer}, filename)\n",
        "\n",
        "    def load(self, filename):\n",
        "        cprint('c', 'Reading %s\\n' % filename)\n",
        "        state_dict = torch.load(filename)\n",
        "        self.epoch = state_dict['epoch']\n",
        "        self.lr = state_dict['lr']\n",
        "        self.model = state_dict['model']\n",
        "        self.optimizer = state_dict['optimizer']\n",
        "        print('  restoring epoch: %d, lr: %f' % (self.epoch, self.lr))\n",
        "        return self.epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW96VvcdU-v2"
      },
      "source": [
        "from torch.optim.optimizer import Optimizer, required\n",
        "class SGLD(Optimizer):\n",
        "    \"\"\"\n",
        "    SGLD optimiser based on pytorch's SGD. \n",
        "    Note that the weight decay is specified in terms of the gaussian prior sigma\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=required, norm_sigma=0,laplace_b=0, addnoise=True):\n",
        "        \n",
        "        if lr is not required and lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        \n",
        "        defaults = dict(lr=lr, norm_sigma=norm_sigma, laplace_b=laplace_b,addnoise=addnoise)\n",
        "        \n",
        "        super(SGLD, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self,closure=0):\n",
        "        \"\"\"\n",
        "        Performs a single optimization step.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        \n",
        "        for group in self.param_groups:\n",
        "\n",
        "            norm_sigma = group['norm_sigma']\n",
        "            laplace_b=group['laplace_b']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad.data\n",
        "                if norm_sigma != 0:\n",
        "                    p.data.add_(-group['lr']/norm_sigma**2, p.data)\n",
        "                if laplace_b != 0:\n",
        "                    p.data.add_(-group['lr']/laplace_b,torch.sign(p.data))\n",
        "\n",
        "                if group['addnoise']:  \n",
        "                    langevin_noise = p.data.new(p.data.size()).normal_(mean=0, std=1)/np.sqrt(group['lr'])\n",
        "                    p.data.add_(-group['lr'], 0.5*d_p + langevin_noise)\n",
        "                else:\n",
        "                    p.data.add_(-group['lr'], 0.5*d_p)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkGTR1neT5rZ"
      },
      "source": [
        "class SampleConvNet_MNIST(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 8, 2, padding=3)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 4, 2)\n",
        "        self.fc1 = nn.Linear(32 * 4 * 4, 32)\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x of shape [B, 1, 28, 28]\n",
        "        x = F.relu(self.conv1(x))  # -> [B, 16, 14, 14]\n",
        "        x = F.max_pool2d(x, 2, 1)  # -> [B, 16, 13, 13]\n",
        "        x = F.relu(self.conv2(x))  # -> [B, 32, 5, 5]\n",
        "        x = F.max_pool2d(x, 2, 1)  # -> [B, 32, 4, 4]\n",
        "        x = x.view(-1, 32 * 4 * 4)  # -> [B, 512]\n",
        "        x = F.relu(self.fc1(x))  # -> [B, 32]\n",
        "        x = self.fc2(x)  # -> [B, 10]\n",
        "        return x\n",
        "\n",
        "    def name(self):\n",
        "        return \"SampleConvNet\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITcTp1glT8nf"
      },
      "source": [
        "class Net_langevin(BaseNet):\n",
        "    eps = 1e-6\n",
        "\n",
        "    def __init__(self, lr=1e-3, cuda=True, classes=10, N_train=60000, norm_sigma=0,laplace_b=0,DP_or_Not=True,which_net=None,SGD_or_Not=False):\n",
        "        super(Net_langevin, self).__init__()\n",
        "        cprint('y', ' Creating Net!! ')\n",
        "        self.lr = lr\n",
        "        self.schedule = None  # [] #[50,200,400,600]\n",
        "        self.cuda = cuda\n",
        "        self.norm_sigma = norm_sigma\n",
        "        self.laplace_b=laplace_b\n",
        "        self.classes = classes\n",
        "        self.N_train = N_train\n",
        "\n",
        "        self.which_net=which_net\n",
        "        self.create_net()\n",
        "        self.SGD_or_Not=SGD_or_Not\n",
        "        self.create_opt()\n",
        "        self.epoch = 0\n",
        "        self.weight_set_samples = []\n",
        "\n",
        "        self.test=False\n",
        "        self.DP_or_Not=DP_or_Not\n",
        "\n",
        "    def create_net(self):\n",
        "        torch.manual_seed(42)\n",
        "        if self.cuda:\n",
        "            torch.cuda.manual_seed(42)\n",
        "\n",
        "        self.model = self.which_net\n",
        "        if self.cuda:\n",
        "            self.model.cuda()\n",
        "        print('    Total params: %.2fM' % (self.get_nb_parameters() / 1000000.0))\n",
        "    \n",
        "    def create_opt(self):\n",
        "        if self.SGD_or_Not:\n",
        "            self.optimizer = optim.SGD(params=self.model.parameters(), lr=self.lr)\n",
        "            if DP_or_Not:\n",
        "                privacy_engine = PrivacyEngine(\n",
        "                        self.model,\n",
        "                        sample_rate=batch_size/self.N_train,\n",
        "                        alphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64)),\n",
        "                        noise_multiplier=noise_multiplier,\n",
        "                        max_grad_norm=clipping_norm)\n",
        "                privacy_engine.attach(self.optimizer)\n",
        "\n",
        "        else:\n",
        "            self.optimizer = SGLD(params=self.model.parameters(), lr=self.lr, norm_sigma=self.norm_sigma,laplace_b=self.laplace_b, addnoise=1-DP_or_Not)\n",
        "            if DP_or_Not:\n",
        "                privacy_engine = PrivacyEngine(\n",
        "                        self.model,\n",
        "                        sample_rate=batch_size/self.N_train,\n",
        "                        alphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64)),\n",
        "                        noise_multiplier=batch_size/np.sqrt(self.lr)/self.N_train/clipping_norm,\n",
        "                        max_grad_norm=clipping_norm*self.N_train)\n",
        "                privacy_engine.attach(self.optimizer)\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        x, y = x.to(device), y.long().to(device)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        out = self.model(x)\n",
        "        loss = F.cross_entropy(out, y, reduction='mean') # We use mean because we treat as an estimation of whole dataset\n",
        "        \n",
        "        if not self.SGD_or_Not:\n",
        "           loss = loss * self.N_train \n",
        "            \n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # out: (batch_size, out_channels, out_caps_dims)\n",
        "        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
        "        err = pred.ne(y.data).sum()\n",
        "\n",
        "        return loss.data*x.shape[0]/self.N_train, err\n",
        "\n",
        "\n",
        "    def eval(self, x, y, train=False):\n",
        "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
        "\n",
        "        out = self.model(x)\n",
        "\n",
        "        loss = F.cross_entropy(out, y, reduction='sum')\n",
        "\n",
        "        probs = F.softmax(out, dim=1).data.cpu()\n",
        "\n",
        "        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
        "        err = pred.ne(y.data).sum()\n",
        "\n",
        "        return loss.data, err, probs\n",
        "    \n",
        "    def save_sampled_net(self, max_samples):\n",
        "        \n",
        "        if len(self.weight_set_samples) >= max_samples:\n",
        "            self.weight_set_samples.pop(0)\n",
        "            \n",
        "        self.weight_set_samples.append(copy.deepcopy(self.model.state_dict()))\n",
        "        \n",
        "        cprint('c', ' saving weight samples %d/%d' % (len(self.weight_set_samples), max_samples) )\n",
        "        \n",
        "        return None\n",
        "        \n",
        "    def sample_eval(self, x, y, Nsamples=0, logits=True, train=False):\n",
        "        if Nsamples == 0:\n",
        "            Nsamples = len(self.weight_set_samples)\n",
        "            \n",
        "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
        "        \n",
        "        out = x.data.new(Nsamples, x.shape[0], self.classes)\n",
        "        \n",
        "        # iterate over all saved weight configuration samples\n",
        "        for idx, weight_dict in enumerate(self.weight_set_samples):\n",
        "            if idx == Nsamples:\n",
        "                break\n",
        "            self.model.load_state_dict(weight_dict)\n",
        "            out[idx] = self.model(x)\n",
        "        \n",
        "        \n",
        "        if logits:\n",
        "            mean_out = out.mean(dim=0, keepdim=False)\n",
        "            loss = F.cross_entropy(mean_out, y, reduction='sum')\n",
        "            probs = F.softmax(mean_out, dim=1).data.cpu()\n",
        "            \n",
        "        else:\n",
        "            mean_out =  F.softmax(out, dim=2).mean(dim=0, keepdim=False)\n",
        "            probs = mean_out.data.cpu()\n",
        "            \n",
        "            log_mean_probs_out = torch.log(mean_out)\n",
        "            loss = F.nll_loss(log_mean_probs_out, y, reduction='sum')\n",
        "\n",
        "        pred = mean_out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
        "        err = pred.ne(y.data).sum()\n",
        "\n",
        "        return loss.data, err, probs\n",
        "    \n",
        "    def all_sample_eval(self, x, y, Nsamples):\n",
        "        if Nsamples == 0:\n",
        "            Nsamples = len(self.weight_set_samples)\n",
        "            \n",
        "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
        "        \n",
        "        out = x.data.new(Nsamples, x.shape[0], self.classes)\n",
        "        \n",
        "        # iterate over all saved weight configuration samples\n",
        "        for idx, weight_dict in enumerate(self.weight_set_samples):\n",
        "            if idx == Nsamples:\n",
        "                break\n",
        "            self.model.load_state_dict(weight_dict)\n",
        "            out[idx] = self.model(x)\n",
        "        \n",
        "        prob_out =  F.softmax(out, dim=2)\n",
        "        prob_out = prob_out.data\n",
        "\n",
        "        return prob_out\n",
        "    \n",
        "    def get_weight_samples(self, Nsamples=0):\n",
        "        weight_vec = []\n",
        "        \n",
        "        if Nsamples == 0 or Nsamples > len(self.weight_set_samples):\n",
        "            Nsamples = len(self.weight_set_samples)\n",
        "            \n",
        "        for idx, state_dict in enumerate(self.weight_set_samples):\n",
        "            if idx == Nsamples:\n",
        "                break\n",
        "                \n",
        "            for key in state_dict.keys():\n",
        "                if 'weight' in key:\n",
        "                    weight_mtx = state_dict[key].cpu().data\n",
        "                    for weight in weight_mtx.view(-1):\n",
        "                        weight_vec.append(weight)\n",
        "            \n",
        "        return np.array(weight_vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCzvF8TpWKIp"
      },
      "source": [
        "import time\n",
        "import torch.utils.data\n",
        "from torchvision import transforms, datasets\n",
        "import matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjU4ge-Pfs-W"
      },
      "source": [
        "## DP-SGD with different noise multiplier and clipping norm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vFkp9CC-VTDD",
        "outputId": "72f198e3-ff66-4ec5-baa0-7b05ab5cd1eb"
      },
      "source": [
        "models_dir = 'models_SGLD_MNIST_gaussian'\n",
        "results_dir = 'result_SGD_MNIST'\n",
        "\n",
        "mkdir(models_dir)\n",
        "mkdir(results_dir)\n",
        "\n",
        "save_dicts = []\n",
        "# ------------------------------------------------------------------------------------------------------\n",
        "# train config\n",
        "NTrainPointsMNIST = 60000\n",
        "batch_size = 256\n",
        "nb_epochs = 15 # We can do less iterations as this method has faster convergence\n",
        "log_interval = 1\n",
        "\n",
        "lr = 0.25\n",
        "norm_sigma =0.1 \n",
        "laplace_b=0\n",
        "#noise_multiplier = 1.3\n",
        "#clipping_norm=1.5\n",
        "DP_or_Not=True\n",
        "# ------------------------------------------------------------------------------------------------------\n",
        "# dataset\n",
        "cprint('c', '\\nData:')\n",
        "\n",
        " \n",
        "\n",
        "# load data\n",
        "\n",
        "# data augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
        "])\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "trainset = datasets.MNIST(root='/content/data', train=True, download=True, transform=transform_train)\n",
        "valset = datasets.MNIST(root='/content/data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "if use_cuda:\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=3)\n",
        "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=3)\n",
        "\n",
        "else:\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n",
        "                                              num_workers=3)\n",
        "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
        "                                            num_workers=3)\n",
        "\n",
        "\n",
        "## ---------------------------------------------------------------------------------------------------------------------\n",
        "# net dims\n",
        "cprint('c', '\\nNetwork:')\n",
        "\n",
        "\n",
        "########################################################################################\n",
        "#net = Net_langevin(lr=lr, cuda=use_cuda, classes=10,N_train=NTrainPointsMNIST, norm_sigma=norm_sigma,laplace_b=laplace_b,which_net=SampleConvNet_MNIST(),SGD_or_Not=True, noise_multiplier = noise_multiplier)\n",
        "\n",
        "## weight saving parameters #######\n",
        "start_save = 1\n",
        "save_every = 1 # We want less correlated samples -> despite having per minibatch noise we see correlations\n",
        "N_saves = 15\n",
        "###################################\n",
        "\n",
        "\n",
        "## ---------------------------------------------------------------------------------------------------------------------\n",
        "# train\n",
        "cprint('c', '\\nTrain:')\n",
        "\n",
        "print('  init cost variables:')\n",
        "pred_cost_train = np.zeros(nb_epochs)\n",
        "err_train = np.zeros(nb_epochs)\n",
        "\n",
        "cost_dev = np.zeros(nb_epochs)\n",
        "err_dev = np.zeros(nb_epochs)\n",
        "# best_cost = np.inf\n",
        "best_err = np.inf\n",
        "\n",
        "\n",
        "nb_its_dev = 1\n",
        "\n",
        "tic0 = time.time()\n",
        "from tqdm import tqdm\n",
        "\n",
        "err_SGD_list = []\n",
        "\n",
        "for noise_multiplier in [0.5,0.9,1.3,2,3]:\n",
        "    for clipping_norm in [0.5,1,1.5,2,5]:\n",
        "        net = Net_langevin(lr=lr, cuda=use_cuda, classes=10,N_train=NTrainPointsMNIST, norm_sigma=norm_sigma,laplace_b=laplace_b,which_net=SampleConvNet_MNIST(),SGD_or_Not=True)\n",
        "        for i in range(nb_epochs):\n",
        "            \n",
        "        #     if i in [1]:\n",
        "        #         print('updating lr')\n",
        "        #         net.sched.step()\n",
        "            \n",
        "            net.set_mode_train(True)\n",
        "\n",
        "            tic = time.time()\n",
        "            nb_samples = 0\n",
        "\n",
        "            for x, y in tqdm(trainloader):\n",
        "                cost_pred, err = net.fit(x, y)\n",
        "                err_train[i] += err\n",
        "                pred_cost_train[i] += torch.mean(cost_pred)\n",
        "                nb_samples += len(x)\n",
        "\n",
        "            pred_cost_train[i] /= nb_samples\n",
        "            err_train[i] /= nb_samples\n",
        "\n",
        "            toc = time.time()\n",
        "            net.epoch = i\n",
        "            # ---- print\n",
        "            print(\"it %d/%d, Jtr_pred = %f, err = %f, \" % (i, nb_epochs, pred_cost_train[i], err_train[i]), end=\"\")\n",
        "            cprint('r', '   time: %f seconds\\n' % (toc - tic))\n",
        "            \n",
        "            # ---- save weights\n",
        "            if i >= start_save and i % save_every == 0:\n",
        "                net.save_sampled_net(max_samples=N_saves)\n",
        "                save_dicts.append(copy.deepcopy(net.model.state_dict()))\n",
        "            # ---- dev\n",
        "            if i % nb_its_dev == 0:\n",
        "                net.set_mode_train(False)\n",
        "                nb_samples = 0\n",
        "                for j, (x, y) in enumerate(valloader):\n",
        "\n",
        "                    cost, err, probs = net.eval(x, y)\n",
        "\n",
        "                    cost_dev[i] += cost\n",
        "                    err_dev[i] += err\n",
        "                    nb_samples += len(x)\n",
        "\n",
        "                cost_dev[i] /= nb_samples\n",
        "                err_dev[i] /= nb_samples\n",
        "\n",
        "                cprint('g', '    Jdev = %f, err = %f\\n' % (cost_dev[i], err_dev[i]))\n",
        "                err_SGD_list.append(err_dev[i])\n",
        "                if err_dev[i] < best_err:\n",
        "                    best_err = err_dev[i]\n",
        "                    cprint('b', 'best test error')\n",
        "                    net.save(models_dir+'/theta_best.dat')\n",
        "        \n",
        "        print('current noise multiplier is ',noise_multiplier, 'current clipping norm is ',clipping_norm)\n",
        "        print(err_SGD_list)\n",
        "\n",
        "toc0 = time.time()\n",
        "runtime_per_it = (toc0 - tic0) / float(nb_epochs)\n",
        "cprint('r', '   average time: %f seconds\\n' % runtime_per_it)\n",
        "\n",
        "net.save(models_dir+'/theta_last.dat')\n",
        "\n",
        "## ---------------------------------------------------------------------------------------------------------------------\n",
        "# results\n",
        "cprint('c', '\\nRESULTS:')\n",
        "nb_parameters = net.get_nb_parameters()\n",
        "best_cost_dev = np.min(cost_dev)\n",
        "best_cost_train = np.min(pred_cost_train)\n",
        "err_dev_min = err_dev[::nb_its_dev].min()\n",
        "\n",
        "print('  cost_dev: %f (cost_train %f)' % (best_cost_dev, best_cost_train))\n",
        "print('  err_dev: %f' % (err_dev_min))\n",
        "print('  nb_parameters: %d (%s)' % (nb_parameters, humansize(nb_parameters)))\n",
        "print('  time_per_it: %fs\\n' % (runtime_per_it))\n",
        "\n",
        "\n",
        "\n",
        "## Save results for plots\n",
        "#np.save('results/test_predictions.npy', test_predictions)\n",
        "np.save(results_dir + '/cost_train.npy', pred_cost_train)\n",
        "np.save(results_dir + '/cost_dev.npy', cost_dev)\n",
        "np.save(results_dir + '/err_train.npy', err_train)\n",
        "np.save(results_dir + '/err_dev.npy', err_dev)\n",
        "np.save(results_dir + '/save_dicts.npy',save_dicts )\n",
        "np.save(results_dir + '/err_list_SGD.npy',err_SGD_list )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[36m\n",
            "Data:\u001b[0m\n",
            "\u001b[36m\n",
            "Network:\u001b[0m\n",
            "\u001b[36m\n",
            "Train:\u001b[0m\n",
            "  init cost variables:\n",
            "\u001b[36m\n",
            "Net:\u001b[0m\n",
            "\u001b[33m Creating Net!! \u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:523: UserWarning: A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            "  \"A ``sample_rate`` has been provided.\"\n",
            "/usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:195: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            "  \"Secure RNG turned off. This is perfectly fine for experimentation as it allows \"\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "    Total params: 0.03M\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "100%|██████████| 1/1 [00:10<00:00, 10.19s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 0/15, Jtr_pred = 0.000039, err = 0.901400, \u001b[31m   time: 10.192263 seconds\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 2.307876, err = 0.904200\n",
            "\u001b[0m\n",
            "\u001b[34mbest test error\u001b[0m\n",
            "\u001b[36mWritting models_SGLD_MNIST_gaussian/theta_best.dat\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:09<00:00,  9.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 1/15, Jtr_pred = 0.000038, err = 0.901400, \u001b[31m   time: 9.903114 seconds\n",
            "\u001b[0m\n",
            "\u001b[36m saving weight samples 1/15\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 2.302990, err = 0.904200\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:09<00:00,  9.78s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 2/15, Jtr_pred = 0.000038, err = 0.901400, \u001b[31m   time: 9.781692 seconds\n",
            "\u001b[0m\n",
            "\u001b[36m saving weight samples 2/15\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 2.298546, err = 0.904200\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:09<00:00,  9.96s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 3/15, Jtr_pred = 0.000038, err = 0.901383, \u001b[31m   time: 9.963508 seconds\n",
            "\u001b[0m\n",
            "\u001b[36m saving weight samples 3/15\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 2.294589, err = 0.904100\n",
            "\u001b[0m\n",
            "\u001b[34mbest test error\u001b[0m\n",
            "\u001b[36mWritting models_SGLD_MNIST_gaussian/theta_best.dat\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:09<00:00,  9.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 4/15, Jtr_pred = 0.000038, err = 0.901133, \u001b[31m   time: 9.854241 seconds\n",
            "\u001b[0m\n",
            "\u001b[36m saving weight samples 4/15\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 2.290977, err = 0.902600\n",
            "\u001b[0m\n",
            "\u001b[34mbest test error\u001b[0m\n",
            "\u001b[36mWritting models_SGLD_MNIST_gaussian/theta_best.dat\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:09<00:00,  9.75s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 5/15, Jtr_pred = 0.000038, err = 0.900350, \u001b[31m   time: 9.758374 seconds\n",
            "\u001b[0m\n",
            "\u001b[36m saving weight samples 5/15\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 2.287720, err = 0.899600\n",
            "\u001b[0m\n",
            "\u001b[34mbest test error\u001b[0m\n",
            "\u001b[36mWritting models_SGLD_MNIST_gaussian/theta_best.dat\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-5f7da812c014>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mnb_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                 \u001b[0mcost_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0merr_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1139\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q90TWQOtpik-"
      },
      "source": [
        "print(err_SGD_list)\n",
        "np.save('/content/noise_clip.npy', err_SGD_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVWGwKBKghfi"
      },
      "source": [
        "## DP-SGLD with different noise multiplier and clipping norm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhF7XRgKVfCm"
      },
      "source": [
        "models_dir = 'models_SGLD_MNIST_gaussian'\n",
        "results_dir = 'result_SGLD_MNIST'\n",
        "\n",
        "mkdir(models_dir)\n",
        "mkdir(results_dir)\n",
        "\n",
        "save_dicts = []\n",
        "# ------------------------------------------------------------------------------------------------------\n",
        "# train config\n",
        "NTrainPointsMNIST = 60000\n",
        "batch_size = 256\n",
        "nb_epochs = 15 # We can do less iterations as this method has faster convergence\n",
        "log_interval = 1\n",
        "\n",
        "lr = 1e-5\n",
        "norm_sigma =0.1 \n",
        "laplace_b=0\n",
        "#noise_multiplier = 1.3\n",
        "#clipping_norm=1.5\n",
        "DP_or_Not=True\n",
        "# ------------------------------------------------------------------------------------------------------\n",
        "# dataset\n",
        "cprint('c', '\\nData:')\n",
        "\n",
        " \n",
        "\n",
        "# load data\n",
        "\n",
        "# data augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
        "])\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "trainset = datasets.MNIST(root='/content/data', train=True, download=True, transform=transform_train)\n",
        "valset = datasets.MNIST(root='/content/data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "if use_cuda:\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=3)\n",
        "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=3)\n",
        "\n",
        "else:\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n",
        "                                              num_workers=3)\n",
        "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
        "                                            num_workers=3)\n",
        "\n",
        "\n",
        "## ---------------------------------------------------------------------------------------------------------------------\n",
        "# net dims\n",
        "cprint('c', '\\nNetwork:')\n",
        "\n",
        "\n",
        "########################################################################################\n",
        "#net = Net_langevin(lr=lr, cuda=use_cuda, classes=10,N_train=NTrainPointsMNIST, norm_sigma=norm_sigma,laplace_b=laplace_b,which_net=SampleConvNet_MNIST(),SGD_or_Not=False)\n",
        "\n",
        "## weight saving parameters #######\n",
        "start_save = 1\n",
        "save_every = 1 # We want less correlated samples -> despite having per minibatch noise we see correlations\n",
        "N_saves = 15\n",
        "###################################\n",
        "\n",
        "\n",
        "## ---------------------------------------------------------------------------------------------------------------------\n",
        "# train\n",
        "cprint('c', '\\nTrain:')\n",
        "\n",
        "print('  init cost variables:')\n",
        "pred_cost_train = np.zeros(nb_epochs)\n",
        "err_train = np.zeros(nb_epochs)\n",
        "\n",
        "cost_dev = np.zeros(nb_epochs)\n",
        "err_dev = np.zeros(nb_epochs)\n",
        "# best_cost = np.inf\n",
        "best_err = np.inf\n",
        "\n",
        "\n",
        "nb_its_dev = 1\n",
        "\n",
        "tic0 = time.time()\n",
        "from tqdm import tqdm\n",
        "\n",
        "err_SGLD_list = []\n",
        "\n",
        "for clipping_norm in [0.5,1,1.5,2,5]:\n",
        "    net = Net_langevin(lr=lr, cuda=use_cuda, classes=10,N_train=NTrainPointsMNIST, norm_sigma=norm_sigma,laplace_b=laplace_b,which_net=SampleConvNet_MNIST(),SGD_or_Not=False)\n",
        "    for i in range(nb_epochs):\n",
        "        \n",
        "    #     if i in [1]:\n",
        "    #         print('updating lr')\n",
        "    #         net.sched.step()\n",
        "        \n",
        "        net.set_mode_train(True)\n",
        "\n",
        "        tic = time.time()\n",
        "        nb_samples = 0\n",
        "\n",
        "        for x, y in tqdm(trainloader):\n",
        "            cost_pred, err = net.fit(x, y)\n",
        "            err_train[i] += err\n",
        "            pred_cost_train[i] += torch.mean(cost_pred)\n",
        "            nb_samples += len(x)\n",
        "\n",
        "        pred_cost_train[i] /= nb_samples\n",
        "        err_train[i] /= nb_samples\n",
        "\n",
        "        toc = time.time()\n",
        "        net.epoch = i\n",
        "        # ---- print\n",
        "        print(\"it %d/%d, Jtr_pred = %f, err = %f, \" % (i, nb_epochs, pred_cost_train[i], err_train[i]), end=\"\")\n",
        "        cprint('r', '   time: %f seconds\\n' % (toc - tic))\n",
        "        \n",
        "        # ---- save weights\n",
        "        if i >= start_save and i % save_every == 0:\n",
        "            net.save_sampled_net(max_samples=N_saves)\n",
        "            save_dicts.append(copy.deepcopy(net.model.state_dict()))\n",
        "        # ---- dev\n",
        "        if i % nb_its_dev == 0:\n",
        "            net.set_mode_train(False)\n",
        "            nb_samples = 0\n",
        "            for j, (x, y) in enumerate(valloader):\n",
        "\n",
        "                cost, err, probs = net.eval(x, y)\n",
        "\n",
        "                cost_dev[i] += cost\n",
        "                err_dev[i] += err\n",
        "                nb_samples += len(x)\n",
        "\n",
        "            cost_dev[i] /= nb_samples\n",
        "            err_dev[i] /= nb_samples\n",
        "\n",
        "            cprint('g', '    Jdev = %f, err = %f\\n' % (cost_dev[i], err_dev[i]))\n",
        "            err_SGLD_list.append(err_dev[i])\n",
        "            if err_dev[i] < best_err:\n",
        "                best_err = err_dev[i]\n",
        "                cprint('b', 'best test error')\n",
        "                net.save(models_dir+'/theta_best.dat')\n",
        "    \n",
        "    print('current clipping norm is ',clipping_norm)\n",
        "    print(err_SGLD_list)\n",
        "\n",
        "toc0 = time.time()\n",
        "runtime_per_it = (toc0 - tic0) / float(nb_epochs)\n",
        "cprint('r', '   average time: %f seconds\\n' % runtime_per_it)\n",
        "\n",
        "net.save(models_dir+'/theta_last.dat')\n",
        "\n",
        "## ---------------------------------------------------------------------------------------------------------------------\n",
        "# results\n",
        "cprint('c', '\\nRESULTS:')\n",
        "nb_parameters = net.get_nb_parameters()\n",
        "best_cost_dev = np.min(cost_dev)\n",
        "best_cost_train = np.min(pred_cost_train)\n",
        "err_dev_min = err_dev[::nb_its_dev].min()\n",
        "\n",
        "print('  cost_dev: %f (cost_train %f)' % (best_cost_dev, best_cost_train))\n",
        "print('  err_dev: %f' % (err_dev_min))\n",
        "print('  nb_parameters: %d (%s)' % (nb_parameters, humansize(nb_parameters)))\n",
        "print('  time_per_it: %fs\\n' % (runtime_per_it))\n",
        "\n",
        "\n",
        "\n",
        "## Save results for plots\n",
        "#np.save('results/test_predictions.npy', test_predictions)\n",
        "np.save(results_dir + '/cost_train.npy', pred_cost_train)\n",
        "np.save(results_dir + '/cost_dev.npy', cost_dev)\n",
        "np.save(results_dir + '/err_train.npy', err_train)\n",
        "np.save(results_dir + '/err_dev.npy', err_dev)\n",
        "np.save(results_dir + '/save_dicts.npy',save_dicts )\n",
        "np.save(results_dir + '/err_SGLD_list.npy',err_SGLD_list )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eazJkRH-pot_"
      },
      "source": [
        "print(err_SGLD_list)\n",
        "\n",
        "np.save('/content/sgld_noise_clip.npy', err_SGD_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJLLzIGV0PYp"
      },
      "source": [
        "## Compute epsilon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PtdYWdfP4_c"
      },
      "source": [
        "from numpy import array\n",
        "err_SGD_list = [0.2322, 0.1485, 0.1161, 0.1003, 0.092, 0.0857, 0.0832, 0.076, 0.0731, 0.0691, 0.0677, 0.0641, 0.0631, 0.0611, 0.0606, 0.2473232177734375, 0.14261484375, 0.13071160888671876, 0.09811002807617188, 0.08230919799804688, 0.0737085693359375, 0.07210831909179688, 0.059107598876953125, 0.05160731201171875, 0.0493069091796875, 0.04930676879882812, 0.04620640869140625, 0.04180631103515625, 0.04200610961914063, 0.040506060791015625, 0.2214247314453125, 0.1012142578125, 0.08401307373046875, 0.064809814453125, 0.0552082275390625, 0.053007373046875, 0.05170721435546875, 0.043205911254882816, 0.04110516052246094, 0.039604931640625, 0.046104931640625, 0.03990462036132812, 0.034204180908203125, 0.03430419921875, 0.03160404968261719, 0.1661221435546875, 0.07831011962890624, 0.0581083984375, 0.05060648193359375, 0.04720552062988281, 0.03970530090332031, 0.039705169677734374, 0.0361043212890625, 0.03340411071777344, 0.03220396118164062, 0.03540461120605469, 0.03320399169921875, 0.03170342102050781, 0.03210343017578125, 0.03340316162109375, 0.06201661376953125, 0.045107830810546874, 0.038605810546875, 0.04030505981445313, 0.03680472106933594, 0.039003970336914065, 0.03960397033691406, 0.03440361022949219, 0.04070333862304688, 0.03680321960449219, 0.0386035400390625, 0.0411033203125, 0.03730317077636719, 0.04120321044921875, 0.042103338623046875, 0.318806201171875, 0.2034045166015625, 0.189103857421875, 0.1469040283203125, 0.1335036865234375, 0.12200390625, 0.115903955078125, 0.0982034423828125, 0.09400407104492188, 0.08470368041992188, 0.079603857421875, 0.07160411376953126, 0.06760372924804688, 0.06350411987304687, 0.06220421142578125, 0.268631884765625, 0.1438203369140625, 0.11771890869140625, 0.09081469116210937, 0.08141334838867187, 0.07531220092773437, 0.07081159057617188, 0.06370982055664062, 0.0538093994140625, 0.0537084716796875, 0.053207958984375, 0.049407159423828124, 0.04990675964355469, 0.047006350708007814, 0.04470621948242187, 0.22012685546875, 0.1044143798828125, 0.07511177368164063, 0.06490908203125, 0.05970814208984375, 0.05510753173828125, 0.053607080078125, 0.0501063720703125, 0.04750538024902344, 0.04260537109375, 0.040805322265625, 0.04230494079589844, 0.03990499267578125, 0.04000469970703125, 0.04180447082519531, 0.16722200927734376, 0.08551044311523437, 0.06060750732421875, 0.062206494140625, 0.06020596923828125, 0.04890551147460938, 0.05180535888671875, 0.049005010986328125, 0.04580474853515625, 0.04380426025390625, 0.04270408020019531, 0.04660422973632813, 0.04490399169921875, 0.03830400085449219, 0.04010418090820313, 0.0799167236328125, 0.07560855102539063, 0.06500606079101562, 0.059406219482421876, 0.07140601806640624, 0.07300488891601563, 0.08630518188476563, 0.06550490112304687, 0.07960457763671876, 0.08440438232421875, 0.0755042724609375, 0.0768046630859375, 0.0777044921875, 0.0804038330078125, 0.07920401000976562, 0.3316080078125, 0.207107568359375, 0.193606494140625, 0.1498059326171875, 0.13540714111328125, 0.1171072998046875, 0.10630863037109375, 0.09710654907226562, 0.091407958984375, 0.08380844116210938, 0.08290755004882812, 0.07660767822265625, 0.07160776977539063, 0.06800804443359375, 0.06500792236328125, 0.247733154296875, 0.143520703125, 0.1516193603515625, 0.10061497802734375, 0.08071353759765625, 0.07361171264648438, 0.06731063232421874, 0.06820971069335938, 0.05770914306640625, 0.055808380126953126, 0.06520828857421875, 0.055707659912109374, 0.05760715942382812, 0.05470679931640625, 0.05170650024414063, 0.2147247802734375, 0.11991435546875, 0.1060151611328125, 0.07211005859375, 0.06240806884765625, 0.06340736083984375, 0.06250673217773438, 0.05480682373046875, 0.05410577392578125, 0.05570557861328125, 0.0533065185546875, 0.05190557250976562, 0.05410576171875, 0.05250546875, 0.055705169677734374, 0.16842147216796874, 0.08521199340820312, 0.060010601806640626, 0.06880721435546874, 0.06240623779296875, 0.059706341552734375, 0.06670625, 0.06030548095703125, 0.06130540771484375, 0.07100557250976562, 0.06410533447265625, 0.07740518798828125, 0.08170540771484375, 0.0671052490234375, 0.06410557250976562, 0.118316845703125, 0.0852085205078125, 0.09860599975585937, 0.106906884765625, 0.13390623779296876, 0.12500596923828125, 0.1243066650390625, 0.1100060302734375, 0.1235061279296875, 0.1192071044921875, 0.12300640869140625, 0.1312077392578125, 0.11660816650390625, 0.1446067138671875, 0.12000640869140625, 0.33771181640625, 0.2109085205078125, 0.19760986328125, 0.156710693359375, 0.13741339111328124, 0.1203125, 0.1094124267578125, 0.09821099853515625, 0.095712353515625, 0.08861192016601563, 0.0877123046875, 0.08181312255859376, 0.07401165771484375, 0.07041445922851562, 0.06761199951171876, 0.2391337646484375, 0.15082109375, 0.15221976318359376, 0.110115673828125, 0.0906137451171875, 0.08061203002929687, 0.0798109375, 0.07510982055664063, 0.0723095703125, 0.0723088623046875, 0.07530877075195312, 0.0778081787109375, 0.0736073974609375, 0.06780704345703124, 0.0732067626953125, 0.20992392578125, 0.118915087890625, 0.15011522216796874, 0.0931110107421875, 0.0996090576171875, 0.111908056640625, 0.1047079833984375, 0.08770750732421875, 0.1100072265625, 0.1032072265625, 0.10240753173828125, 0.09070778198242188, 0.09780736083984375, 0.09090678100585937, 0.10930732421875, 0.16272099609375, 0.0942118896484375, 0.1015150146484375, 0.12200931396484375, 0.1038099609375, 0.13691119384765624, 0.1240104736328125, 0.1134087646484375, 0.11231099853515625, 0.1202103271484375, 0.15001024169921875, 0.16400906982421876, 0.12190977783203125, 0.12250909423828125, 0.1340109375, 0.17991627197265625, 0.251109423828125, 0.20571015625, 0.26041220703125, 0.2631103759765625, 0.2005136962890625, 0.21061240234375, 0.229611328125, 0.34031123046875, 0.29971201171875, 0.318014990234375, 0.34081640625, 0.22751220703125, 0.230112255859375, 0.2470134033203125, 0.3121179931640625, 0.18662510986328126, 0.16012056884765624, 0.14852603759765626, 0.133626318359375, 0.1245200439453125, 0.1115210693359375, 0.10852296142578124, 0.100634033203125, 0.09672996826171874, 0.09503179931640625, 0.09413408203125, 0.09052275390625, 0.09112301025390625, 0.08852470092773437, 0.266931201171875, 0.17081866455078126, 0.142416015625, 0.13381485595703124, 0.1171133544921875, 0.132112451171875, 0.1313111572265625, 0.15141085205078125, 0.11121005859375, 0.12270966796875, 0.1410094970703125, 0.15120941162109375, 0.1252090576171875, 0.1353091064453125, 0.13460885009765625, 0.238726708984375, 0.15871708984375, 0.2248142578125, 0.15391337890625, 0.20981171875, 0.1799132080078125, 0.167013134765625, 0.14891513671875, 0.15471112060546874, 0.17961226806640626, 0.16481409912109374, 0.15871512451171876, 0.1869125244140625, 0.152413525390625, 0.15681346435546875, 0.262723876953125, 0.196815869140625, 0.1628224853515625, 0.17771539306640624, 0.23612099609375, 0.2068179931640625, 0.17161669921875, 0.168614892578125, 0.167415478515625, 0.16251796875, 0.2039164794921875, 0.162415869140625, 0.20361868896484375, 0.175515234375, 0.176715673828125, 0.31172626953125, 0.344719677734375, 0.47671630859375, 0.5089177734375, 0.5142236328125, 0.521020703125, 0.7755171875, 0.704316845703125, 0.731416748046875, 0.817616259765625, 0.83372041015625, 0.8301162109375, 0.8814203125, 0.887017578125, 0.88611767578125]\n",
        "err_SGLD_list = [0.3423, 0.2554, 0.266, 0.1889, 0.1636, 0.1369, 0.1367, 0.1116, 0.1059, 0.1107, 0.1038, 0.0939, 0.093, 0.0942, 0.0929, 0.257934228515625, 0.132525537109375, 0.10762659912109375, 0.08281889038085938, 0.100216357421875, 0.06801369018554687, 0.070713671875, 0.0634111572265625, 0.06121058959960937, 0.05521107177734375, 0.0665103759765625, 0.05850938720703125, 0.0608093017578125, 0.056709423828125, 0.05650928955078125, 0.22042578125, 0.1031132568359375, 0.0860107666015625, 0.06880828247070313, 0.06651002197265625, 0.05150679931640625, 0.05890706787109375, 0.04920634155273437, 0.05590612182617188, 0.04710552062988281, 0.07930665283203125, 0.04780585021972656, 0.0502060791015625, 0.05050567016601563, 0.04440565185546875, 0.1575220458984375, 0.06951031494140625, 0.06480859985351563, 0.058306884765625, 0.05510665283203125, 0.0465051513671875, 0.056005889892578126, 0.04910491943359375, 0.0441055908203125, 0.0468047119140625, 0.0636079345703125, 0.04560477905273438, 0.043105020141601565, 0.0512050537109375, 0.04280444030761719, 0.06181575317382813, 0.03970695190429688, 0.04640648193359375, 0.03370582885742188, 0.03820551147460938, 0.02890465087890625, 0.06760560302734375, 0.030904910278320312, 0.030204409790039063, 0.027104681396484375, 0.03770635986328125, 0.040004559326171876, 0.02590430908203125, 0.03240512084960938, 0.030804278564453124]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RO1Xu9dTAEK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6509ef08-0d32-4e54-acd0-06362571b73b"
      },
      "source": [
        "!pip install tensorflow_privacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_privacy in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied: tensorflow-estimator>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_privacy) (2.4.0)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.7/dist-packages (from tensorflow_privacy) (1.2.1)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.7/dist-packages (from tensorflow_privacy) (1.4.1)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_privacy) (0.1.6)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy>=0.17->tensorflow_privacy) (1.19.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from dm-tree~=0.1.1->tensorflow_privacy) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c96xP6y80Sgo"
      },
      "source": [
        "r\"\"\"This code applies the moments accountant (MA), Dual and Central Limit \n",
        "Theorem (CLT) to estimate privacy budget of an iterated subsampled \n",
        "Gaussian Mechanism (either uniformly or by Poisson subsampling). \n",
        "The mechanism's parameters are controlled by flags.\n",
        "Example:\n",
        "  compute_muP\n",
        "    --N=60000 \\\n",
        "    --batch_size=256 \\\n",
        "    --noise_multiplier=1.3 \\\n",
        "    --epochs=15\n",
        "The output states that DP-optimizer satisfies 0.227-GDP.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "from scipy import optimize\n",
        "\n",
        "# Total number of examples:N\n",
        "# batch size:batch_size\n",
        "# Noise multiplier for DP-SGD/DP-Adam:noise_multiplier\n",
        "# current epoch:epoch\n",
        "# Target delta:delta\n",
        "\n",
        "# Compute mu from uniform subsampling\n",
        "def compute_muU(epoch,noise_multi,N,batch_size):\n",
        "    T=epoch*N/batch_size\n",
        "    c=batch_size*np.sqrt(T)/N\n",
        "    return(np.sqrt(2)*c*np.sqrt(np.exp(noise_multi**(-2))*norm.cdf(1.5/noise_multi)+3*norm.cdf(-0.5/noise_multi)-2))\n",
        "\n",
        "# Compute mu from Poisson subsampling\n",
        "def compute_muP(epoch,noise_multi,N,batch_size):\n",
        "    T=epoch*N/batch_size\n",
        "    return(np.sqrt(np.exp(noise_multi**(-2))-1)*np.sqrt(T)*batch_size/N)\n",
        "    \n",
        "# Dual between mu-GDP and (epsilon,delta)-DP\n",
        "def delta_eps_mu(eps,mu):\n",
        "    return norm.cdf(-eps/mu+mu/2)-np.exp(eps)*norm.cdf(-eps/mu-mu/2)\n",
        "\n",
        "# inverse Dual\n",
        "def eps_from_mu(mu,delta):\n",
        "    def f(x):\n",
        "        return delta_eps_mu(x,mu)-delta    \n",
        "    return optimize.root_scalar(f, bracket=[0, 500], method='brentq').root\n",
        "\n",
        "# inverse Dual of uniform subsampling\n",
        "def compute_epsU(epoch,noise_multi,N,batch_size,delta):\n",
        "    return(eps_from_mu(compute_muU(epoch,noise_multi,N,batch_size),delta))\n",
        "\n",
        "# inverse Dual of Poisson subsampling\n",
        "def compute_epsP(epoch,noise_multi,N,batch_size,delta):\n",
        "    return(eps_from_mu(compute_muP(epoch,noise_multi,N,batch_size),delta))\n",
        "\n",
        "from tensorflow_privacy.privacy.analysis.rdp_accountant import compute_rdp\n",
        "from tensorflow_privacy.privacy.analysis.rdp_accountant import get_privacy_spent\n",
        "\n",
        "# Compute epsilon by MA\n",
        "def compute_epsilon(epoch,noise_multi,N,batch_size,delta):\n",
        "  \"\"\"Computes epsilon value for given hyperparameters.\"\"\"\n",
        "  orders = [1 + x / 10. for x in range(1, 100)] + list(np.arange(12, 60,0.2))+list(np.arange(60,100,1))\n",
        "  sampling_probability = batch_size / N\n",
        "  rdp = compute_rdp(q=sampling_probability,\n",
        "                    noise_multiplier=noise_multi,\n",
        "                    steps=epoch*N/batch_size,\n",
        "                    orders=orders)\n",
        "  return get_privacy_spent(orders, rdp, target_delta=delta)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcdnY9lz4VhW"
      },
      "source": [
        "def mu_SGD(epoch, noise_multi, batch_size, N):\n",
        "    return np.sqrt(epoch * (np.exp(1/(noise_multi**2) - 1 )) ) * batch_size/N\n",
        "\n",
        "def mu_SGLD(epoch, batch_size, N, clipping_norm, lr):\n",
        "    noise_multi = batch_size/N/clipping_norm/np.sqrt(lr)\n",
        "    return np.sqrt(epoch * (np.exp(1/(noise_multi**2) - 1 )) ) * batch_size/N"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvz0Zqj41Rq-"
      },
      "source": [
        "from numpy import array\n",
        "\n",
        "\n",
        "epsilon_SGD_list = np.zeros(375)\n",
        "epsilon_SGLD_list = np.zeros(75)\n",
        "\n",
        "k = 0\n",
        "for noise_multiplier in [0.5,0.9,1.3,2,3]:\n",
        "    for clipping_norm in [0.5,1,1.5,2,5]:\n",
        "        for epoch in [1*14,2*14,3*14,4*14,5*14,6*14,7*14,8*14,9*14,10*14,11*14,12*14,13*14,14*14,15*14]:\n",
        "            epsilon_SGD_list[k] = eps_from_mu(mu_SGD(epoch, noise_multiplier, 256, 60000),1/100000)\n",
        "            k += 1\n",
        "\n",
        "\n",
        "k = 0\n",
        "for clipping_norm in [0.5,1,1.5,2,5]:\n",
        "    for epoch in [1*7,2*7,3*7,4*7,5*7,6*7,7*7,8*7,9*7,10*7,11*7,12*7,13*7,14*7,15*7]:\n",
        "        epsilon_SGLD_list[k] = eps_from_mu(mu_SGLD(epoch, 256, 60000, clipping_norm, 1e-5),1/100000)\n",
        "        k += 1\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atTh5b3a72ty",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51e58531-6f0c-46fc-95fd-ccfe86ed6ea2"
      },
      "source": [
        "print(epsilon_SGD_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.13490288 0.19692489 0.24562017 0.28728771 0.3244101  0.35827516\n",
            " 0.38965448 0.41905427 0.44682817 0.47323495 0.49847081 0.52268876\n",
            " 0.54601091 0.56853658 0.59034785 0.13490288 0.19692489 0.24562017\n",
            " 0.28728771 0.3244101  0.35827516 0.38965448 0.41905427 0.44682817\n",
            " 0.47323495 0.49847081 0.52268876 0.54601091 0.56853658 0.59034785\n",
            " 0.13490288 0.19692489 0.24562017 0.28728771 0.3244101  0.35827516\n",
            " 0.38965448 0.41905427 0.44682817 0.47323495 0.49847081 0.52268876\n",
            " 0.54601091 0.56853658 0.59034785 0.13490288 0.19692489 0.24562017\n",
            " 0.28728771 0.3244101  0.35827516 0.38965448 0.41905427 0.44682817\n",
            " 0.47323495 0.49847081 0.52268876 0.54601091 0.56853658 0.59034785\n",
            " 0.13490288 0.19692489 0.24562017 0.28728771 0.3244101  0.35827516\n",
            " 0.38965448 0.41905427 0.44682817 0.47323495 0.49847081 0.52268876\n",
            " 0.54601091 0.56853658 0.59034785 0.02943242 0.04322255 0.05406531\n",
            " 0.06334678 0.07161564 0.07915731 0.08614315 0.09268575 0.0988639\n",
            " 0.1047353  0.11034374 0.11572342 0.12090163 0.1259006  0.13073872\n",
            " 0.02943242 0.04322255 0.05406531 0.06334678 0.07161564 0.07915731\n",
            " 0.08614315 0.09268575 0.0988639  0.1047353  0.11034374 0.11572342\n",
            " 0.12090163 0.1259006  0.13073872 0.02943242 0.04322255 0.05406531\n",
            " 0.06334678 0.07161564 0.07915731 0.08614315 0.09268575 0.0988639\n",
            " 0.1047353  0.11034374 0.11572342 0.12090163 0.1259006  0.13073872\n",
            " 0.02943242 0.04322255 0.05406531 0.06334678 0.07161564 0.07915731\n",
            " 0.08614315 0.09268575 0.0988639  0.1047353  0.11034374 0.11572342\n",
            " 0.12090163 0.1259006  0.13073872 0.02943242 0.04322255 0.05406531\n",
            " 0.06334678 0.07161564 0.07915731 0.08614315 0.09268575 0.0988639\n",
            " 0.1047353  0.11034374 0.11572342 0.12090163 0.1259006  0.13073872\n",
            " 0.02056551 0.03026689 0.03790197 0.04444102 0.05026861 0.05558498\n",
            " 0.06051039 0.06512394 0.06948097 0.07362202 0.0775779  0.08137263\n",
            " 0.08502543 0.08855193 0.09196506 0.02056551 0.03026689 0.03790197\n",
            " 0.04444102 0.05026861 0.05558498 0.06051039 0.06512394 0.06948097\n",
            " 0.07362202 0.0775779  0.08137263 0.08502543 0.08855193 0.09196506\n",
            " 0.02056551 0.03026689 0.03790197 0.04444102 0.05026861 0.05558498\n",
            " 0.06051039 0.06512394 0.06948097 0.07362202 0.0775779  0.08137263\n",
            " 0.08502543 0.08855193 0.09196506 0.02056551 0.03026689 0.03790197\n",
            " 0.04444102 0.05026861 0.05558498 0.06051039 0.06512394 0.06948097\n",
            " 0.07362202 0.0775779  0.08137263 0.08502543 0.08855193 0.09196506\n",
            " 0.02056551 0.03026689 0.03790197 0.04444102 0.05026861 0.05558498\n",
            " 0.06051039 0.06512394 0.06948097 0.07362202 0.0775779  0.08137263\n",
            " 0.08502543 0.08855193 0.09196506 0.01698159 0.02502455 0.0313581\n",
            " 0.03678422 0.04162102 0.04603422 0.05012338 0.053954   0.05757191\n",
            " 0.06101072 0.06429593 0.06744748 0.07048127 0.07341027 0.07624519\n",
            " 0.01698159 0.02502455 0.0313581  0.03678422 0.04162102 0.04603422\n",
            " 0.05012338 0.053954   0.05757191 0.06101072 0.06429593 0.06744748\n",
            " 0.07048127 0.07341027 0.07624519 0.01698159 0.02502455 0.0313581\n",
            " 0.03678422 0.04162102 0.04603422 0.05012338 0.053954   0.05757191\n",
            " 0.06101072 0.06429593 0.06744748 0.07048127 0.07341027 0.07624519\n",
            " 0.01698159 0.02502455 0.0313581  0.03678422 0.04162102 0.04603422\n",
            " 0.05012338 0.053954   0.05757191 0.06101072 0.06429593 0.06744748\n",
            " 0.07048127 0.07341027 0.07624519 0.01698159 0.02502455 0.0313581\n",
            " 0.03678422 0.04162102 0.04603422 0.05012338 0.053954   0.05757191\n",
            " 0.06101072 0.06429593 0.06744748 0.07048127 0.07341027 0.07624519\n",
            " 0.01570691 0.02315894 0.02902861 0.03405803 0.03854165 0.04263288\n",
            " 0.04642392 0.04997543 0.05332985 0.05651831 0.05956443 0.06248668\n",
            " 0.0652998  0.06801579 0.0706446  0.01570691 0.02315894 0.02902861\n",
            " 0.03405803 0.03854165 0.04263288 0.04642392 0.04997543 0.05332985\n",
            " 0.05651831 0.05956443 0.06248668 0.0652998  0.06801579 0.0706446\n",
            " 0.01570691 0.02315894 0.02902861 0.03405803 0.03854165 0.04263288\n",
            " 0.04642392 0.04997543 0.05332985 0.05651831 0.05956443 0.06248668\n",
            " 0.0652998  0.06801579 0.0706446  0.01570691 0.02315894 0.02902861\n",
            " 0.03405803 0.03854165 0.04263288 0.04642392 0.04997543 0.05332985\n",
            " 0.05651831 0.05956443 0.06248668 0.0652998  0.06801579 0.0706446\n",
            " 0.01570691 0.02315894 0.02902861 0.03405803 0.03854165 0.04263288\n",
            " 0.04642392 0.04997543 0.05332985 0.05651831 0.05956443 0.06248668\n",
            " 0.0652998  0.06801579 0.0706446 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExCFAzrg77bt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6d1e5a6-a816-4659-85ce-fe7cdb179bdc"
      },
      "source": [
        "print(epsilon_SGLD_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.59401250e-02 2.35003162e-02 2.94549054e-02 3.45569388e-02\n",
            " 3.91052115e-02 4.32553794e-02 4.71009983e-02 5.07035991e-02\n",
            " 5.41062570e-02 5.73405456e-02 6.04304406e-02 6.33946660e-02\n",
            " 6.62481819e-02 6.90031695e-02 7.16697069e-02 2.00833905e-02\n",
            " 2.95619064e-02 3.70221095e-02 4.34116259e-02 4.91061065e-02\n",
            " 5.43011304e-02 5.91141978e-02 6.36225618e-02 6.78802884e-02\n",
            " 7.19269983e-02 7.57927714e-02 7.95010946e-02 8.30707305e-02\n",
            " 8.65169540e-02 8.98524014e-02 2.94552415e-02 4.32558702e-02\n",
            " 5.41068691e-02 6.33953819e-02 7.16705151e-02 7.92179003e-02\n",
            " 8.62090272e-02 9.27565866e-02 9.89394124e-02 1.04815254e-01\n",
            " 1.10427942e-01 1.15811689e-01 1.20993821e-01 1.25996577e-01\n",
            " 1.30838353e-01 5.01608811e-02 7.34649476e-02 9.17692887e-02\n",
            " 1.07429849e-01 1.21377629e-01 1.34096384e-01 1.45876317e-01\n",
            " 1.56908009e-01 1.67324710e-01 1.77223989e-01 1.86679870e-01\n",
            " 1.95750113e-01 2.04480837e-01 2.12909566e-01 2.21067332e-01\n",
            " 3.83563687e+01 6.35232091e+01 8.64358376e+01 1.08177536e+02\n",
            " 1.29169586e+02 1.49628524e+02 1.69683168e+02 1.89417540e+02\n",
            " 2.08890046e+02 2.28143240e+02 2.47209264e+02 2.66113096e+02\n",
            " 2.84874612e+02 3.03509927e+02 3.22032331e+02]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRP5k67u8CzK"
      },
      "source": [
        "## Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1rzQY_W8EqA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "outputId": "c0f60cd2-df05-46f5-d270-ff0ab142e820"
      },
      "source": [
        "import matplotlib.pyplot as plt;\n",
        "import numpy as np;\n",
        "import scipy.optimize as opt;\n",
        "\n",
        "\n",
        "\n",
        "# Generate some data, you don't have to do this, as you already have your data\n",
        "\n",
        "xdata1 = epsilon_SGD_list\n",
        "ydata1 = np.ones(375) - err_SGD_list\n",
        "\n",
        "xdata2 = epsilon_SGLD_list\n",
        "ydata2 = np.ones(75) -  err_SGLD_list\n",
        "\n",
        "# Plot the actual data\n",
        "plt.plot(xdata1, ydata1, \".\", label=\"DP-SGD\")\n",
        "plt.plot(xdata2, ydata2, \".\", label=\"DP-SGLD\")\n",
        "\n",
        "plt.xlim([0, 1.05])\n",
        "\n",
        "plt.xlabel(r'$\\epsilon$',fontsize=16)\n",
        "plt.ylabel('Accuracy',fontsize=16)\n",
        "\n",
        "plt.legend(loc=4, prop={'size': 14})\n",
        "plt.xticks(fontsize= 14 )\n",
        "\n",
        "# Use the optimized parameters to plot the best fit\n",
        "# Show the graph\n",
        "\n",
        "plt.savefig('DP_SGD_vs_DP_SGLD.pdf',bbox_inches = 'tight')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAESCAYAAAAMifkAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXwU9fnA8c/MbhIICSGEXCSEK5BwGgRUECgiVwGL4IGA2orFehStrXgWQbFatP5spVilLSiHeNQWKgJiVRTkRsAgEggQAiEkISQx4Up2Zn5/LLvk3s1mr7DP+/VC2d3ZmeebJfPs91YMwzAQQggh6qH6OgAhhBD+T5KFEEIIhyRZCCGEcEiShRBCCIckWQghhHBIkoUQQgiHvJos5s2bx7Bhw0hJSeHgwYO1HqNpGs899xzDhw9nxIgRfPjhh94MUQghRC3M3rzYjTfeyN13383UqVPrPObjjz8mOzub9evXU1xczM0338yAAQNITExs0LWKis6i64E5hSQqKozCwjJfh+EzgVz+QC47SPkbU35VVYiMbFHn615NFv369XN4zJo1a7jttttQVZXWrVszfPhw1q1bxy9/+csGXUvXjYBNFkBAlx0Cu/yBXHaQ8nuq/H7XZ5Gbm0vbtm3tj+Pj4zl16pQPIxJCCOHVmoU3RUWF+ToEn4qODvd1CD4VyOUP5LKDlN9T5fe7ZBEfH8/Jkyfp3bs3ULOm4azCwrKArY5GR4dTUFDq6zB8JpDLH8hlByl/Y8qvqkq9X7L9rhlq9OjRfPjhh+i6zpkzZ/jf//7HqFGjfB2WEEIENK8mixdeeIEhQ4Zw6tQp7rnnHsaOHQvA9OnTSU9PB2D8+PEkJiYycuRIbr/9dh566CHatWvnzTCFEEJUo1ypS5RLM5RUxQNRIJcdpPwB1QwlhBCOZOaU8MmWLDJzSrz6Xle4cr2Gvsd2/IGsM05fY8OeHF59fzcb9uQ4dbzfdXALIarKzCkhI7uIlKRIkhMifB2OW2zYk8OujHz6psQwNC2hQe/NzCnhlRW7sWg6ZpPKzMl9nP65ZOaUMO/db9E0A5NJ4YkpVzfovQ39HFyJNTOnhHnLd6HpYFLhial9631PZk4JL7/7LRbN4L/fZDl1jQ17cliyLgOA748WATDs6vonPkuyuEJU/odceeic7flzFyxk55fSNyUGwOVf1CuBp2++7jx/Y26MtWnMTdpd56ntRtWQc2RkF1Fh0QGosOhkZBc5/TPZnJ6LplmbpzXNYHN6rlPvdTXJuBLr2q3H0KxvQdOtj2fc0rveMlkulanCojtVpl0Z+TUeS7JoorS8TMoPfoMCBHW9HlNsco2bkO0XNikmnPU7j6NpBp2CCzi3q4K9pa1p1rar/Xkb2y8nwNnjB4k7qdGl7zUAWE4ewNw2FVNsco1Y6nrNXbz17dndN19Pnz8juwiLpmMYoGkNuzFW19ibtLvOU9uNqiHvP3fBUu/j+pScLa/3cV1cTTKuxHrqzLl6H1d38vTZeh/XJrx5cL2PayPJwsOq32irPz62bw8lh9OJiomiTTMdpVkY2uljWA58DYaGAZz/4Wt2Jt7Fh98bWDQDs0lhRL92rN2WTQdzARW5p2inxIEZHmixHlOuRgImFuwaSTsFkpudItMSR5Yl2h5XB3MBD4Wvx3xC59zJL6xPGhrlahCh4x63JwUtL5Nzq18GvaLGa+7i6Rt4Ze68+dZ5fouOAVjccP6UpEhMqoJFM1BVhZSkSJfP1dibtLvOkxQTXuVLS1JMwyaRZeeX1vu4PhEtgut9XBdXk8yB7KJ6H9cmrnUouYXnqjyuT4WtGlLH49qUni+v93FtJFk0ku3mrzQLw7hQVuXbt+1Ga2gVnFdMHE8aS7vsT1ANjXJTED92v5mw7z4iAg31FFxUrOc0DFAUULD+UQ2NgoN7sWi9ALBoBlv3n7Lf8E1oaJjYfrEzJjRMCmDo9A8+zDUhh+2vLygdaU8YyeZTmNCsIxx0DbhU+9AtWE4esF7n5AGMskLQK6xBXXrN3cnC0zfwylKSIjGbVDRNx2RSG3XzrU2L5kG2nySGYX3cWLYBi40duNg3JabKTdrWJOnt84Q2M9f72JPXT4oLr/exu7UKCwFKqz2u30+va8/ew6fRdVBV6+P6DL6qLUdzM6o8dsSVn6Eki0awf+vWLmflcsWEqfM1cL4UQyvH0MpRAHQLoYe/QDFZUBTQtXLK0r+kxaWbu2FYE4NhgHrpse3WoKGSaYkDrDWCZPMpCk1JRF264duSg/VYExi6LQ1UeT3ZfIosSzQKkGmJQ8OEioGiXhoUZ+igmlGahdlrEygm6x+sr5nbprr95+jpG3hlyQkRTB7exd7e7u6kdPZ8hf3vSrXHrticnlul/drZ5o/aJEaHoarYb0KJ0a4tidPY81g/b8VeS27o592Y67v6+bhaI2nojR+s/0afnNrX6WZZW61uV0Y+Q/sm0Tc5yuE1Kr/H2X4nSRZOqNye3tFcYK9JWI7sxNAqrDd5bDd7DUvmlstvvnTTV4E2plJ7QlCAGArRUcHQUblcozAM0FHYV55IqdGcHeWdybJEV6lJ6Eo6Hxn9qiSHHeWd2VHemS7mUxyyxKEA14Qctr+eaYmjc3ABo9tfYP+FaI4k/ZIEPYfMiljax4UTV3ECc9tUa83CVptAx5z6E9SwKI/1WSQnRDBzch+v9Vms+N8hLJrOweMlJEaHufV6KUmRBJm9k/gaKiO7CHvlxMDlGpw7zmNU+7+3ru/q5zOwVzwb03PRNQPVpDCwV7xT72vojd9VidFhnD1fQYf4lg1+j7PJVpKFA5k5JXzwwad0VHP5enszYlrsRNGt30aMS//R4XLCUKre9BUF9EvJQa30d0UBxTDYerELRXoLminlDGu2H8UwMFD48Ny1bLnYtUosXSrVJFQ0WprKeaN0JF2C8zhsiSVbi8ZkUhk7bjgtC0pJSYrkyJEkCg/uZUeR9Zfi/hbrMZ3W6ICJN3eMJEuLQdcrMJuKmTl5MMmx1n/M5WoQ6BZQzQRf6mD3pOSECK8MC/V0k5e7ay6u3qRq464aXGPPk5FdZJ8wa+hGgz+Dxlzf1S8myQkRTB3R1WM10soa2odX+fiPN2fx2B3ODc+tfo2u7VrV+x5JFg6c/OE7fhX6KSY04NLdnko1iUv/t6mcKOyPAQ0F5VKNAUA1DHttwNaPsK8iiWRzzc5om9PNkkBNxzB0FJOZ64YNIay0FSlJkfQD+y9AZKtQThWUcqKgjBXbLlJhsSad4c3SqzRLdVRPcbjcep3KN05TbDKh4x73+AgoX/B0k5e7ay7JCRE8MeVqt3wzdVcNrrEJsbGfgTdrojaufq6uDN5o6BeaysdbnByeW9s1JFk0UnJQnv0Ge7mj0fqarUahXMoWeqUEQaXXdAN7DcLW91BbUjipxJJ1oWaSsMnSogmf8KT9Jh4em4ytBdQ20/NEQZl9go5iy2a2Yy71U9iapY7qcZhMCoZu1PilNcUmX1FJwsbTNxpP1Fy8VetyVmMToqf7jerj6sg7Vz9XV97X0GRa+Xiz2bnk60rClmRRiyp9FKlpnD30KbpeYR+hxKWaQ4keSivVOsTN2iFtbT5qoVzkrBHCxNAdmCr1J1SvLSSbrZs62Z4vr6h/yFtKu0jWHTGTknS5ucgWrz1BQJXROLYRVSigRHfiSKdfEl56lB1Frbj+2quYFB3mF7ODvTlL2ZM3X2921jeUu4YoNzYhNjbZNKYcrsbu6ufqyvsa+oWm8vHX9U4gqoXjEXiufGmSZFFNZk4JK95bR2fTKVZsjmPyHaPpeNMTlB/8hvIfvrKPOrKg8umF3twSuh2Todfaz5CrRdZag6g+5LXykNbK4qNCaR0eQkHJeTrFR7DrYAHbfsiz/4KcKChjV0Y+FRW6fQZnjQ5DW5OYAVm5pSzMV1CIR9MNzLmHmDm5D2MHdHDzT7FhvDnPwnY9TyUmT9Rc3BWvu+aANHbuR2OTTWPe7+pNvzF9Ha6+r6G1teSEiAYtJNjQa0iyqGbnxi08GHb5Rv7Zxgg63pCMGhZFdvubOJHxA4C9plBbQlAUuLZbLHlF4XyRF1Nj9dvkakNebUNaK1NVhXvGdLN/mJ9syWLbD3n2X5C1W4+x+9DpWstg60hXK/WdgDWR6JpxeUiuh+c0OMub8yy8kZjcWXNxZ7zunAPSmNFMja19+aKD2/ZeV372/taM6CpJFpfYvr21+PFIlRt5u7J0zn78EegWEjGxVetHoukM/YMPE28qItFUdZVHRYHR1yTxv10n7GvC1LhWtb4DWz9GZbpedUmB6t/missu1niPAqgmhfsn9OZUQSktmgdZq/uXvk0qijUJKZfO7y/NJN5suvFmYnIHd84Id9cckMaOZmps7csd7/fnz9xfSbKg6re3juYoBoZdvpG3CgtBL6mwDlc1LNwWug3V9n3KNgzKgGtDMvlr6SiyLNF8uTunzkQB1j6KBaUjSTaf4ogWd2nIq0JMq+ZVpvnbYsvILqr6rRBITYrkaO7l6uZPr00itJmZlKRIBqQl2quiiZf6JFo0D+Ls+Qr7jdgf+ilsvDm6xRuJyZ3NXO6sDbhrDog7foaNvWHLDd/7JFlQ9dtmliWaHS1uIOF8Bj8GxxJ8sfzSxDnrr6yCYR/9ZB82q4CpUnPShXLN4TWzLNFkWaIZmtaWPhHN7L9w89791j6mPiku3J7EFMU6agms3+ZCm5m5e3SKwxmYdf1SBeovmqcTU+XBBmaTwuMNWAK7Nu6cEe7OobPeHroqfE+SBVW/KXUKPs2AC1+BXoFy4Tg61lFOxy1R5Ovh9A0+WrUf4FLiqKs5qT4m1TrJqvIvW+Ux9ZWTGIa1+ckwLjcfJSdEXBFLjHu7g9uT30orLxdtacDqpHVx94xwd5VdvtkHHkkWWP/hPz26FSWH04kNOY+SbbFPrLPNr0gyn6Y9pzEADfi+oh0/VCTY+yxqGxpbH1VVGNm/HRmXVqG0/eJV/yWsXN2fPLyLvSnpSvpFbWr9CN4k3+KFv5BkgXVBwNbb3qD1pYXzDNWErlns6zjB5ZFF6qWJDNmWNjWW43DEpMLUkSmcPV9xufO52rfp6u3dgXCj8PbcBE8OnXXn8hw28i1e+IOATxaZOSWc27mFDloFCgaGoXMuaQBZJWZaXTxJ/IVDNRb5M1Aa3OTEpXPYagarNh2xj3KxfZsGauzGFQi8mRQ93eTlzuU5hPAnAZ0sbDeOBIJ5KFzFhI6Gwj/2t+RoRTSdg1vwUHgWmlaBinXugo7CyovXOd3kpKqKNcnoBoqqkH64kH9/daTK+HQD6w5a1XfjWrv1GPuOnvFaW74veevbszeavKQmIK5EAZ0sbDeOLOPyUNbKk+uOlLdhY/QdlGbto0wPoYVysc5F/uqi6wZdEyOo0HSOnSrl4ImSGscYBqzdlk3XxKo3mOKyi9KW72b+vByHEP4soJNFz/Biyprv42B5LEct0RzTojGpCiYT9sX1zoTEseG84vhk9cjMKaHaJO5aFZScx6Qq1qU4TAqDr2rLiYJDcmNzI18uYidEUxawycLWqf3TZhWMam7iUOq95JvjrUMVi45y4YdNmM0qP1zUUFUF3cG2trYlNqpTqi25UZ+i0nJMJoWhaW3tQ2oT/WShvyuFpzc/EuJKFbDJwrYbnIKBCZ3eYYWE9BnAsX17iNg8n0g0MCCSnexWR3FUr7/pSTess6jPX7TYN3OPaBFMUlw4yz87aO+LqKzaCuKAtUYTFdGszqG0onFkmK4QrlF9HYCvmNumghoEimrfWzozp4StX3yNYmj2vShM6CQHnbLP2q5Pdn4pA3vFs+/oGfZknuabfadIjA5jcB3DJ22JwrZmk6pgb27KzCnhky1Z9n0qrnTeKq+tz6Lyz1oI4VjA1iyq7wZ31BLNqk1HOFsey4gQFeXSUuQaKke0OEZfk8Sn27Pr7XvomxJT45vr2q3HKC67iElV0PXLK76aVOvmGLa+kcoT7gCvzmj2NXcvkVGfQJm7IoS7BWyygMu7wdmG0FoX/4vmr6Wj6B98GLDOzD5miabdRQtXJbepsSx45b22s0+VkhQXbh9tA1Q5Pj4qlLTkNvYF/6D2Bf0+2ZIVUE0l7l4iwxFp2hOi4QI6WdhsTs+tskqsbZG/DuYC+252G/ZYZ2BX77CuvNf2hj0nCTJbawnZp0rZsOdklevkFp4jv/i4vVmqrpuWDO8UQvibgE0WlZf+3pSeC1h3sOsSdIrjtOVChV7rbnZdEyM4fPJH9Gr7cdtYLDq7MvIJNptqva6mGWzYc5Jv9p2qs3kp0JpKPLFEhhDCvQIyWVRe8kHBOpKp6lan37Fd6VzrbnZt27Tg1huS2ZyeS8nZcvYePl1lWK0BfH+0yGEMjpqXvNVU4s29r+siS2QI4f8CMllU7oS2VQyqb3UKoGFC4fJuduZK33o3XlqaQ1WsfRF5RedrbJ9aH39oXvL20uD1kX4EIfxbQCYLW59A5X6K6lud7ijvzI7yztzcpZyIzr3oW9qKKZe+9S5Zd8A+b0I3qLG7XXUmVcHAsOYgBTrEhTP4qrY+vznKnAMhhLMCMlnYlnzYuPckWadK7TvkvVE6kmujitlW2Iosi3Wr0z0h8QyMjGdsT+tNNDOnhGN5pXWeW7WvTAsmk8LgXvEkxYXz7mcHsWB9ISu3lBMFh3w+e1g60oUQzgrIZFF5yQdVsd7YDeAEsUweOZpOWEdIbUrP5au9lzujgUpDbC+zTarr3SmK744UWjtqVYWpI7oyNC2BT7ZkoVVqoqq8LLkvk0WgdaQLIVwXkMmiep/F8PblRF/MpmWnXvYbZkZ2EZpuVGmiAbBcmj+hAN07RtI3Jca+mdGujHx07fJYWtt+yS2aB6EqCpptH28/mj0sfQVCCGd4PVkcPXqUJ598kuLiYlq1asW8efPo0KFDlWMKCwt56qmnyM3NxWKxcO211/L73/8es9k94VZufukQVMCIkkujoPZv41hkc9r3TKtyjKoqFJZcqDLhzmRSGT+ok313u8o1DqXash0r/ncITbfWNkb1b2eflCc3aSFEU+H1ZDF79mymTJnC+PHjWbVqFc8++yxLliypcsybb75J586dWbhwIRUVFUyZMoX169czZswYt8RQufklKvsYpuLLo6BKDqdDzzQAru8ZR8nZcr47UshXe09i3lf7Pti2mgpcqnF0iLQnEttsbAAMg9BmZsYO6OCWcgghhLd4dSHBwsJC9u/fz7hx4wAYN24c+/fv58yZM1WOUxSFs2fPous65eXlVFRUEBsb69ZYkhMiGDugA6a2qWiY0AwFDZWLrS8v//HV3pN8d7gQTbvcHJV9ytq5faKgzL7wXeXF6czmyzUOkIXrhBBXBq/WLHJzc4mNjcVkss5uNplMxMTEkJubS+vWre3HPfjgg8yYMYNBgwZx/vx5pk6dSt++fT0SU745nnWlI+lsPsVhSxxXm+P5odLyH5phbT7i0v+//u6kfRKegjU5zJzcp0pHMVjXd7LVPmSzHSFEU+eXHdzr1q0jJSWFd955h7NnzzJ9+nTWrVvH6NGjnT5HVFRYva8fyDpD+uHTxEWHk6PEknUxGlVRuFpV2fhd1TWdJvykM+cuVLDvSCHH88rszxtYl/dYuy2bKaNS+UVaIgeyzvDMm99gseiYzSrTx/fivc8PYbHoHDpRQq8uMaR2aI2nRUeHe/wa/iyQyx/IZQcpv6fK79VkER8fT15eHpqmYTKZ0DSN/Px84uOrrgW0bNkyXnzxRVRVJTw8nGHDhrFt27YGJYvCwrI6Z1RXnrlsUhU6xbckM6cETTdY+dVhtGq74hUWneObfadqDJkFa8LYc7CAfUcK7bWLCot1pJXForNhV3aVx1u/yyGqRZDT5XBFdHQ4BQV1zwW50gVy+QO57CDlb0z5VVWp90u2V/ssoqKi6NatG6tXrwZg9erVdOvWrUoTFEBiYiJff/01AOXl5WzZsoUuXbq4LQ5bh3R7UwFDg76j/FSmfZ8K3TCovM+R2WR9ZKmWQUyqQsf4cPtud7bhtdX7KPqmxEifhRCiyfN6M9ScOXN48skneeONN2jZsiXz5s0DYPr06Tz88MP06tWLp59+mtmzZ3PTTTehaRrXXnstt99+u9tiSEmKpHPwaX4VWnNVWVUBHeDSHhVTRnQlMTqMb/adQtN0FNU6K9u2RtQrK3ZXmQFd20Q3f95H2x8WEhRC+D/FMKovsn1lqK8ZCuDkV/+iRcYnqBhohsK6i30oSBjK3szT9lqGAkz8SSfGDuhQ5021oTdbb9ycna2K+tNCgu4UyE0RgVx2kPJ7shnKLzu4vSE2NY1zmesxNAuoKgOGDWFfaUv2VNrZTlEVe7NRXTOdbc/b9pCuLwlUnrynqgp3jrQuB+IrGdlFWCy6taPeD5YfEUL4r4BNFtX34G4Vm0xFTglms3ppzSiFqSO72m+e9dUInP2Gbuv8BtB1g+XrD/p0McEWzYPsS7QbhvWxEELUJiCTRWZOCZsv7Y43sNdgkmOtN+u6FtZzlAycXeo7JSkSVVXszWO6Yfj027xt7SqwNrlVfiyEEJUFXLLIzCnh5Xe/xXJpwb+N6bk8MeVq+w27tuYmR8nA2aW+kxMiuHNkV5avP4huGJh9PDoqJSmSILMsUS6EcCzgkoX1xn+541vTDFZtOmJfPba2ZiZHyaAhS30PTUuwj45q0TzIvpqtL2oXMrtcCOGsgEsW1mGzBXRUT5FpiSPLEs33R4v4/mhRleU7Kt84nUkGDVnq23acr0ciVd7X4+DxEp9vxiSE8F8BlyySftzNjPBPMQwdDTN//XEEWZZooP5Nidy974M/bGnqDzEIIZqGgEoWWl4mFzctBUNHBRQspATnccwSjUHdmxLZRkK1aB5UZ1NVQ/nDlqb+EIMQomkIqGRhOXkAw9CtS3QYoKPQr52Z+PAg9Dad7IkALq8aC1W3Uq2rqaqh/GFLU3+IQQjRNARUsjC3TeWcYkLVLRgoGECbgh3EnNlDaK/HMcUm1xgme33PuCrrQrlz/2x/2NLUH2IQQvg/ry4k6Gum2GS+iZnMJ+f7sOViFxQMVAzQLVhOHgBqtuMDmE2qfXFBf9o/WwghvCWgahYAqf36Me+gSpKSzzUhh1ExUFQz5rapQM12/IGXFg10d5+FEEI0JQGXLJITInhiytVkZBdRFp5CXMUJzG1TMcUm21+vrR1fkoMQIpAFXLKA6u30aQ5eF0IIEVB9FkIIIVwjyUIIIYRDkiw8xLa/RWZOia9DEUKIRgvIPgtPu1J3oBNCBK6ArFls2JPDq+/vZsOeHI+cv7Y1l4QQoikLuJrFhj05LFmXAcD3R603cXdvbSprLgkhrjRO1Sx+97vfsXPnTk/H4hW7MvLrfQyN72+wzdWYMKQTk4d3ISO7SPouhBBNmlM1iz179rBmzRo6derEpEmTuPnmm2nZsqWnY/OIvikx9hqF7XFl7upv8Jc9K4QQwh2cqll8/vnnLFy4kI4dO/Lyyy8zZMgQnnrqKfbs2ePp+NxuaFoCd49OoUfHSO4enVKjCcqd/Q3SdyGEuFI43WcxePBgBg8eTEFBAR988AEfffQRK1euJCUlhUmTJvGzn/2MFi1aeDJWtxmalmBPEra9KmxLe7izv0H6LoQQVwrFMAzD8WE15eXl8dhjj7Fjxw4AQkNDmTRpEjNmzCA0NNStQbqisLAMXa+7aJk5JWxOz2Vjei66blRpJqqeQBrDnedyVnR0OAUFpV65lj8K5PIHctlByt+Y8quqQlRUWJ2vN3g01JYtW3jvvff4/PPPadGiBb/4xS8YPXo0X375JUuWLOHEiRPMnz/fpWA9ScvLxHLyAOa2qRy1RFfZ0Aiq7lHRmLWhqicH2x9bp7msWCuEaIqcShZFRUX8+9//5oMPPiA7O5vu3bvz3HPPMW7cOEJCQgBIS0uja9euPPPMMx4N2BVaXibnVr8MegXlahAn299VZUMjABSFFs2DGnWdujrHZZKeEKKpcypZDBkyBFVVGTNmDK+88gq9e/eu9bhOnTrRunVrtwboDpaTB0CvuLSXqoXkoDzMpjZomo6iWJ82dIMV/ztEYnSYyzfy2jq0kxMi6nxeCCGaCqeSxW9/+1smTpxIRET9N7hu3brxxRdfuCUwdzK3TaVcDQLdAqqZ2NQ0ZiZHk5FdRGHJBb7ae9It26XW1aEtHd1CiKbO5Q5uf1e9g7tyn4VtoyO43HRku5E3tomorg5tb3Z0Sydf4JY/kMsOUn5PdnA7lSxefPFFioqKeOWVV2q8NnPmTNq0acMTTzzhUoCe4mg0VGW+GLHkSfILE7jlD+Syg5Tfk8nCqUl5X3zxBddff32trw0aNIjPP//cpeD8RXJCBGMHdLgiEoUQQniCU8kiLy+Ptm3b1vpaXFwceXl5bg1KCCGEf3EqWURERHDs2LFaX8vOzvaLSXhCCCE8x6lkMWDAAP72t79x+vTpKs+fPn2aN998k4EDB3okOCGEEP7BqaGzjzzyCLfeeisjR47khhtuIDY2lry8PDZs2EBISAi/+c1vPB2nEEIIH3IqWSQmJvKvf/2L119/nc2bN1NcXEyrVq0YMWIEM2bMICHBvZsHCSGE8C9Orw2VmJjIyy+/3OgLHj16lCeffNKecObNm0eHDh1qHLdmzRr+9re/YRgGiqKwePFi2rRp0+jrCyGEaDivb6s6e/ZspkyZwvjx41m1ahXPPvssS5YsqXJMeno6f/3rX3nnnXeIjo6mtLSU4OBgb4cqhBDiEqeTRWFhIatXr+bo0aNcvHixymuKovDiiy86dY79+/ezePFiAMaNG8fcuXM5c+ZMlTWl3n77baZNm0Z0dDQA4eHhzoYphBDCA5xKFkeOHOGOO+7AYrFw/vx5IiMjKSkpQdM0IiIiCAure9ZfZbm5ucTGxmIymQAwmUzExMSQm5tbJVkcPnyYxMREpk6dyrlz5xgxYgQPPPAAiqK4UEQhhBCN5VSyePnll+nVqxcLFiwgLS2Nv//976SkpLBy5Urmz5/PggUL3BqUpmlkZGSwePFiysvL+eUvf0nbtm25+eabnT5HfdPWA0F0dGDXxgK5/IFcdpDye7EFl6oAACAASURBVKr8TiWLffv2MWfOHHu/ga7rmM1mbr31Vs6cOcMf/vAHli5d6vA88fHx5OXloWkaJpMJTdPIz88nPj6+ynFt27Zl9OjRBAcHExwczI033sh3333XoGTRkLWhrjSyPk7glj+Qyw5Sfp+vDXX27FlatWqFqqqEh4dTVFRkf61Xr17s27fPqWCioqLo1q0bq1evBmD16tV069atxh4Y48aNY9OmTRiGQUVFBVu3biU1NdWpawghhHA/p5JFYmIiBQUFAHTs2JF169bZX9uwYUODOqDnzJnDsmXLGDVqFMuWLeO5554DYPr06aSnpwMwduxYoqKiGDNmDDfffDPJycnceuutTl9DCCGEezm1RPlLL73EuXPnmDt3LmvWrOG3v/0t7du3x2w2c+TIEe6//34eeeQRb8TrNGmGkqp4IArksoOU35PNUE71Wfzud7+jvLwcgDFjxtCsWTPWrFnDhQsXuPvuu7n99ttdCk4IIUTT4DBZaJrGkSNHiImJsT83bNgwhg0b5tHAhBBC+A+HfRaKonDLLbewf/9+b8QjhBDCDzlMFqqqEhcXx/nz570RjxBCCD/k1GioSZMm8c4779j7LYQQQgQWpzq4z549S3Z2NsOHD2fw4MFER0dXWXpDURQefvhhjwUphBDCt5xKFm+99Zb97x999FGN1yVZCCHElc2pZHHgwAFPxyGEEMKPOdVnIYQQIrBJshBCCOGQU81QqampDveS+OGHH9wSkK9l5pSQkV1ESlIkyQkRvg5HCCH8glPJ4qGHHqqRLIqLi9m0aRPl5eVMnDjRI8F5W2ZOCa+s2I1F0zGbVGZO7iMJQwghcDJZzJgxo9bnNU3j/vvvd3qnPH+XkV2ERdMxDNA0nYzsIkkWQghBI/ssTCYTU6ZMYcmSJe6Kx6dSkiIxm1RUBUwmlZSkSF+HJIQQfsGpmkV9ysvLKS4udkcsPpecEMHMyX2kz0IIIapxKlmcPHmyxnMVFRUcOnSIV199lZ49e7o9MF9JToiQJCGEENU4lSyGDRtW62gowzBISkpi9uzZbg9MCCGE/3AqWbz44os1kkVISAht27alV69emEwmjwQnhBDCPziVLK6UobENJXMuhBDCyqlkcfToUQoKCrjmmmtqvLZjxw6io6Pp0KGDu2PzqepzLiYP78LZ8xWSOIQQAcnpZqjk5ORak8WXX37J4cOHq6xMeyWoPOfCYtFZvv4gumGgqgqDe8UzsFe8JA0hRMBwap7Fvn376NevX62v9e/fn/T0dLcG5Q8qz7lQVAVdNy5N1jPYsOckr6zYTWZOia/DFEIIr3AqWZw9e5aQkJBaXzObzZSWlro1KH9gm3MxYUgn7hzZFbNZpXIXv22GtxBCBAKnmqHatWvHli1bGDRoUI3Xtm7dSkJCgtsD8weV51wkRoexOT2XTem56LohM7yFEAHFqWQxfvx4Xn/9ddq2bcttt91GcHAw5eXlfPjhh7zzzjt1rh11JbEljoG94mWElBAi4CiGYRiODtI0jUcffZT169ejqioRERGUlJSg6zojR47kz3/+M6rqX1tjFBaWoesOi3ZFio4Op6DgymsadFYglz+Qyw5S/saUX1UVoqLqXhTWqZqFyWTi9ddfZ8uWLWzevJni4mIiIyO5/vrrufbaa10KTAghRNPRoIUEBwwYwIABAzwVixBCCD/lVNvRl19+ybJly2p9bfny5Xz11VduDUoIIYR/cSpZvPHGG5w7d67W1y5cuMAbb7zh1qCEEEL4F6eSxZEjR+jRo0etr3Xr1o3Dhw+7NSghhBD+xalkoet6nTWLs2fPYrFY3BqUEEII/+JUskhNTeXjjz+u9bWPP/6YlJQUtwYlhBDCvziVLKZNm8b69et5+OGH2bRpE5mZmXzzzTc8/PDDfPbZZ9x7772ejlMIIYQPOTV0dsSIETzzzDO89tprfPbZZ4B1l7zQ0FB+//vfM3LkSI8GKYQQwrecnmdx1113MWHCBL799ltKSkqIjIykT58+fP/99zz11FO89NJLnoxTCCGEDzVojY6wsDCGDBlC79692bVrFzfddBM///nPWbdunafiE0II4QecThalpaW8//773HHHHYwePZo333yTiIgIZs+ezcaNG52+4NGjR5k0aRKjRo1i0qRJZGVl1XnskSNHuOqqq5g3b57T5xdCCOF+9TZD6brOxo0b+c9//sOXX37JxYsXiYmJYerUqSxfvpynn36a/v37N+iCs2fPZsqUKYwfP55Vq1bx7LPPsmTJkhrHaZrG7NmzGT58eMNKJIQQwu3qTBZ//OMfWb16NYWFhYSEhDB8+HAmTJjAwIEDKSsrq3P5j/oUFhayf/9+Fi9eDMC4ceOYO3cuZ86coXXr1lWOXbhwIUOHDuXcuXN1zvEQQgjhHXU2Q7399tsUFhbyk5/8hC+//JJXX32VQYMGoaoqiqLU9bZ65ebmEhsbi8lkAqyr2cbExJCbm1vluAMHDrBp0yZ+8YtfuHQdIYQQ7lVnzeLWW29l3bp1bNiwgdGjRzN27Fhuvvlmevfu7dGAKioqmDVrFi+99JI9qbiivnXZA0F0dLivQ/CpQC5/IJcdpPyeKn+dyeKFF15g1qxZfPbZZ/znP//h/fffZ8WKFXTo0IERI0a4VLuIj48nLy8PTdMwmUxomkZ+fj7x8fH2YwoKCsjOzua+++4D4Mcff8QwDMrKypg7d67T15LNj2QDmEAUyGUHKb/PNj8KCQlh3LhxjBs3jvz8fFatWsWqVatYuHAhAK+++iqTJ09m9OjRhISEOAwmKiqKbt26sXr1asaPH8/q1avp1q1blf6Ktm3bsm3bNvvj+fPnc+7cOZ544gmH5xdCCOEZTg+djYmJYfr06axevZoPP/yQqVOnkpWVxRNPPMGgQYOcvuCcOXNYtmwZo0aNYtmyZTz33HMATJ8+nfT09IaXQAghhMc5tQd3XSoqKtiwYQMrV65kwYIF7oyr0aQZSqrigSiQyw5Sfp/vwV2XoKAgRowYwYgRIxpzGiGEEH6uQct9CCGECEySLIQQQjgkyUIIIYRDkiyEEEI4JMlCCCGEQ5IshBBCOCTJQgghhEOSLIQQQjgkyUIIIYRDkiyEEEI4JMlCCCGEQ5IshBBCOCTJQgghhEOSLIQQQjgkyUIIIYRDkiwaKTOnhE+2ZJGZU+LrUIQQwmMatflRoMvMKeGVFbuxaDpmk8rMyX1ITojwdVhCeNT582cpKytG0yy+DqWG/HwVXdd9HYbP1Fd+k8lMWFgrmjdv4dK5JVk0QkZ2ERZNxzBA03QysoskWYgr2vnzZyktLaJVq2iCgoJRFMXXIVVhNqtYLIGbLOoqv2EYVFSUU1xcAOBSwpBmqEZISYrEbFJRFTCZVFKSIn0dkhAeVVZWTKtW0QQHh/hdohB1UxSF4OAQWrWKpqys2KVzSM2iEZITIpg5uQ8Z2UWkJEVKrUJc8TTNQlBQsK/DEC4KCgp2uflQkkUjJSdESJIQAUVqFE1XYz47aYYSQgjhkCQLN5EhtEKIK5k0Q7mBDKEVwr/94Q9zWLt2NQAmk4nw8JZ07NiJoUNvZPz4iZjN1lvhr399H3v2fAtAUFAQsbHxjBkzjqlTf47JZKr13EVFRfzzn2+xdes3FBaeJiwsnE6dOnPnnT+nf//r7Mfl5Jxg6dLF7NixjTNnCmnZMoKkpPaMGXMTw4ePIigoCIBBg/rZ3xMSEkLr1m3o0aMnEybcylVX9fHIz8cZkizcQIbQCuH/+vW7hlmznkfXdYqLi9i1aweLFr3Fp5+u4S9/+RvNmzcHYMyYm/jVrx6ivLyczZs38uc//wlVVbnzzl/Uet7f//5xLly4wJNPziIxsR1FRWfYvftbSkoutzIcOLCfRx55gPbtO/LoozNJSuqAqqocPJjBypX/IiEhkd690+zHP/HE7xk4cBAVFRWcPJnD2rWr+fWv7+OBB2YwZcrdHv051UWShRvYhtBqmi5DaIXwU8HBwURFtQEgOjqGLl1SuOaa65g27U7efXcJ9977KwCaNWtmP+6WWyaxceNXbNz4Va3JorS0lL17d/Paawvo1+8aAOLi4unWrYf9GMMweOGFOSQmJvHmm4tQ1cut/4mJ7Rg2bDiGYVQ5b1hYmD2GuLh4rr66H23aRPPWWwsYMuQGEhPbuevH4jTps3AD2xDaCUM6SROUEE7yh36+Tp2SufbaAWzY8Hmdx4SEhGCx1D7ctHnz5jRvHso333zNxYsXaz3m0KEMsrKOMHnynVUSRWXOjFK6446p6LrO119vcHisJ0jNwk1kCK0QzvOnfr4OHTqxc+f2Gs/rus727VvZvn0rt902udb3ms1mnnlmNvPm/YH//vc/dOmSQq9eV3HDDcPp0aMnAMePZwOQlNTe/r6ysjImTPip/fFdd93D3XdPqzfOiIhWREa25uTJnAaX0R0kWQghvM6f+vkMw6jyzf6///0Pa9eupqKiAoBRo8Ywbdp97N27m8cee9h+3MyZTzNy5E8ZOvRGBgwYxHff7WbfvnS2bdvCe+8t4777HqwzAYSGhrJ48bsAPPbYI/ZrNTRWb5JkIYTwOn/q58vKOkrbtgn2xzfeOIJ77rmPoKAg2rSJto+CSk3tZr/BA7Ru3dr+95CQEPr3v47+/a/jnnum88c/zmXRooVMnnwX7dolAXDsWBZdu6YCoKqqvd8hKMi523BxcTHFxUVVYvUmSRZCCK/zl6VyjhzJZNu2zfz85/fan2vRIqzWDuSQkGZOdyx36NARTdMoL79Ily4pdOjQkXffXcKwYSPqHILryHvvLUNVVYYMGerS+xtLkoUQwie83c9XXl5OYeFpDMOgqKiIXbu2s3TpYlJSujF58l0unbOkpJhZs55k7Nif0blzF0JDQzlw4AfefXcpffv2p0WLMACefno2v/nNQ9x/v7Vvon37jui6Tnr6XgoK8mskkLKyMgoLT1NRYeHkyROsXbuades+4YEHHiYhIbHRPwtXSLJwUWZOic+/FQkhnLdz53bGjx+NyWQiLCyMjh07M23affzsZxPtE+IaqnnzUHr06MWHH77HiRPHqagoJzo6hhEjRlWprXTv3pNFi5axdOliXnvtFc6cKSQkJITOnbswffqD3HTTzVXOO2/eC8Dl4b7du/dk/vy3SEu72vUfQCMpRvUBvleIwsIydN0zRfOnkRy1iY4Op6Cg1Ndh+Ewgl9/TZT916hhxce0dH+gjsp+F4/LX9RmqqkJUVFid75N5Fi6obSSHEEJcySRZuEA2PRJCBBqv91kcPXqUJ598kuLiYlq1asW8efPo0KFDlWMWLFjAmjVrUFWVoKAgHn30UQYPHuztUOvkLyM5hBDCW7yeLGbPns2UKVMYP348q1at4tlnn2XJkiVVjunduzfTpk2jefPmHDhwgDvvvJNNmzbRrFkzb4dbJ5mxLYQIJF5thiosLGT//v2MGzcOgHHjxrF//37OnDlT5bjBgwfbV4BMSUnBMAyKi13bN1YIIUTjebVmkZubS2xsrH1MsclkIiYmhtzc3CqzIStbuXIlSUlJxMXFNeha9fXqB4Lo6HBfh+BTgVx+T5Y9P1/FbPbvrk5/j8/THJVfVVWX/o349TyL7du385e//IVFixY1+L2eHDrr7wJ56CgEdvk9XXZd1/16aKoMnXVcfl3Xa/034ldDZ+Pj48nLy0PTNAA0TSM/P5/4+Pgax+7evZuZM2eyYMECOnXq5M0wG8Ufll0WQgh382qyiIqKolu3bqxebd3ecPXq1XTr1q1GE9R3333Ho48+yuuvv06PHj1qO5Vfsk3W+/fXR3hlxW5JGEKIK4bXG/fmzJnDsmXLGDVqFMuWLeO5554DYPr06aSnpwPw3HPPceHCBZ599lnGjx/P+PHjycjI8HaoDSaT9YQQVyqv91l07tyZDz/8sMbzf//73+1//+ijj7wZkttUX3a5RfMgPtmSJXMxhPCxP/xhDmvXWls0TCYT4eEt6dixE0OH3sj48RMxm623wl//+j727PkWgKCgIGJj4xkzZhxTp/68ztVii4qK+Oc/32Lr1m8oLDxNWFg4nTp15s47f07//tfZj8vJOcHSpYvZsWMbZ84U0rJlBElJ7Rkz5iaGDx9lX59q0KB+zJ37R264YXiNa3377U4efvh+wLq7XvPmocTHx9OnTz9uv30ySUme227Vrzu4mxrbZL3N6bmUnC3n3c8OoumGX64fJUSg6dfvGmbNeh5d1ykuLmLXrh0sWvQWn366hr/85W/24fpjxtzEr371EOXl5WzevJE///lPqKpa6x7cAL///eNcuHCBJ5+cRWJiO4qKzrB797eUlFxuhj5wYD+PPPIA7dt35NFHZ5KU1AFVVTl4MIOVK/9FQkIivXunOV2WpUs/oGXLlpw/f57MzIN88MEKfv7zO3j11dfp3btPo35OdZFk4QHf7DtFRaURCb7eCUwIcXkFV4Do6Bi6dEnhmmuuY9q0O3n33SXce++vAGjWrJn9uFtumcTGjV+xceNXtSaL0tJS9u7dzWuvLaBfv2sAiIuLp1u3y32thmHwwgtzSExM4s03F1XZhzsxsR3Dhg2noeu5Rka2plWrVvZzDB48lBkzfsUf/vAc7777kct7ZtQnsAcke4Ct36IyWT9KiJq0vEwu7l6Nlpfpsxg6dUrm2msHsGHD53UeExISgsViqfW15s2b07x5KN988zUXL16s9ZhDhzLIyjrC5Ml3VkkUlTV2q1STycSkSVPIyTnBoUOe6d+VmoWbVe63UFSFwb3iGdgrXmoVQlSi5WVybvXLoFdQrgYROu5xTLHJPomlQ4dO7Ny5vcbzuq6zfftWtm/fym23Ta71vWazmWeemc28eX/gv//9D126pNCr11XccMNwevToCcDx49kAJCVdXha8rKyMCRN+an9811331Llfd0PKAZCTk0NqavdGnas2kizcTBYZFMIxy8kDoFeAYYBuwXLygM+ShWEYVb7Z//e//2Ht2tVUVFQAMGrUGKZNu4+9e3fz2GMP24+bOfNpRo78KUOH3siAAYP47rvd7NuXzrZtW3jvvWXcd9+DdSaA0NBQ+37ejz32iP1ajS0HNL6WUhdJFh4giwwKUT9z21TK1SDQLaCaMbdN9VksWVlHads2wf74xhtHcM899xEUFESbNtH29v/U1G72GzxQZX5YSEgI/ftfR//+13HPPdP54x/nsmjRQiZPvot27ZIAOHYsi65dreVUVdW+n3dQkHtuw1lZRwCqlMWdJFkIIbzOFJtM6LjHsZw8gLltqs9qFUeOZLJt2+YqW6C2aBFmv5FXFhLSrNbna9OhQ0c0TaO8/CJduqTQoUNH3n13CcOGjfBI57OmaXzwwQoSExPp0qWr288PkiyEED5iik32apIoLy+nsPA0hmFQVFTErl3bWbp0MSkp3Zg8+S6XzllSUsysWU8yduzP6Ny5C6GhoRw48APvvruUvn3706KFda2lp5+ezW9+8xD332/tm2jfviO6rpOevpeCgvwaCSQ3N7dGR3XlGkNR0Rk0zcL58+c5fPgQ77//LocOZfB///e6R5IRSLIQQgSInTu3M378aEwmE2FhYXTs2Jlp0+7jZz+baJ8Q11DNm4fSo0cvPvzwPU6cOE5FRTnR0TGMGDGqSm2le/eeLFq0jKVLF/Paa69w5kwhISEhdO7chenTH+Smm26uct433vhLjWvNm/eafS7IXXfdbr9+XFwcffv255ln5tC+fZLHFlJUjIYO8G0iZNXZwFx1FQK7/J4u+6lTx4iLa+/4QB+RVWcdl7+uz9CvVp0VQgjRNEmy8AJZtlwI0dRJn4WH2ZYtr7DoqKrCqP7tCG1mljkYQogmRZKFh2VkF9nXidJ1g7XbslGwti3K4oJCiKZCmqE8LCUpElWtOqPSQPa7EEI0LZIsPCw5IYI7R3bFVClhKIosLiiaKgXDCNzRRk2d9bNzbTkQaYbygqFpCSRGh5GRXUSL5kGcPV8hfRaiSQoObkZx8WnCwyMxmcweW4dIuJdhGGiahdLSIoKDm7l0DkkWXiLrRYkrQWRkNGVlJZw5k4eua74OpwZVVdH1wK351Fd+VTXRvHkYYWGu3YckWQghnKYoCuHhrQgPb+XrUGoVyBMywbPllz4LIYQQDkmyEEII4ZAkCyGEEA5JshBCCOHQFdvBXX0iXKCR8gdu+QO57CDld7X8jt53xS5RLoQQwn2kGUoIIYRDkiyEEEI4JMlCCCGEQ5IshBBCOCTJQgghhEOSLIQQQjgkyUIIIYRDkiyEEEI4JMlCCCGEQ5IshBBCONQkk8Xy5csZNmwYvXr1YuLEiezcubPe47dv387EiRPp1asXN954IytWrPBSpJ7RkPKvX7+eadOmcd1119GnTx9uu+02Pv/8cy9G614N/extdu7cSffu3Rk3bpyHI/Sshpa/vLycv/zlLwwbNoyePXsydOhQlixZ4qVo3a+h5f/4448ZP348V111Fddffz2PPfYYBQUFXorWfXbs2MH999/P4MGDSUlJ4d///rfD92RkZHDnnXfSu3dvBg8ezF//+lcatbqT0cR88sknRvfu3Y3333/fyMzMNJ5//nkjLS3NyMnJqfX47Oxs46qrrjKef/55IzMz03j//feN7t27G+vWrfNy5O7R0PLPnTvXeOutt4y9e/caWVlZxvz5843U1FRjx44dXo688Rpadpvi4mJj2LBhxrRp04yxY8d6KVr3c6X8Dz30kHHLLbcYmzZtMo4fP27s2bPH2Lp1qxejdp+Gln/nzp1GamqqsXjxYiM7O9vYvXu3cfPNNxt33323lyNvvA0bNhivvvqqsXbtWqN3797GRx99VO/xpaWlxsCBA42HH37YyMjIMNauXWukpaUZ//znP12Oockli1tvvdV45plnqjw3YsQI409/+lOtx7/88svGiBEjqjz39NNPG7fffrvHYvSkhpa/Nrfccovx0ksvuTs0j3O17A899JAxf/584/XXX2/SyaKh5d+4caNx9dVXG4WFhd4Iz+MaWv5//OMfxtChQ6s8969//ctIS0vzWIzekJaW5jBZLF++3OjTp49x/vx5+3MLFiwwBg0aZOi67tJ1m1QzVHl5Od9//z3XX399leevv/56du/eXet79uzZU+P4QYMGsW/fPioqKjwWqye4Uv7anD17lpYtW7o7PI9ytezLly/n9OnTPPDAA54O0aNcKf///vc/evXqxdtvv82QIUMYOXIkL7zwAmfPnvVGyG7lSvmvvvpqCgoK+OKLLzAMgzNnzrBmzRqGDBnijZB9as+ePfTr149mzZrZnxs0aBD5+fmcOHHCpXM2qWRRVFSEpmm0adOmyvNRUVF1tkOePn2aqKioKs+1adMGi8VCUVGRx2L1BFfKX93y5cs5deoU48eP90SIHuNK2TMyMliwYAGvvPIKJpPJG2F6jCvlP378OLt27eLAgQPMnz+fWbNmsXHjRp566ilvhOxWrpS/T58+/N///R+PPfYYPXv2ZMCAARiGwbx587wRsk/Vdd+zveaKJpUsRON8+umnvPzyy7z66qskJCT4OhyPKi8v59FHH+Xxxx+nXbt2vg7HJwzDQFEUXn31Va666ioGDx7MrFmz+PTTT12+YTQlmZmZzJ07lwcffJCPPvqIf/zjHxQUFPDss8/6OrQmqUntlBcZGYnJZKrxD72wsJDo6Oha39OmTRsKCwurPHf69GnMZjORkZEei9UTXCm/zbp163jiiSeYN28ew4YN82SYHtHQsufn53P48GGefvppnn76aQB0XccwDLp3787ChQsZNGiQV2J3B1c+++joaGJjYwkPD7c/17lzZwBOnjxZ41u6P3Ol/G+99Ra9e/fml7/8JQCpqak0b96cqVOn8tvf/pa4uDiPx+0rdd33bK+5oknVLIKDg+nRowebN2+u8vzmzZvp06dPre9JS0ur9fiePXsSFBTksVg9wZXyA6xZs4bHH3+cl156idGjR3s6TI9oaNljY2P5+OOPWblypf3PHXfcQfv27Vm5cmW9Py9/5Mpnf/XVV5Ofn1+ljyIrKwugydUsXSn/hQsXajQ/2h7ruu6ZQP1EWloaO3fu5OLFi/bnNm/eTExMDImJia6d1KVucR/65JNPjB49ehgffPCBkZmZacydO9dIS0szTpw4YRiGYcycOdOYOXOm/Xjb0NkXXnjByMzMND744AOjR48eTXrobEPKv3r1aqN79+7G22+/beTn59v/FBUV+aoILmto2atr6qOhGlr+srIyY8iQIcaMGTOMgwcPGjt37jTGjh1rzJgxw1dFaJSGlv+jjz4yunfvbixfvtzIzs42du7caUycONGYMGGCr4rgsrKyMmP//v3G/v37jd69exvz58839u/fbx82/Kc//anKkOAff/zRGDhwoPGb3/zGyMjIMD799FOjT58+jRo626SaoQDGjBlDUVERf/vb38jPz6dr164sXLjQ/k0pNze3yvHt2rVj4cKFvPTSS6xYsYKYmBieeeYZRo0a5YvwG62h5X/vvfewWCy8+OKLvPjii/bnr7nmGpYuXerV2BuroWW/0jS0/C1atGDx4sW88MIL3HrrrbRs2ZLhw4fzu9/9zhfhN1pDyz9x4kTOnj3L8uXLmTdvHuHh4Vx77bXMnDnTF+E3yr59+7j77rvtj+fPn8/8+fOZMGECf/zjHykoKOD48eP218PDw1m0aBHPP/88t9xyCxEREUybNo177rnH5RgUw2jMlD4hhBCBoEn1WQghhPANSRZCCCEckmQhhBDCIUkWQgghHJJkIYQQwiFJFkIIIRySZCGEEMIhSRZCCCEckmQhhBDCoSa33IcQTdHx48dZsGAB33zzDUVFRYSHh5OcnMyiRYua3IKWIjBJshDCw3788UcmT55M7969mTNnDhERERQXF5OXlyeJQjQZkiyE8LDdu3dTUFDAPnWlpAAAAQRJREFUmDFjGDhwIEFBQZjN8qsnmhZZSFAIDztz5gyTJ0+27yURERHB9u3bfRuUEA0kX2+E8LBz584xePBgfvGLX5CSkkLLli19HZIQDSY1CyE8qKysjJtuuolZs2Y1ye1shbCRobNCeNCOHTs4efIkHTt29HUoQjSKNEMJ4UGtW7cG4IknnuDee++lVatWnDp1iq1bt/LSSy/5ODohnCfNUEJ42KpVq3jnnXfIyspC0zQSEhIYN24cDz74oK9DE8JpkiyEEEI4JH0WQgghHJJkIYQQwiFJFkIIIRySZCGEEMIhSRZCCCEckmQhhBDCIUkWQgghHJJkIYQQwiFJFkIIIRz6f67GR69sqtzRAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}