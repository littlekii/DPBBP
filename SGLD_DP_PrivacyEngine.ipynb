{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SGLD_DP_PrivacyEngine.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/littlekii/DPBBP/blob/main/SGLD_DP_PrivacyEngine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYz1GdMWxUOA",
        "outputId": "7f5c06c5-447f-407e-cf3b-850bfa0fd721"
      },
      "source": [
        "!git clone https://github.com/JavierAntoran/Bayesian-Neural-Networks.git\n",
        "!git clone https://github.com/knamdar/data.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Bayesian-Neural-Networks'...\n",
            "remote: Enumerating objects: 369, done.\u001b[K\n",
            "remote: Total 369 (delta 0), reused 0 (delta 0), pack-reused 369\u001b[K\n",
            "Receiving objects: 100% (369/369), 15.98 MiB | 5.03 MiB/s, done.\n",
            "Resolving deltas: 100% (191/191), done.\n",
            "Cloning into 'data'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 16 (delta 2), reused 16 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (16/16), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvU02KZBy0eg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e5e9ee5-8830-4813-8aff-dc8b4ecf52da"
      },
      "source": [
        "!pip install opacus==0.13"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opacus==0.13\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/c4/36f07b54413e607f07858d5b668379991948f0de9979185a895fee9c2208/opacus-0.13.0-py3-none-any.whl (102kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 4.3MB/s \n",
            "\u001b[?25hCollecting requests>=2.25.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.40 in /usr/local/lib/python3.7/dist-packages (from opacus==0.13) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from opacus==0.13) (1.9.0+cu102)\n",
            "Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.7/dist-packages (from opacus==0.13) (0.10.0+cu102)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from opacus==0.13) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.7/dist-packages (from opacus==0.13) (1.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->opacus==0.13) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->opacus==0.13) (2021.5.30)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->opacus==0.13) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->opacus==0.13) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3->opacus==0.13) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.9->opacus==0.13) (7.1.2)\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: requests, opacus\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "Successfully installed opacus-0.13.0 requests-2.25.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWemUlvRxSxU"
      },
      "source": [
        "#### Base imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5xiHDAMxSxb"
      },
      "source": [
        "%matplotlib inline\n",
        "from __future__ import print_function, division\n",
        "import numpy as np\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "import torch.utils.data as data\n",
        "from torch.optim import Optimizer\n",
        "from opacus import PrivacyEngine\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import collections\n",
        "import h5py, sys\n",
        "import gzip\n",
        "import os\n",
        "import math\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y99_HtZKxSxd"
      },
      "source": [
        "## Some utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSCd9YcexSxd"
      },
      "source": [
        "def mkdir(paths):\n",
        "    if not isinstance(paths, (list, tuple)):\n",
        "        paths = [paths]\n",
        "    for path in paths:\n",
        "        if not os.path.isdir(path):\n",
        "            os.makedirs(path)\n",
        "\n",
        "\n",
        "suffixes = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']\n",
        "def humansize(nbytes):\n",
        "    i = 0\n",
        "    while nbytes >= 1024 and i < len(suffixes)-1:\n",
        "        nbytes /= 1024.\n",
        "        i += 1\n",
        "    f = ('%.2f' % nbytes)\n",
        "    return '%s%s' % (f, suffixes[i])\n",
        "\n",
        "def to_variable(var=(), cuda=True, volatile=False):\n",
        "    out = []\n",
        "    for v in var:\n",
        "        if isinstance(v, np.ndarray):\n",
        "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
        "\n",
        "        if not v.is_cuda and cuda:\n",
        "            v = v.cuda()\n",
        "\n",
        "        if not isinstance(v, Variable):\n",
        "            v = Variable(v, volatile=volatile)\n",
        "\n",
        "        out.append(v)\n",
        "    return out\n",
        "  \n",
        "def cprint(color, text, **kwargs):\n",
        "    if color[0] == '*':\n",
        "        pre_code = '1;'\n",
        "        color = color[1:]\n",
        "    else:\n",
        "        pre_code = ''\n",
        "    code = {\n",
        "        'a': '30',\n",
        "        'r': '31',\n",
        "        'g': '32',\n",
        "        'y': '33',\n",
        "        'b': '34',\n",
        "        'p': '35',\n",
        "        'c': '36',\n",
        "        'w': '37'\n",
        "    }\n",
        "    print(\"\\x1b[%s%sm%s\\x1b[0m\" % (pre_code, code[color], text), **kwargs)\n",
        "    sys.stdout.flush()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVX03SMqxSxh"
      },
      "source": [
        "## Base network wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfam0Wp8xSxi"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "class BaseNet(object):\n",
        "    def __init__(self):\n",
        "        cprint('c', '\\nNet:')\n",
        "\n",
        "    def get_nb_parameters(self):\n",
        "        return np.sum(p.numel() for p in self.model.parameters())\n",
        "\n",
        "    def set_mode_train(self, train=True):\n",
        "        if train:\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "\n",
        "    def update_lr(self, epoch, gamma=0.99):\n",
        "        self.epoch += 1\n",
        "        if self.schedule is not None:\n",
        "            if len(self.schedule) == 0 or epoch in self.schedule:\n",
        "                self.lr *= gamma\n",
        "                print('learning rate: %f  (%d)\\n' % self.lr, epoch)\n",
        "                for param_group in self.optimizer.param_groups:\n",
        "                    param_group['lr'] = self.lr\n",
        "\n",
        "    def save(self, filename):\n",
        "        cprint('c', 'Writting %s\\n' % filename)\n",
        "        torch.save({\n",
        "            'epoch': self.epoch,\n",
        "            'lr': self.lr,\n",
        "            'model': self.model,\n",
        "            'optimizer': self.optimizer}, filename)\n",
        "\n",
        "    def load(self, filename):\n",
        "        cprint('c', 'Reading %s\\n' % filename)\n",
        "        state_dict = torch.load(filename)\n",
        "        self.epoch = state_dict['epoch']\n",
        "        self.lr = state_dict['lr']\n",
        "        self.model = state_dict['model']\n",
        "        self.optimizer = state_dict['optimizer']\n",
        "        print('  restoring epoch: %d, lr: %f' % (self.epoch, self.lr))\n",
        "        return self.epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFdrPqN-xSxj"
      },
      "source": [
        "## Bayesian Linear 2 Layers MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yJOpqL0xSxj"
      },
      "source": [
        "class Linear_2L(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Linear_2L, self).__init__()\n",
        "        \n",
        "        n_hid = 1200\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        self.fc1 = nn.Linear(input_dim, n_hid)\n",
        "        self.fc2 = nn.Linear(n_hid, n_hid)\n",
        "        self.fc3 = nn.Linear(n_hid, output_dim)\n",
        "        \n",
        "        # choose your non linearity\n",
        "        #self.act = nn.Tanh()\n",
        "        #self.act = nn.Sigmoid()\n",
        "        self.act = nn.ReLU(inplace=True)\n",
        "        #self.act = nn.ELU(inplace=True)\n",
        "        #self.act = nn.SELU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.view(-1, self.input_dim) # view(batch_size, input_dim)\n",
        "        # -----------------\n",
        "        x = self.fc1(x)\n",
        "        # -----------------\n",
        "        x = self.act(x)\n",
        "        # -----------------\n",
        "        x = self.fc2(x)\n",
        "        # -----------------\n",
        "        x = self.act(x)\n",
        "        # -----------------\n",
        "        y = self.fc3(x)\n",
        "\n",
        "        return y\n",
        "    \n",
        "    \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swRDeyBmcu08"
      },
      "source": [
        "## CNN model for DP DNN MNIST (state of art) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZF9m1YCxSxk"
      },
      "source": [
        "class SampleConvNet_MNIST(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 8, 2, padding=3)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 4, 2)\n",
        "        self.fc1 = nn.Linear(32 * 4 * 4, 32)\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x of shape [B, 1, 28, 28]\n",
        "        x = F.relu(self.conv1(x))  # -> [B, 16, 14, 14]\n",
        "        x = F.max_pool2d(x, 2, 1)  # -> [B, 16, 13, 13]\n",
        "        x = F.relu(self.conv2(x))  # -> [B, 32, 5, 5]\n",
        "        x = F.max_pool2d(x, 2, 1)  # -> [B, 32, 4, 4]\n",
        "        x = x.view(-1, 32 * 4 * 4)  # -> [B, 512]\n",
        "        x = F.relu(self.fc1(x))  # -> [B, 32]\n",
        "        x = self.fc2(x)  # -> [B, 10]\n",
        "        return x\n",
        "\n",
        "    def name(self):\n",
        "        return \"SampleConvNet\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfsNtEaUxSxl"
      },
      "source": [
        "# Custom SGLD optimiser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdZWRc54xSxl"
      },
      "source": [
        "from torch.optim.optimizer import Optimizer, required\n",
        "class SGLD(Optimizer):\n",
        "    \"\"\"\n",
        "    SGLD optimiser based on pytorch's SGD. \n",
        "    Note that the weight decay is specified in terms of the gaussian prior sigma\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=required, norm_sigma=0,laplace_b=0, addnoise=True):\n",
        "        \n",
        "        if lr is not required and lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        \n",
        "        defaults = dict(lr=lr, norm_sigma=norm_sigma, laplace_b=laplace_b,addnoise=addnoise)\n",
        "        \n",
        "        super(SGLD, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self,closure=0):\n",
        "        \"\"\"\n",
        "        Performs a single optimization step.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        \n",
        "        for group in self.param_groups:\n",
        "\n",
        "            norm_sigma = group['norm_sigma']\n",
        "            laplace_b=group['laplace_b']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad.data\n",
        "                if norm_sigma != 0:\n",
        "                    p.data.add_(-group['lr']/norm_sigma**2, p.data)\n",
        "                if laplace_b != 0:\n",
        "                    p.data.add_(-group['lr']/laplace_b,torch.sign(p.data))\n",
        "\n",
        "                if group['addnoise']:  \n",
        "                    langevin_noise = p.data.new(p.data.size()).normal_(mean=0, std=1)/np.sqrt(group['lr'])\n",
        "                    p.data.add_(-group['lr'], d_p  + langevin_noise)\n",
        "                else:\n",
        "                    p.data.add_(-group['lr'], d_p )\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9EWoFTaxSxm"
      },
      "source": [
        "# pSGLD optimiser\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHtuVlUWxSxm"
      },
      "source": [
        "class pSGLD(Optimizer):\n",
        "    \"\"\"\n",
        "    RMSprop preconditioned SGLD using pytorch rmsprop implementation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=required, norm_sigma=0,laplace_b=0, alpha=0.99, eps=1e-8, centered=False, addnoise=True):\n",
        "        \n",
        "        if lr is not required and lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        defaults = dict(lr=lr, norm_sigma=norm_sigma,laplace_b=laplace_b, alpha=alpha, eps=eps, centered=centered, addnoise=addnoise)\n",
        "        super(pSGLD, self).__init__(params, defaults)\n",
        "        \n",
        "    def __setstate__(self, state):\n",
        "        super(pSGLD, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('centered', False)\n",
        "\n",
        "    def step(self,closure=0):\n",
        "        \"\"\"\n",
        "        Performs a single optimization step.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            norm_sigma = group['norm_sigma']\n",
        "            laplace_b=group['laplace_b']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad.data\n",
        "\n",
        "\n",
        "\n",
        "                state = self.state[p]\n",
        "                \n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['square_avg'] = torch.zeros_like(p.data)\n",
        "                    if group['centered']:\n",
        "                        state['grad_avg'] = torch.zeros_like(p.data)\n",
        "                        \n",
        "                square_avg = state['square_avg']\n",
        "                alpha = group['alpha']\n",
        "                state['step'] += 1\n",
        "                \n",
        "                if norm_sigma != 0:\n",
        "                    p.data.add_(-group['lr']/norm_sigma**2, p.data)\n",
        "                if laplace_b != 0:\n",
        "                    p.data.add_(-group['lr']/laplace_b,torch.sign(p.data))\n",
        "\n",
        "                # sqavg x alpha + (1-alph) sqavg *(elemwise) sqavg\n",
        "                square_avg.mul_(alpha).addcmul_(1-alpha, d_p, d_p)\n",
        "                \n",
        "                if group['centered']:\n",
        "                    grad_avg = state['grad_avg']\n",
        "                    grad_avg.mul_(alpha).add_(1-alpha, d_p)\n",
        "                    avg = square_avg.cmul(-1, grad_avg, grad_avg).sqrt().add_(group['eps'])\n",
        "                else:\n",
        "                    avg = square_avg.sqrt().add_(group['eps'])\n",
        "                    \n",
        "#                 print(avg.shape)\n",
        "                if group['addnoise']:\n",
        "                    langevin_noise = p.data.new(p.data.size()).normal_(mean=0, std=1)/np.sqrt(group['lr'])\n",
        "                    p.data.add_(-group['lr'],\n",
        "                                0.5*d_p.div_(avg) + langevin_noise/torch.sqrt(avg))\n",
        "                    \n",
        "                else:\n",
        "                    p.data.addcdiv_(-group['lr'], 0.5*d_p, avg)\n",
        "\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp-ulJ9CxSxn"
      },
      "source": [
        "# Network wrapper "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8b4qXzNxSxn"
      },
      "source": [
        "class Net_langevin(BaseNet):\n",
        "    eps = 1e-6\n",
        "\n",
        "    def __init__(self, lr=1e-3, cuda=True, classes=10, N_train=60000, norm_sigma=0,laplace_b=0,DP_or_Not=True,which_net=None,pSGLD_or_Not=False):\n",
        "        super(Net_langevin, self).__init__()\n",
        "        cprint('y', ' Creating Net!! ')\n",
        "        self.lr = lr\n",
        "        self.schedule = None  # [] #[50,200,400,600]\n",
        "        self.cuda = cuda\n",
        "        self.norm_sigma = norm_sigma\n",
        "        self.laplace_b=laplace_b\n",
        "        self.classes = classes\n",
        "        self.N_train = N_train\n",
        "\n",
        "        self.which_net=which_net\n",
        "        self.create_net()\n",
        "        self.pSGLD_or_Not=pSGLD_or_Not\n",
        "        self.create_opt()\n",
        "        self.epoch = 0\n",
        "        self.weight_set_samples = []\n",
        "\n",
        "        self.test=False\n",
        "        self.DP_or_Not=DP_or_Not\n",
        "\n",
        "    def create_net(self):\n",
        "        torch.manual_seed(42)\n",
        "        if self.cuda:\n",
        "            torch.cuda.manual_seed(42)\n",
        "\n",
        "        self.model = self.which_net\n",
        "        if self.cuda:\n",
        "            self.model.cuda()\n",
        "        print('    Total params: %.2fM' % (self.get_nb_parameters() / 1000000.0))\n",
        "    \n",
        "    def create_opt(self):\n",
        "        if self.pSGLD_or_Not:\n",
        "            self.optimizer = pSGLD(params=self.model.parameters(), lr=self.lr, norm_sigma=self.norm_sigma,laplace_b=self.laplace_b, addnoise=1-DP_or_Not)\n",
        "        else:\n",
        "            self.optimizer = SGLD(params=self.model.parameters(), lr=self.lr, norm_sigma=self.norm_sigma,laplace_b=self.laplace_b, addnoise=1-DP_or_Not)\n",
        "        if DP_or_Not:\n",
        "            privacy_engine = PrivacyEngine(\n",
        "                    self.model,\n",
        "                    sample_rate=batch_size/self.N_train,\n",
        "                    alphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64)),\n",
        "                    noise_multiplier=batch_size/np.sqrt(self.lr)/self.N_train/clipping_norm,\n",
        "                    max_grad_norm=clipping_norm * 60000)\n",
        "            privacy_engine.attach(self.optimizer)\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        x, y = x.to(device), y.long().to(device)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        out = self.model(x)\n",
        "        loss = F.cross_entropy(out, y, reduction='mean') # We use mean because we treat as an estimation of whole dataset\n",
        "        loss = loss * self.N_train \n",
        "            \n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # out: (batch_size, out_channels, out_caps_dims)\n",
        "        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
        "        err = pred.ne(y.data).sum()\n",
        "\n",
        "        return loss.data*x.shape[0]/self.N_train, err\n",
        "\n",
        "\n",
        "    def eval(self, x, y, train=False):\n",
        "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
        "\n",
        "        out = self.model(x)\n",
        "\n",
        "        loss = F.cross_entropy(out, y, reduction='sum')\n",
        "\n",
        "        probs = F.softmax(out, dim=1).data.cpu()\n",
        "\n",
        "        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
        "        err = pred.ne(y.data).sum()\n",
        "\n",
        "        return loss.data, err, probs\n",
        "    \n",
        "    def save_sampled_net(self, max_samples):\n",
        "        \n",
        "        if len(self.weight_set_samples) >= max_samples:\n",
        "            self.weight_set_samples.pop(0)\n",
        "            \n",
        "        self.weight_set_samples.append(copy.deepcopy(self.model.state_dict()))\n",
        "        \n",
        "        cprint('c', ' saving weight samples %d/%d' % (len(self.weight_set_samples), max_samples) )\n",
        "        \n",
        "        return None\n",
        "        \n",
        "    def sample_eval(self, x, y, Nsamples=0, logits=True, train=False):\n",
        "        if Nsamples == 0:\n",
        "            Nsamples = len(self.weight_set_samples)\n",
        "            \n",
        "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
        "        \n",
        "        out = x.data.new(Nsamples, x.shape[0], self.classes)\n",
        "        \n",
        "        # iterate over all saved weight configuration samples\n",
        "        for idx, weight_dict in enumerate(self.weight_set_samples):\n",
        "            if idx == Nsamples:\n",
        "                break\n",
        "            self.model.load_state_dict(weight_dict)\n",
        "            out[idx] = self.model(x)\n",
        "        \n",
        "        \n",
        "        if logits:\n",
        "            mean_out = out.mean(dim=0, keepdim=False)\n",
        "            loss = F.cross_entropy(mean_out, y, reduction='sum')\n",
        "            probs = F.softmax(mean_out, dim=1).data.cpu()\n",
        "            \n",
        "        else:\n",
        "            mean_out =  F.softmax(out, dim=2).mean(dim=0, keepdim=False)\n",
        "            probs = mean_out.data.cpu()\n",
        "            \n",
        "            log_mean_probs_out = torch.log(mean_out)\n",
        "            loss = F.nll_loss(log_mean_probs_out, y, reduction='sum')\n",
        "\n",
        "        pred = mean_out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
        "        err = pred.ne(y.data).sum()\n",
        "\n",
        "        return loss.data, err, probs\n",
        "    \n",
        "    def all_sample_eval(self, x, y, Nsamples):\n",
        "        if Nsamples == 0:\n",
        "            Nsamples = len(self.weight_set_samples)\n",
        "            \n",
        "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
        "        \n",
        "        out = x.data.new(Nsamples, x.shape[0], self.classes)\n",
        "        \n",
        "        # iterate over all saved weight configuration samples\n",
        "        for idx, weight_dict in enumerate(self.weight_set_samples):\n",
        "            if idx == Nsamples:\n",
        "                break\n",
        "            self.model.load_state_dict(weight_dict)\n",
        "            out[idx] = self.model(x)\n",
        "        \n",
        "        prob_out =  F.softmax(out, dim=2)\n",
        "        prob_out = prob_out.data\n",
        "\n",
        "        return prob_out\n",
        "    \n",
        "    def get_weight_samples(self, Nsamples=0):\n",
        "        weight_vec = []\n",
        "        \n",
        "        if Nsamples == 0 or Nsamples > len(self.weight_set_samples):\n",
        "            Nsamples = len(self.weight_set_samples)\n",
        "            \n",
        "        for idx, state_dict in enumerate(self.weight_set_samples):\n",
        "            if idx == Nsamples:\n",
        "                break\n",
        "                \n",
        "            for key in state_dict.keys():\n",
        "                if 'weight' in key:\n",
        "                    weight_mtx = state_dict[key].cpu().data\n",
        "                    for weight in weight_mtx.view(-1):\n",
        "                        weight_vec.append(weight)\n",
        "            \n",
        "        return np.array(weight_vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9n4W83rmZf_"
      },
      "source": [
        "import time\n",
        "import torch.utils.data\n",
        "from torchvision import transforms, datasets\n",
        "import matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQti1THk9yfb"
      },
      "source": [
        "# Training Bayesian Linear 2 Layers MNIST (Laplacian prior) DP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgFVPHZp9w4e",
        "outputId": "d5dff4ea-1665-417a-ad68-4f0c57e10dd5"
      },
      "source": [
        "models_dir = 'models_SGLD_MNIST_laplacian'\n",
        "results_dir = 'results_SGLD_MNIST_laplacian'\n",
        "\n",
        "mkdir(models_dir)\n",
        "mkdir(results_dir)\n",
        "\n",
        "save_dicts = []\n",
        "\n",
        "# ------------------------------------------------------------------------------------------------------\n",
        "# train config\n",
        "NTrainPointsMNIST = 60000\n",
        "batch_size = 256\n",
        "nb_epochs = 15 # We can do less iterations as this method has faster convergence\n",
        "log_interval = 1\n",
        "\n",
        "lr = 1e-5\n",
        "norm_sigma = 0\n",
        "laplace_b=0.1\n",
        "\n",
        "noise_multiplier = 1.3\n",
        "clipping_norm=1.5\n",
        "DP_or_Not=True\n",
        "\n",
        "# ------------------------------------------------------------------------------------------------------\n",
        "# dataset\n",
        "cprint('c', '\\nData:')\n",
        " \n",
        "\n",
        "# load data\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
        "])\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "trainset = datasets.MNIST(root='/content/data', train=True, download=True, transform=transform_train)\n",
        "valset = datasets.MNIST(root='/content/data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "if use_cuda:\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=3)\n",
        "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=3)\n",
        "\n",
        "else:\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n",
        "                                              num_workers=3)\n",
        "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
        "                                            num_workers=3)\n",
        "\n",
        "\n",
        "## ---------------------------------------------------------------------------------------------------------------------\n",
        "# net dims\n",
        "cprint('c', '\\nNetwork:')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[36m\n",
            "Data:\u001b[0m\n",
            "\u001b[36m\n",
            "Network:\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWNqilpuZnrh"
      },
      "source": [
        "########################################################################################\n",
        "net = Net_langevin(lr=lr, cuda=use_cuda, classes=10, N_train=NTrainPointsMNIST, norm_sigma=norm_sigma,laplace_b=laplace_b,which_net=Linear_2L(28*28,10))\n",
        "\n",
        "\n",
        "## weight saving parameters #######\n",
        "start_save = 15\n",
        "save_every = 2 # We want less correlated samples -> despite having per minibatch noise we see correlations\n",
        "N_saves = 90\n",
        "###################################\n",
        "\n",
        "\n",
        "## ---------------------------------------------------------------------------------------------------------------------\n",
        "# train\n",
        "cprint('c', '\\nTrain:')\n",
        "\n",
        "print('  init cost variables:')\n",
        "pred_cost_train = np.zeros(nb_epochs)\n",
        "err_train = np.zeros(nb_epochs)\n",
        "\n",
        "cost_dev = np.zeros(nb_epochs)\n",
        "err_dev = np.zeros(nb_epochs)\n",
        "# best_cost = np.inf\n",
        "best_err = np.inf\n",
        "\n",
        "\n",
        "nb_its_dev = 1\n",
        "\n",
        "tic0 = time.time()\n",
        "from tqdm import tqdm\n",
        "\n",
        "for i in range(nb_epochs):\n",
        "    \n",
        "#     if i in [1]:\n",
        "#         print('updating lr')\n",
        "#         net.sched.step()\n",
        "    \n",
        "    net.set_mode_train(True)\n",
        "\n",
        "    tic = time.time()\n",
        "    nb_samples = 0\n",
        "\n",
        "    for x, y in tqdm(trainloader):\n",
        "        cost_pred, err = net.fit(x, y)\n",
        "\n",
        "        err_train[i] += err\n",
        "        pred_cost_train[i] += torch.mean(cost_pred)\n",
        "        nb_samples += len(x)\n",
        "\n",
        "    pred_cost_train[i] /= nb_samples\n",
        "    err_train[i] /= nb_samples\n",
        "\n",
        "    toc = time.time()\n",
        "    net.epoch = i\n",
        "    # ---- print\n",
        "    print(\"it %d/%d, Jtr_pred = %f, err = %f, \" % (i, nb_epochs, pred_cost_train[i], err_train[i]), end=\"\")\n",
        "    cprint('r', '   time: %f seconds\\n' % (toc - tic))\n",
        "    \n",
        "    # ---- save weights\n",
        "    if i >= start_save and i % save_every == 0:\n",
        "        net.save_sampled_net(max_samples=N_saves)\n",
        "        save_dicts.append(copy.deepcopy(net.model.state_dict()))\n",
        "\n",
        "    # ---- dev\n",
        "    if i % nb_its_dev == 0:\n",
        "        net.set_mode_train(False)\n",
        "        nb_samples = 0\n",
        "        for j, (x, y) in enumerate(valloader):\n",
        "\n",
        "            cost, err, probs = net.eval(x, y)\n",
        "\n",
        "            cost_dev[i] += cost\n",
        "            err_dev[i] += err\n",
        "            nb_samples += len(x)\n",
        "\n",
        "        cost_dev[i] /= nb_samples\n",
        "        err_dev[i] /= nb_samples\n",
        "\n",
        "        cprint('g', '    Jdev = %f, err = %f\\n' % (cost_dev[i], err_dev[i]))\n",
        "\n",
        "        if err_dev[i] < best_err:\n",
        "            best_err = err_dev[i]\n",
        "            cprint('b', 'best test error')\n",
        "            net.save(models_dir+'/theta_best.dat')\n",
        "   \n",
        "toc0 = time.time()\n",
        "runtime_per_it = (toc0 - tic0) / float(nb_epochs)\n",
        "cprint('r', '   average time: %f seconds\\n' % runtime_per_it)\n",
        "\n",
        "net.save(models_dir+'/theta_last.dat')\n",
        "\n",
        "## ---------------------------------------------------------------------------------------------------------------------\n",
        "# results\n",
        "cprint('c', '\\nRESULTS:')\n",
        "nb_parameters = net.get_nb_parameters()\n",
        "best_cost_dev = np.min(cost_dev)\n",
        "best_cost_train = np.min(pred_cost_train)\n",
        "err_dev_min = err_dev[::nb_its_dev].min()\n",
        "\n",
        "print('  cost_dev: %f (cost_train %f)' % (best_cost_dev, best_cost_train))\n",
        "print('  err_dev: %f' % (err_dev_min))\n",
        "print('  nb_parameters: %d (%s)' % (nb_parameters, humansize(nb_parameters)))\n",
        "print('  time_per_it: %fs\\n' % (runtime_per_it))\n",
        "\n",
        "\n",
        "\n",
        "## Save results for plots\n",
        "# np.save('results/test_predictions.npy', test_predictions)\n",
        "np.save(results_dir + '/cost_train.npy', pred_cost_train)\n",
        "np.save(results_dir + '/cost_dev.npy', cost_dev)\n",
        "np.save(results_dir + '/err_train.npy', err_train)\n",
        "np.save(results_dir + '/err_dev.npy', err_dev)\n",
        "np.save(results_dir + '/save_dicts.npy', save_dicts)\n",
        " \n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu_TwffIDB_r"
      },
      "source": [
        "# Prediction Uncertainty "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV6L_wGFkJqs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "816c850e-0a54-4220-8cbb-dadd0f4d22d8"
      },
      "source": [
        "target_index = 195\n",
        "valloader_plot = torch.utils.data.DataLoader(valset, batch_size=1, shuffle=False)\n",
        "examples = enumerate(valloader)\n",
        "batch_idx, (x, y) = next(examples)\n",
        "for index, (x,y) in enumerate(valloader_plot):\n",
        "    if index == target_index:\n",
        "       break\n",
        "\n",
        "prob_list = [[],[],[],[],[],[],[],[],[],[]]\n",
        "for para in save_dicts_non_DP:\n",
        "    network.load_state_dict(para)\n",
        "    output = network(x)\n",
        "    probs = (F.softmax(output, dim=1).data.cpu())\n",
        "    k = 0\n",
        "    for l in prob_list:\n",
        "        l.append((probs[0])[k].numpy())\n",
        "        k += 1\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
        "\n",
        "plt.hist(prob_list,bins=5, range=[0, 1])\n",
        "\n",
        "#plt.legend(['0','1','2','3','4','5','6','7','8','9'],loc='upper right')\n",
        "plt.savefig('ndp_sgld_hist3.pdf',bbox_inches = 'tight')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#np.random.seed(19680801)\n",
        "\n",
        "#n_bins = 10\n",
        "#x = np.random.randn(1000, 3)\n",
        "\n",
        "#fig, ((ax0, ax1), (ax2, ax3)) = plt.subplots(nrows=2, ncols=2)\n",
        "\n",
        "#colors = ['red', 'tan', 'lime']\n",
        "#ax0.hist(x, n_bins, density=True, histtype='bar', color=colors, label=colors)\n",
        "#ax0.legend(prop={'size': 10})\n",
        "#ax0.set_title('bars with legend')\n",
        "\n",
        "prob_list = [[],[],[],[],[],[],[],[],[],[]]\n",
        "for para in save_dicts_DP:\n",
        "    network.load_state_dict(para)\n",
        "    output = network(x)\n",
        "    probs = (F.softmax(output, dim=1).data.cpu())\n",
        "    k = 0\n",
        "    for l in prob_list:\n",
        "        l.append((probs[0])[k].numpy())\n",
        "        k += 1\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
        "plt.hist(prob_list,bins=5, range=[0, 1])\n",
        "#plt.legend(['0','1','2','3','4','5','6','7','8','9'],loc='upper right')\n",
        "plt.savefig('dp_sgld_hist3.pdf',bbox_inches = 'tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXC79W45_tad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "36dd4268-cd60-49a3-baaf-c4343ddb7f56"
      },
      "source": [
        "target_index = 404\n",
        "valloader_plot = torch.utils.data.DataLoader(valset, batch_size=1, shuffle=False)\n",
        "examples = enumerate(valloader)\n",
        "batch_idx, (x, y) = next(examples)\n",
        "for index, (x,y) in enumerate(valloader_plot):\n",
        "    if index == target_index:\n",
        "       break\n",
        "\n",
        "prob_list = [[],[],[],[],[],[],[],[],[],[]]\n",
        "for para in save_dicts_non_DP:\n",
        "    network.load_state_dict(para)\n",
        "    output = network(x)\n",
        "    probs = (F.softmax(output, dim=1).data.cpu())\n",
        "    k = 0\n",
        "    for l in prob_list:\n",
        "        l.append((probs[0])[k].numpy())\n",
        "        k += 1\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
        "plt.hist(prob_list,bins=5, range=[0, 1])\n",
        "\n",
        "#plt.legend(['0','1','2','3','4','5','6','7','8','9'],loc='upper right')\n",
        "plt.savefig('ndp_sgld_hist2.pdf',bbox_inches = 'tight')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#np.random.seed(19680801)\n",
        "\n",
        "#n_bins = 10\n",
        "#x = np.random.randn(1000, 3)\n",
        "\n",
        "#fig, ((ax0, ax1), (ax2, ax3)) = plt.subplots(nrows=2, ncols=2)\n",
        "\n",
        "#colors = ['red', 'tan', 'lime']\n",
        "#ax0.hist(x, n_bins, density=True, histtype='bar', color=colors, label=colors)\n",
        "#ax0.legend(prop={'size': 10})\n",
        "#ax0.set_title('bars with legend')\n",
        "\n",
        "prob_list = [[],[],[],[],[],[],[],[],[],[]]\n",
        "for para in save_dicts_DP:\n",
        "    network.load_state_dict(para)\n",
        "    output = network(x)\n",
        "    probs = (F.softmax(output, dim=1).data.cpu())\n",
        "    k = 0\n",
        "    for l in prob_list:\n",
        "        l.append((probs[0])[k].numpy())\n",
        "        k += 1\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(prob_list,bins=5, range=[0, 1])\n",
        "#plt.legend(['0','1','2','3','4','5','6','7','8','9'],loc='upper right')\n",
        "plt.savefig('dp_sgld_hist2.pdf',bbox_inches = 'tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzdECDpFZoGy"
      },
      "source": [
        "net = Net_langevin(lr=lr, cuda=use_cuda, classes=10, N_train=NTrainPointsMNIST, norm_sigma=norm_sigma,laplace_b=laplace_b,which_net=Linear_2L(28*28,10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHvF-MHeRSTZ"
      },
      "source": [
        "# Training Bayesian Linear 2 Layers MNIST (Laplacian prior) non-DP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYlJNTbBQbta",
        "outputId": "8a66968a-298b-4158-d790-d7e995485c52"
      },
      "source": [
        "models_dir = 'models_SGLD_MNIST_ndplaplacian'\n",
        "results_dir = 'results_SGLD_MNIST_ndplaplacian'\n",
        "\n",
        "mkdir(models_dir)\n",
        "mkdir(results_dir)\n",
        "\n",
        "save_dicts = []\n",
        "\n",
        "# ------------------------------------------------------------------------------------------------------\n",
        "# train config\n",
        "NTrainPointsMNIST = 60000\n",
        "batch_size = 256\n",
        "nb_epochs = 15 # We can do less iterations as this method has faster convergence\n",
        "log_interval = 1\n",
        "\n",
        "lr = 1e-5\n",
        "norm_sigma = 0\n",
        "laplace_b=0.1\n",
        "\n",
        "noise_multiplier = 1.3\n",
        "clipping_norm=1.5\n",
        "DP_or_Not=True\n",
        "\n",
        "# ------------------------------------------------------------------------------------------------------\n",
        "# dataset\n",
        "cprint('c', '\\nData:')\n",
        " \n",
        "\n",
        "# load data\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
        "])\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "trainset = datasets.MNIST(root='/content/data', train=True, download=True, transform=transform_train)\n",
        "valset = datasets.MNIST(root='/content/data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "if use_cuda:\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=3)\n",
        "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=3)\n",
        "\n",
        "else:\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n",
        "                                              num_workers=3)\n",
        "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
        "                                            num_workers=3)\n",
        "\n",
        "\n",
        "## ---------------------------------------------------------------------------------------------------------------------\n",
        "# net dims\n",
        "cprint('c', '\\nNetwork:')\n",
        "\n",
        "\n",
        "\n",
        "########################################################################################\n",
        "net = Net_langevin(lr=lr, cuda=use_cuda, classes=10,DP_or_Not=False, N_train=NTrainPointsMNIST, norm_sigma=norm_sigma,laplace_b=laplace_b,which_net=Linear_2L(28*28,10))\n",
        "\n",
        "\n",
        "## weight saving parameters #######\n",
        "start_save = 15\n",
        "save_every = 2 # We want less correlated samples -> despite having per minibatch noise we see correlations\n",
        "N_saves = 90\n",
        "###################################\n",
        "\n",
        "\n",
        "## ---------------------------------------------------------------------------------------------------------------------\n",
        "# train\n",
        "cprint('c', '\\nTrain:')\n",
        "\n",
        "print('  init cost variables:')\n",
        "pred_cost_train = np.zeros(nb_epochs)\n",
        "err_train = np.zeros(nb_epochs)\n",
        "\n",
        "cost_dev = np.zeros(nb_epochs)\n",
        "err_dev = np.zeros(nb_epochs)\n",
        "# best_cost = np.inf\n",
        "best_err = np.inf\n",
        "\n",
        "\n",
        "nb_its_dev = 1\n",
        "\n",
        "tic0 = time.time()\n",
        "from tqdm import tqdm\n",
        "\n",
        "for i in range(nb_epochs):\n",
        "    \n",
        "#     if i in [1]:\n",
        "#         print('updating lr')\n",
        "#         net.sched.step()\n",
        "    \n",
        "    net.set_mode_train(True)\n",
        "\n",
        "    tic = time.time()\n",
        "    nb_samples = 0\n",
        "\n",
        "    for x, y in tqdm(trainloader):\n",
        "        cost_pred, err = net.fit(x, y)\n",
        "\n",
        "        err_train[i] += err\n",
        "        pred_cost_train[i] += torch.mean(cost_pred)\n",
        "        nb_samples += len(x)\n",
        "\n",
        "    pred_cost_train[i] /= nb_samples\n",
        "    err_train[i] /= nb_samples\n",
        "\n",
        "    toc = time.time()\n",
        "    net.epoch = i\n",
        "    # ---- print\n",
        "    print(\"it %d/%d, Jtr_pred = %f, err = %f, \" % (i, nb_epochs, pred_cost_train[i], err_train[i]), end=\"\")\n",
        "    cprint('r', '   time: %f seconds\\n' % (toc - tic))\n",
        "    \n",
        "    # ---- save weights\n",
        "    if i >= start_save and i % save_every == 0:\n",
        "        net.save_sampled_net(max_samples=N_saves)\n",
        "        save_dicts.append(copy.deepcopy(net.model.state_dict()))\n",
        "\n",
        "    # ---- dev\n",
        "    if i % nb_its_dev == 0:\n",
        "        net.set_mode_train(False)\n",
        "        nb_samples = 0\n",
        "        for j, (x, y) in enumerate(valloader):\n",
        "\n",
        "            cost, err, probs = net.eval(x, y)\n",
        "\n",
        "            cost_dev[i] += cost\n",
        "            err_dev[i] += err\n",
        "            nb_samples += len(x)\n",
        "\n",
        "        cost_dev[i] /= nb_samples\n",
        "        err_dev[i] /= nb_samples\n",
        "\n",
        "        cprint('g', '    Jdev = %f, err = %f\\n' % (cost_dev[i], err_dev[i]))\n",
        "\n",
        "        if err_dev[i] < best_err:\n",
        "            best_err = err_dev[i]\n",
        "            cprint('b', 'best test error')\n",
        "            net.save(models_dir+'/theta_best.dat')\n",
        "   \n",
        "toc0 = time.time()\n",
        "runtime_per_it = (toc0 - tic0) / float(nb_epochs)\n",
        "cprint('r', '   average time: %f seconds\\n' % runtime_per_it)\n",
        "\n",
        "net.save(models_dir+'/theta_last.dat')\n",
        "\n",
        "## ---------------------------------------------------------------------------------------------------------------------\n",
        "# results\n",
        "cprint('c', '\\nRESULTS:')\n",
        "nb_parameters = net.get_nb_parameters()\n",
        "best_cost_dev = np.min(cost_dev)\n",
        "best_cost_train = np.min(pred_cost_train)\n",
        "err_dev_min = err_dev[::nb_its_dev].min()\n",
        "\n",
        "print('  cost_dev: %f (cost_train %f)' % (best_cost_dev, best_cost_train))\n",
        "print('  err_dev: %f' % (err_dev_min))\n",
        "print('  nb_parameters: %d (%s)' % (nb_parameters, humansize(nb_parameters)))\n",
        "print('  time_per_it: %fs\\n' % (runtime_per_it))\n",
        "\n",
        "\n",
        "\n",
        "## Save results for plots\n",
        "# np.save('results/test_predictions.npy', test_predictions)\n",
        "np.save(results_dir + '/cost_train.npy', pred_cost_train)\n",
        "np.save(results_dir + '/cost_dev.npy', cost_dev)\n",
        "np.save(results_dir + '/err_train.npy', err_train)\n",
        "np.save(results_dir + '/err_dev.npy', err_dev)\n",
        "np.save(results_dir + '/save_dicts.npy', save_dicts)\n",
        " \n",
        " \n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[36m\n",
            "Data:\u001b[0m\n",
            "\u001b[36m\n",
            "Network:\u001b[0m\n",
            "\u001b[36m\n",
            "Net:\u001b[0m\n",
            "\u001b[33m Creating Net!! \u001b[0m\n",
            "    Total params: 2.40M\n",
            "\u001b[36m\n",
            "Train:\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:523: UserWarning: A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            "  \"A ``sample_rate`` has been provided.\"\n",
            "/usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:195: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            "  \"Secure RNG turned off. This is perfectly fine for experimentation as it allows \"\n",
            "\r  0%|          | 0/235 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  init cost variables:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "100%|██████████| 235/235 [00:09<00:00, 23.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 0/15, Jtr_pred = 0.575287, err = 0.164633, \u001b[31m   time: 9.951154 seconds\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.409699, err = 0.113100\n",
            "\u001b[0m\n",
            "\u001b[34mbest test error\u001b[0m\n",
            "\u001b[36mWritting models_SGLD_MNIST_ndplaplacian/theta_best.dat\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:09<00:00, 23.69it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 1/15, Jtr_pred = 0.428962, err = 0.110767, \u001b[31m   time: 9.923389 seconds\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.451488, err = 0.110100\n",
            "\u001b[0m\n",
            "\u001b[34mbest test error\u001b[0m\n",
            "\u001b[36mWritting models_SGLD_MNIST_ndplaplacian/theta_best.dat\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:10<00:00, 23.36it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 2/15, Jtr_pred = 0.444653, err = 0.107850, \u001b[31m   time: 10.061934 seconds\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.486672, err = 0.113500\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:09<00:00, 23.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 3/15, Jtr_pred = 0.475376, err = 0.107667, \u001b[31m   time: 9.887226 seconds\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.466036, err = 0.104300\n",
            "\u001b[0m\n",
            "\u001b[34mbest test error\u001b[0m\n",
            "\u001b[36mWritting models_SGLD_MNIST_ndplaplacian/theta_best.dat\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:09<00:00, 23.58it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 4/15, Jtr_pred = 0.499270, err = 0.108433, \u001b[31m   time: 9.968985 seconds\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.590824, err = 0.124300\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:09<00:00, 23.89it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 5/15, Jtr_pred = 0.514864, err = 0.108583, \u001b[31m   time: 9.844457 seconds\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.499215, err = 0.108100\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:09<00:00, 23.87it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 6/15, Jtr_pred = 0.536860, err = 0.110967, \u001b[31m   time: 9.854952 seconds\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.630002, err = 0.127200\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:09<00:00, 23.92it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 7/15, Jtr_pred = 0.535930, err = 0.109200, \u001b[31m   time: 9.836045 seconds\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.560682, err = 0.110800\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:09<00:00, 23.71it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 8/15, Jtr_pred = 0.547129, err = 0.109517, \u001b[31m   time: 9.915766 seconds\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.604353, err = 0.118300\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:09<00:00, 23.71it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 9/15, Jtr_pred = 0.537406, err = 0.105917, \u001b[31m   time: 9.915068 seconds\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.549893, err = 0.107700\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:09<00:00, 23.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 10/15, Jtr_pred = 0.532504, err = 0.106217, \u001b[31m   time: 9.963025 seconds\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.520974, err = 0.104900\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:09<00:00, 23.65it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 11/15, Jtr_pred = 0.523396, err = 0.108317, \u001b[31m   time: 9.942082 seconds\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.574687, err = 0.116500\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:09<00:00, 23.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 12/15, Jtr_pred = 0.523351, err = 0.105900, \u001b[31m   time: 9.950850 seconds\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.656560, err = 0.126200\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:09<00:00, 23.75it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 13/15, Jtr_pred = 0.530333, err = 0.106433, \u001b[31m   time: 9.902237 seconds\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.533488, err = 0.110400\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:09<00:00, 23.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 14/15, Jtr_pred = 0.526827, err = 0.106817, \u001b[31m   time: 9.912264 seconds\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.586912, err = 0.112900\n",
            "\u001b[0m\n",
            "\u001b[31m   average time: 11.261992 seconds\n",
            "\u001b[0m\n",
            "\u001b[36mWritting models_SGLD_MNIST_ndplaplacian/theta_last.dat\n",
            "\u001b[0m\n",
            "\u001b[36m\n",
            "RESULTS:\u001b[0m\n",
            "  cost_dev: 0.409699 (cost_train 0.428962)\n",
            "  err_dev: 0.104300\n",
            "  nb_parameters: 2395210 (2.28MB)\n",
            "  time_per_it: 11.261992s\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ODUmk-l4nC8"
      },
      "source": [
        "# Training Bayesian Linear 2 Layers MNIST (Gaussian prior) DP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z61wJUA6iuiF",
        "outputId": "4a080d18-7a63-4ca6-bce7-2727122ccda9"
      },
      "source": [
        "models_dir = 'models_SGLD_MNIST_gaussian'\n",
        "results_dir = 'results_SGLD_MNIST_gaussian'\n",
        "\n",
        "mkdir(models_dir)\n",
        "mkdir(results_dir)\n",
        "\n",
        "save_dicts = []\n",
        "# ------------------------------------------------------------------------------------------------------\n",
        "# train config\n",
        "NTrainPointsMNIST = 60000\n",
        "batch_size = 256\n",
        "nb_epochs = 15 # We can do less iterations as this method has faster convergence\n",
        "log_interval = 1\n",
        "\n",
        "lr = 1e-5\n",
        "norm_sigma =0.1 \n",
        "laplace_b=0\n",
        "noise_multiplier = 1.3\n",
        "clipping_norm=1.5\n",
        "DP_or_Not=True\n",
        "# ------------------------------------------------------------------------------------------------------\n",
        "# dataset\n",
        "cprint('c', '\\nData:')\n",
        "\n",
        " \n",
        "\n",
        "# load data\n",
        "\n",
        "# data augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
        "])\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "trainset = datasets.MNIST(root='/content/data', train=True, download=True, transform=transform_train)\n",
        "valset = datasets.MNIST(root='/content/data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "if use_cuda:\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=3)\n",
        "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=3)\n",
        "\n",
        "else:\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n",
        "                                              num_workers=3)\n",
        "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
        "                                            num_workers=3)\n",
        "\n",
        "\n",
        "## ---------------------------------------------------------------------------------------------------------------------\n",
        "# net dims\n",
        "cprint('c', '\\nNetwork:')\n",
        "\n",
        "\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[36m\n",
            "Data:\u001b[0m\n",
            "\u001b[36m\n",
            "Network:\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZqbFXB3Z2Q6"
      },
      "source": [
        "########################################################################################\n",
        "net = Net_langevin(lr=lr, cuda=use_cuda, classes=10,DP_or_Not=True, N_train=NTrainPointsMNIST, norm_sigma=norm_sigma,laplace_b=laplace_b,which_net=Linear_2L(28*28,10))\n",
        "\n",
        "## weight saving parameters #######\n",
        "start_save = 5\n",
        "save_every = 1 # We want less correlated samples -> despite having per minibatch noise we see correlations\n",
        "N_saves = 90\n",
        "###################################\n",
        "\n",
        "\n",
        "## ---------------------------------------------------------------------------------------------------------------------\n",
        "# train\n",
        "cprint('c', '\\nTrain:')\n",
        "\n",
        "print('  init cost variables:')\n",
        "pred_cost_train = np.zeros(nb_epochs)\n",
        "err_train = np.zeros(nb_epochs)\n",
        "\n",
        "cost_dev = np.zeros(nb_epochs)\n",
        "err_dev = np.zeros(nb_epochs)\n",
        "# best_cost = np.inf\n",
        "best_err = np.inf\n",
        "\n",
        "\n",
        "nb_its_dev = 1\n",
        "\n",
        "tic0 = time.time()\n",
        "from tqdm import tqdm\n",
        "\n",
        "for i in range(nb_epochs):\n",
        "    \n",
        "#     if i in [1]:\n",
        "#         print('updating lr')\n",
        "#         net.sched.step()\n",
        "    \n",
        "    net.set_mode_train(True)\n",
        "\n",
        "    tic = time.time()\n",
        "    nb_samples = 0\n",
        "\n",
        "    for x, y in tqdm(trainloader):\n",
        "        cost_pred, err = net.fit(x, y)\n",
        "        #print(cost_pred)\n",
        "        #print(err)\n",
        "        err_train[i] += err\n",
        "        #pred_cost_train[i] += cost_pred\n",
        "        pred_cost_train[i] += torch.mean(cost_pred)\n",
        "        nb_samples += len(x)\n",
        "\n",
        "    pred_cost_train[i] /= nb_samples\n",
        "    err_train[i] /= nb_samples\n",
        "\n",
        "    toc = time.time()\n",
        "    net.epoch = i\n",
        "    # ---- print\n",
        "    print(\"it %d/%d, Jtr_pred = %f, err = %f, \" % (i, nb_epochs, pred_cost_train[i], err_train[i]), end=\"\")\n",
        "    cprint('r', '   time: %f seconds\\n' % (toc - tic))\n",
        "    \n",
        "    # ---- save weights\n",
        "    if i >= start_save and i % save_every == 0:\n",
        "        net.save_sampled_net(max_samples=N_saves)\n",
        "        save_dicts.append(copy.deepcopy(net.model.state_dict()))\n",
        "    # ---- dev\n",
        "    if i % nb_its_dev == 0:\n",
        "        net.set_mode_train(False)\n",
        "        nb_samples = 0\n",
        "        for j, (x, y) in enumerate(valloader):\n",
        "\n",
        "            cost, err, probs = net.eval(x, y)\n",
        "\n",
        "            cost_dev[i] += cost\n",
        "            err_dev[i] += err\n",
        "            nb_samples += len(x)\n",
        "\n",
        "        cost_dev[i] /= nb_samples\n",
        "        err_dev[i] /= nb_samples\n",
        "\n",
        "        cprint('g', '    Jdev = %f, err = %f\\n' % (cost_dev[i], err_dev[i]))\n",
        "\n",
        "        if err_dev[i] < best_err:\n",
        "            best_err = err_dev[i]\n",
        "            cprint('b', 'best test error')\n",
        "            net.save(models_dir+'/theta_best.dat')\n",
        "   \n",
        "toc0 = time.time()\n",
        "runtime_per_it = (toc0 - tic0) / float(nb_epochs)\n",
        "cprint('r', '   average time: %f seconds\\n' % runtime_per_it)\n",
        "\n",
        "net.save(models_dir+'/theta_last.dat')\n",
        "\n",
        "## ---------------------------------------------------------------------------------------------------------------------\n",
        "# results\n",
        "cprint('c', '\\nRESULTS:')\n",
        "nb_parameters = net.get_nb_parameters()\n",
        "best_cost_dev = np.min(cost_dev)\n",
        "best_cost_train = np.min(pred_cost_train)\n",
        "err_dev_min = err_dev[::nb_its_dev].min()\n",
        "\n",
        "print('  cost_dev: %f (cost_train %f)' % (best_cost_dev, best_cost_train))\n",
        "print('  err_dev: %f' % (err_dev_min))\n",
        "print('  nb_parameters: %d (%s)' % (nb_parameters, humansize(nb_parameters)))\n",
        "print('  time_per_it: %fs\\n' % (runtime_per_it))\n",
        "\n",
        "\n",
        "\n",
        "## Save results for plots\n",
        "# np.save('results/test_predictions.npy', test_predictions)\n",
        "np.save(results_dir + '/cost_train.npy', pred_cost_train)\n",
        "np.save(results_dir + '/cost_dev.npy', cost_dev)\n",
        "np.save(results_dir + '/err_train.npy', err_train)\n",
        "np.save(results_dir + '/err_dev.npy', err_dev)\n",
        "np.save(results_dir + '/save_dicts.npy',save_dicts )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGYNfrtK5al2"
      },
      "source": [
        "# Training Bayesian Linear 2 Layers MNIST (Gaussian prior) non-DP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "790ay07F5r-N",
        "outputId": "64fcb45b-b0bf-460e-e49d-ae54557b912b"
      },
      "source": [
        "models_dir = 'models_SGLD_MNIST_gaussian'\n",
        "results_dir = 'results_SGLD_MNIST_gaussian'\n",
        "\n",
        "mkdir(models_dir)\n",
        "mkdir(results_dir)\n",
        "\n",
        "save_dicts = []\n",
        "# ------------------------------------------------------------------------------------------------------\n",
        "# train config\n",
        "NTrainPointsMNIST = 60000\n",
        "batch_size = 256\n",
        "nb_epochs = 15 # We can do less iterations as this method has faster convergence\n",
        "log_interval = 1\n",
        "\n",
        "lr = 1e-5\n",
        "norm_sigma =0.1 \n",
        "laplace_b=0\n",
        "noise_multiplier = 1.3\n",
        "clipping_norm=1.5\n",
        "DP_or_Not=True\n",
        "# ------------------------------------------------------------------------------------------------------\n",
        "# dataset\n",
        "cprint('c', '\\nData:')\n",
        "\n",
        " \n",
        "\n",
        "# load data\n",
        "\n",
        "# data augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
        "])\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "trainset = datasets.MNIST(root='/content/data', train=True, download=True, transform=transform_train)\n",
        "valset = datasets.MNIST(root='/content/data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "if use_cuda:\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=3)\n",
        "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=3)\n",
        "\n",
        "else:\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n",
        "                                              num_workers=3)\n",
        "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
        "                                            num_workers=3)\n",
        "\n",
        "\n",
        "## ---------------------------------------------------------------------------------------------------------------------\n",
        "# net dims\n",
        "cprint('c', '\\nNetwork:')\n",
        "\n",
        "\n",
        "########################################################################################\n",
        "net = Net_langevin(lr=lr, cuda=use_cuda, classes=10,DP_or_Not=False, N_train=NTrainPointsMNIST, norm_sigma=norm_sigma,laplace_b=laplace_b,which_net=Linear_2L(28*28,10))\n",
        "\n",
        "## weight saving parameters #######\n",
        "start_save = 5\n",
        "save_every = 1 # We want less correlated samples -> despite having per minibatch noise we see correlations\n",
        "N_saves = 90\n",
        "###################################\n",
        "\n",
        "\n",
        "## ---------------------------------------------------------------------------------------------------------------------\n",
        "# train\n",
        "cprint('c', '\\nTrain:')\n",
        "\n",
        "print('  init cost variables:')\n",
        "pred_cost_train = np.zeros(nb_epochs)\n",
        "err_train = np.zeros(nb_epochs)\n",
        "\n",
        "cost_dev = np.zeros(nb_epochs)\n",
        "err_dev = np.zeros(nb_epochs)\n",
        "# best_cost = np.inf\n",
        "best_err = np.inf\n",
        "\n",
        "\n",
        "nb_its_dev = 1\n",
        "\n",
        "tic0 = time.time()\n",
        "from tqdm import tqdm\n",
        "\n",
        "for i in range(nb_epochs):\n",
        "    \n",
        "#     if i in [1]:\n",
        "#         print('updating lr')\n",
        "#         net.sched.step()\n",
        "    \n",
        "    net.set_mode_train(True)\n",
        "\n",
        "    tic = time.time()\n",
        "    nb_samples = 0\n",
        "\n",
        "    for x, y in tqdm(trainloader):\n",
        "        cost_pred, err = net.fit(x, y)\n",
        "        #print(cost_pred)\n",
        "        #print(err)\n",
        "        err_train[i] += err\n",
        "        #pred_cost_train[i] += cost_pred\n",
        "        pred_cost_train[i] += torch.mean(cost_pred)\n",
        "        nb_samples += len(x)\n",
        "\n",
        "    pred_cost_train[i] /= nb_samples\n",
        "    err_train[i] /= nb_samples\n",
        "\n",
        "    toc = time.time()\n",
        "    net.epoch = i\n",
        "    # ---- print\n",
        "    print(\"it %d/%d, Jtr_pred = %f, err = %f, \" % (i, nb_epochs, pred_cost_train[i], err_train[i]), end=\"\")\n",
        "    cprint('r', '   time: %f seconds\\n' % (toc - tic))\n",
        "    \n",
        "    # ---- save weights\n",
        "    if i >= start_save and i % save_every == 0:\n",
        "        net.save_sampled_net(max_samples=N_saves)\n",
        "        save_dicts.append(copy.deepcopy(net.model.state_dict()))\n",
        "    # ---- dev\n",
        "    if i % nb_its_dev == 0:\n",
        "        net.set_mode_train(False)\n",
        "        nb_samples = 0\n",
        "        for j, (x, y) in enumerate(valloader):\n",
        "\n",
        "            cost, err, probs = net.eval(x, y)\n",
        "\n",
        "            cost_dev[i] += cost\n",
        "            err_dev[i] += err\n",
        "            nb_samples += len(x)\n",
        "\n",
        "        cost_dev[i] /= nb_samples\n",
        "        err_dev[i] /= nb_samples\n",
        "\n",
        "        cprint('g', '    Jdev = %f, err = %f\\n' % (cost_dev[i], err_dev[i]))\n",
        "\n",
        "        if err_dev[i] < best_err:\n",
        "            best_err = err_dev[i]\n",
        "            cprint('b', 'best test error')\n",
        "            net.save(models_dir+'/theta_best.dat')\n",
        "   \n",
        "toc0 = time.time()\n",
        "runtime_per_it = (toc0 - tic0) / float(nb_epochs)\n",
        "cprint('r', '   average time: %f seconds\\n' % runtime_per_it)\n",
        "\n",
        "net.save(models_dir+'/theta_last.dat')\n",
        "\n",
        "## ---------------------------------------------------------------------------------------------------------------------\n",
        "# results\n",
        "cprint('c', '\\nRESULTS:')\n",
        "nb_parameters = net.get_nb_parameters()\n",
        "best_cost_dev = np.min(cost_dev)\n",
        "best_cost_train = np.min(pred_cost_train)\n",
        "err_dev_min = err_dev[::nb_its_dev].min()\n",
        "\n",
        "print('  cost_dev: %f (cost_train %f)' % (best_cost_dev, best_cost_train))\n",
        "print('  err_dev: %f' % (err_dev_min))\n",
        "print('  nb_parameters: %d (%s)' % (nb_parameters, humansize(nb_parameters)))\n",
        "print('  time_per_it: %fs\\n' % (runtime_per_it))\n",
        "\n",
        "\n",
        "\n",
        "## Save results for plots\n",
        "# np.save('results/test_predictions.npy', test_predictions)\n",
        "np.save(results_dir + '/cost_train.npy', pred_cost_train)\n",
        "np.save(results_dir + '/cost_dev.npy', cost_dev)\n",
        "np.save(results_dir + '/err_train.npy', err_train)\n",
        "np.save(results_dir + '/err_dev.npy', err_dev)\n",
        "np.save(results_dir + '/save_dicts.npy',save_dicts )\n",
        "\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[36m\n",
            "Data:\u001b[0m\n",
            "\u001b[36m\n",
            "Network:\u001b[0m\n",
            "\u001b[36m\n",
            "Net:\u001b[0m\n",
            "\u001b[33m Creating Net!! \u001b[0m\n",
            "    Total params: 2.40M\n",
            "\u001b[36m\n",
            "Train:\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:523: UserWarning: A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            "  \"A ``sample_rate`` has been provided.\"\n",
            "/usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:195: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            "  \"Secure RNG turned off. This is perfectly fine for experimentation as it allows \"\n",
            "\r  0%|          | 0/235 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  init cost variables:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)\n",
            "100%|██████████| 235/235 [00:08<00:00, 27.50it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 0/15, Jtr_pred = 0.567571, err = 0.158783, \u001b[31m   time: 8.549930 seconds\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.422772, err = 0.108200\n",
            "\u001b[0m\n",
            "\u001b[34mbest test error\u001b[0m\n",
            "\u001b[36mWritting models_SGLD_MNIST_gaussian/theta_best.dat\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:08<00:00, 27.77it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 1/15, Jtr_pred = 0.457773, err = 0.108033, \u001b[31m   time: 8.465600 seconds\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.477143, err = 0.107500\n",
            "\u001b[0m\n",
            "\u001b[34mbest test error\u001b[0m\n",
            "\u001b[36mWritting models_SGLD_MNIST_gaussian/theta_best.dat\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:08<00:00, 27.36it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 2/15, Jtr_pred = 0.493562, err = 0.109017, \u001b[31m   time: 8.592522 seconds\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.532220, err = 0.114500\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:08<00:00, 27.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 3/15, Jtr_pred = 0.516935, err = 0.110017, \u001b[31m   time: 8.450509 seconds\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.519353, err = 0.110300\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:08<00:00, 27.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 4/15, Jtr_pred = 0.519199, err = 0.105783, \u001b[31m   time: 8.679873 seconds\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.539097, err = 0.109200\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:08<00:00, 26.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 5/15, Jtr_pred = 0.547890, err = 0.108267, \u001b[31m   time: 8.780450 seconds\n",
            "\u001b[0m\n",
            "\u001b[36m saving weight samples 1/90\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.606393, err = 0.116700\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:08<00:00, 27.99it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 6/15, Jtr_pred = 0.561729, err = 0.109650, \u001b[31m   time: 8.400120 seconds\n",
            "\u001b[0m\n",
            "\u001b[36m saving weight samples 2/90\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.643397, err = 0.121000\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:08<00:00, 27.35it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 7/15, Jtr_pred = 0.556156, err = 0.109300, \u001b[31m   time: 8.597215 seconds\n",
            "\u001b[0m\n",
            "\u001b[36m saving weight samples 3/90\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.566837, err = 0.114200\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:08<00:00, 27.48it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 8/15, Jtr_pred = 0.547178, err = 0.107483, \u001b[31m   time: 8.559087 seconds\n",
            "\u001b[0m\n",
            "\u001b[36m saving weight samples 4/90\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.527794, err = 0.103600\n",
            "\u001b[0m\n",
            "\u001b[34mbest test error\u001b[0m\n",
            "\u001b[36mWritting models_SGLD_MNIST_gaussian/theta_best.dat\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:08<00:00, 27.27it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 9/15, Jtr_pred = 0.555151, err = 0.108567, \u001b[31m   time: 8.619252 seconds\n",
            "\u001b[0m\n",
            "\u001b[36m saving weight samples 5/90\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.613209, err = 0.117700\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:08<00:00, 27.93it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 10/15, Jtr_pred = 0.552088, err = 0.108667, \u001b[31m   time: 8.420816 seconds\n",
            "\u001b[0m\n",
            "\u001b[36m saving weight samples 6/90\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.529457, err = 0.107500\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:08<00:00, 26.57it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 11/15, Jtr_pred = 0.547161, err = 0.107133, \u001b[31m   time: 8.847456 seconds\n",
            "\u001b[0m\n",
            "\u001b[36m saving weight samples 7/90\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.541717, err = 0.103800\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:08<00:00, 27.42it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 12/15, Jtr_pred = 0.543331, err = 0.106200, \u001b[31m   time: 8.575445 seconds\n",
            "\u001b[0m\n",
            "\u001b[36m saving weight samples 8/90\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.556220, err = 0.111900\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:08<00:00, 28.17it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 13/15, Jtr_pred = 0.548069, err = 0.109750, \u001b[31m   time: 8.349768 seconds\n",
            "\u001b[0m\n",
            "\u001b[36m saving weight samples 9/90\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.610495, err = 0.117400\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:08<00:00, 27.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 14/15, Jtr_pred = 0.548381, err = 0.109933, \u001b[31m   time: 8.508524 seconds\n",
            "\u001b[0m\n",
            "\u001b[36m saving weight samples 10/90\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.559244, err = 0.113400\n",
            "\u001b[0m\n",
            "\u001b[31m   average time: 9.928762 seconds\n",
            "\u001b[0m\n",
            "\u001b[36mWritting models_SGLD_MNIST_gaussian/theta_last.dat\n",
            "\u001b[0m\n",
            "\u001b[36m\n",
            "RESULTS:\u001b[0m\n",
            "  cost_dev: 0.422772 (cost_train 0.457773)\n",
            "  err_dev: 0.103600\n",
            "  nb_parameters: 2395210 (2.28MB)\n",
            "  time_per_it: 9.928762s\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzb3hsfyWgGz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "outputId": "f95295f6-e1a8-456d-ec8d-cbefc8d25f14"
      },
      "source": [
        "\n",
        "\n",
        "## ---------------------------------------------------------------------------------------------------------------------\n",
        "# fig cost vs its\n",
        "\n",
        "textsize = 15\n",
        "marker=5\n",
        "\n",
        "plt.figure(dpi=100)\n",
        "fig, ax1 = plt.subplots()\n",
        "ax1.plot(range(0, nb_epochs, nb_its_dev), cost_dev[::nb_its_dev], 'b-')\n",
        "ax1.plot(pred_cost_train, 'r--')\n",
        "ax1.set_ylabel('Cross Entropy')\n",
        "plt.xlabel('epoch')\n",
        "plt.grid(b=True, which='major', color='k', linestyle='-')\n",
        "plt.grid(b=True, which='minor', color='k', linestyle='--')\n",
        "lgd = plt.legend(['test error', 'train error'], markerscale=marker, prop={'size': textsize, 'weight': 'normal'})\n",
        "ax = plt.gca()\n",
        "plt.title('classification costs')\n",
        "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
        "    ax.get_xticklabels() + ax.get_yticklabels()):\n",
        "    item.set_fontsize(textsize)\n",
        "    item.set_weight('normal')\n",
        "plt.savefig(results_dir + '/cost.png', bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(dpi=100)\n",
        "fig2, ax2 = plt.subplots()\n",
        "ax2.set_ylabel('% error')\n",
        "ax2.semilogy(range(0, nb_epochs, nb_its_dev), 100 * err_dev[::nb_its_dev], 'b-')\n",
        "ax2.semilogy(100 * err_train, 'r--')\n",
        "plt.xlabel('epoch')\n",
        "plt.grid(b=True, which='major', color='k', linestyle='-')\n",
        "plt.grid(b=True, which='minor', color='k', linestyle='--')\n",
        "ax2.get_yaxis().set_minor_formatter(matplotlib.ticker.ScalarFormatter())\n",
        "ax2.get_yaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
        "lgd = plt.legend(['test error', 'train error'], markerscale=marker, prop={'size': textsize, 'weight': 'normal'})\n",
        "ax = plt.gca()\n",
        "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
        "    ax.get_xticklabels() + ax.get_yticklabels()):\n",
        "    item.set_fontsize(textsize)\n",
        "    item.set_weight('normal')\n",
        "plt.savefig(results_dir + '/err.png',  bbox_extra_artists=(lgd,), box_inches='tight')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP1nKk91-GAP"
      },
      "source": [
        "# Training for Bayesian CNN training MNIST (Gaussian prior) DP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyPYlYx2xSxp",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c31e5b2-5b15-4516-e644-9391651cf13b"
      },
      "source": [
        "models_dir = 'models_SGLD_MNIST_gaussian'\n",
        "results_dir = 'results_SGLD_MNIST_gaussian'\n",
        "\n",
        "mkdir(models_dir)\n",
        "mkdir(results_dir)\n",
        "\n",
        "save_dicts = []\n",
        "# ------------------------------------------------------------------------------------------------------\n",
        "# train config\n",
        "NTrainPointsMNIST = 60000\n",
        "batch_size = 256\n",
        "nb_epochs = 15 # We can do less iterations as this method has faster convergence\n",
        "log_interval = 1\n",
        "\n",
        "lr = 1e-5\n",
        "norm_sigma =0.1 \n",
        "laplace_b=0\n",
        "noise_multiplier = 1.3\n",
        "clipping_norm=1.5\n",
        "DP_or_Not=True\n",
        "# ------------------------------------------------------------------------------------------------------\n",
        "# dataset\n",
        "cprint('c', '\\nData:')\n",
        "\n",
        " \n",
        "\n",
        "# load data\n",
        "\n",
        "# data augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
        "])\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "trainset = datasets.MNIST(root='/content/data', train=True, download=True, transform=transform_train)\n",
        "valset = datasets.MNIST(root='/content/data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "if use_cuda:\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=3)\n",
        "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=3)\n",
        "\n",
        "else:\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n",
        "                                              num_workers=3)\n",
        "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
        "                                            num_workers=3)\n",
        "\n",
        "\n",
        "## ---------------------------------------------------------------------------------------------------------------------\n",
        "# net dims\n",
        "cprint('c', '\\nNetwork:')\n",
        "\n",
        "\n",
        "########################################################################################\n",
        "net = Net_langevin(lr=lr, cuda=use_cuda, classes=10,N_train=NTrainPointsMNIST, norm_sigma=norm_sigma,laplace_b=laplace_b,which_net=SampleConvNet_MNIST())\n",
        "\n",
        "## weight saving parameters #######\n",
        "start_save = 5\n",
        "save_every = 1 # We want less correlated samples -> despite having per minibatch noise we see correlations\n",
        "N_saves = 90\n",
        "###################################\n",
        "\n",
        "\n",
        "## ---------------------------------------------------------------------------------------------------------------------\n",
        "# train\n",
        "cprint('c', '\\nTrain:')\n",
        "\n",
        "print('  init cost variables:')\n",
        "pred_cost_train = np.zeros(nb_epochs)\n",
        "err_train = np.zeros(nb_epochs)\n",
        "\n",
        "cost_dev = np.zeros(nb_epochs)\n",
        "err_dev = np.zeros(nb_epochs)\n",
        "# best_cost = np.inf\n",
        "best_err = np.inf\n",
        "\n",
        "\n",
        "nb_its_dev = 1\n",
        "\n",
        "tic0 = time.time()\n",
        "from tqdm import tqdm\n",
        "\n",
        "for i in range(nb_epochs):\n",
        "    \n",
        "#     if i in [1]:\n",
        "#         print('updating lr')\n",
        "#         net.sched.step()\n",
        "    \n",
        "    net.set_mode_train(True)\n",
        "\n",
        "    tic = time.time()\n",
        "    nb_samples = 0\n",
        "\n",
        "    for x, y in tqdm(trainloader):\n",
        "        cost_pred, err = net.fit(x, y)\n",
        "        #print(cost_pred)\n",
        "        #print(err)\n",
        "        err_train[i] += err\n",
        "        #pred_cost_train[i] += cost_pred\n",
        "        pred_cost_train[i] += torch.mean(cost_pred)\n",
        "        nb_samples += len(x)\n",
        "\n",
        "    pred_cost_train[i] /= nb_samples\n",
        "    err_train[i] /= nb_samples\n",
        "\n",
        "    toc = time.time()\n",
        "    net.epoch = i\n",
        "    # ---- print\n",
        "    print(\"it %d/%d, Jtr_pred = %f, err = %f, \" % (i, nb_epochs, pred_cost_train[i], err_train[i]), end=\"\")\n",
        "    cprint('r', '   time: %f seconds\\n' % (toc - tic))\n",
        "    \n",
        "    # ---- save weights\n",
        "    if i >= start_save and i % save_every == 0:\n",
        "        net.save_sampled_net(max_samples=N_saves)\n",
        "        save_dicts.append(copy.deepcopy(net.model.state_dict()))\n",
        "    # ---- dev\n",
        "    if i % nb_its_dev == 0:\n",
        "        net.set_mode_train(False)\n",
        "        nb_samples = 0\n",
        "        for j, (x, y) in enumerate(valloader):\n",
        "\n",
        "            cost, err, probs = net.eval(x, y)\n",
        "\n",
        "            cost_dev[i] += cost\n",
        "            err_dev[i] += err\n",
        "            nb_samples += len(x)\n",
        "\n",
        "        cost_dev[i] /= nb_samples\n",
        "        err_dev[i] /= nb_samples\n",
        "\n",
        "        cprint('g', '    Jdev = %f, err = %f\\n' % (cost_dev[i], err_dev[i]))\n",
        "\n",
        "        if err_dev[i] < best_err:\n",
        "            best_err = err_dev[i]\n",
        "            cprint('b', 'best test error')\n",
        "            net.save(models_dir+'/theta_best.dat')\n",
        "   \n",
        "toc0 = time.time()\n",
        "runtime_per_it = (toc0 - tic0) / float(nb_epochs)\n",
        "cprint('r', '   average time: %f seconds\\n' % runtime_per_it)\n",
        "\n",
        "net.save(models_dir+'/theta_last.dat')\n",
        "\n",
        "## ---------------------------------------------------------------------------------------------------------------------\n",
        "# results\n",
        "cprint('c', '\\nRESULTS:')\n",
        "nb_parameters = net.get_nb_parameters()\n",
        "best_cost_dev = np.min(cost_dev)\n",
        "best_cost_train = np.min(pred_cost_train)\n",
        "err_dev_min = err_dev[::nb_its_dev].min()\n",
        "\n",
        "print('  cost_dev: %f (cost_train %f)' % (best_cost_dev, best_cost_train))\n",
        "print('  err_dev: %f' % (err_dev_min))\n",
        "print('  nb_parameters: %d (%s)' % (nb_parameters, humansize(nb_parameters)))\n",
        "print('  time_per_it: %fs\\n' % (runtime_per_it))\n",
        "\n",
        "\n",
        "\n",
        "## Save results for plots\n",
        "# np.save('results/test_predictions.npy', test_predictions)\n",
        "np.save(results_dir + '/cost_train.npy', pred_cost_train)\n",
        "np.save(results_dir + '/cost_dev.npy', cost_dev)\n",
        "np.save(results_dir + '/err_train.npy', err_train)\n",
        "np.save(results_dir + '/err_dev.npy', err_dev)\n",
        "np.save(results_dir + '/save_dicts.npy',save_dicts )\n",
        "\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[36m\n",
            "Data:\u001b[0m\n",
            "\u001b[36m\n",
            "Network:\u001b[0m\n",
            "\u001b[36m\n",
            "Net:\u001b[0m\n",
            "\u001b[33m Creating Net!! \u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "    Total params: 0.03M\n",
            "\u001b[36m\n",
            "Train:\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:523: UserWarning: A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            "  \"A ``sample_rate`` has been provided.\"\n",
            "/usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:195: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            "  \"Secure RNG turned off. This is perfectly fine for experimentation as it allows \"\n",
            "\r  0%|          | 0/235 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  init cost variables:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)\n",
            "100%|██████████| 235/235 [00:08<00:00, 27.73it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 0/15, Jtr_pred = 0.594567, err = 0.183083, \u001b[31m   time: 8.482497 seconds\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.391650, err = 0.087800\n",
            "\u001b[0m\n",
            "\u001b[34mbest test error\u001b[0m\n",
            "\u001b[36mWritting models_SGLD_MNIST_gaussian/theta_best.dat\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:08<00:00, 28.16it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 1/15, Jtr_pred = 0.261851, err = 0.066467, \u001b[31m   time: 8.349429 seconds\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.210605, err = 0.054700\n",
            "\u001b[0m\n",
            "\u001b[34mbest test error\u001b[0m\n",
            "\u001b[36mWritting models_SGLD_MNIST_gaussian/theta_best.dat\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:08<00:00, 27.77it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 2/15, Jtr_pred = 0.237260, err = 0.061050, \u001b[31m   time: 8.467314 seconds\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.215413, err = 0.056200\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:08<00:00, 27.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 3/15, Jtr_pred = 0.216040, err = 0.056817, \u001b[31m   time: 8.508117 seconds\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.196983, err = 0.048700\n",
            "\u001b[0m\n",
            "\u001b[34mbest test error\u001b[0m\n",
            "\u001b[36mWritting models_SGLD_MNIST_gaussian/theta_best.dat\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:08<00:00, 27.02it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 4/15, Jtr_pred = 0.203423, err = 0.052850, \u001b[31m   time: 8.701454 seconds\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.236167, err = 0.059900\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:08<00:00, 27.27it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 5/15, Jtr_pred = 0.207524, err = 0.054000, \u001b[31m   time: 8.625386 seconds\n",
            "\u001b[0m\n",
            "\u001b[36m saving weight samples 1/90\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.219064, err = 0.056500\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:08<00:00, 27.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 6/15, Jtr_pred = 0.195699, err = 0.050450, \u001b[31m   time: 8.558553 seconds\n",
            "\u001b[0m\n",
            "\u001b[36m saving weight samples 2/90\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.185892, err = 0.046600\n",
            "\u001b[0m\n",
            "\u001b[34mbest test error\u001b[0m\n",
            "\u001b[36mWritting models_SGLD_MNIST_gaussian/theta_best.dat\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:08<00:00, 27.56it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 7/15, Jtr_pred = 0.189985, err = 0.048500, \u001b[31m   time: 8.531038 seconds\n",
            "\u001b[0m\n",
            "\u001b[36m saving weight samples 3/90\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.193641, err = 0.052600\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:08<00:00, 26.97it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 8/15, Jtr_pred = 0.181305, err = 0.046917, \u001b[31m   time: 8.716759 seconds\n",
            "\u001b[0m\n",
            "\u001b[36m saving weight samples 4/90\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.171101, err = 0.045800\n",
            "\u001b[0m\n",
            "\u001b[34mbest test error\u001b[0m\n",
            "\u001b[36mWritting models_SGLD_MNIST_gaussian/theta_best.dat\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:08<00:00, 27.56it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 9/15, Jtr_pred = 0.180980, err = 0.047233, \u001b[31m   time: 8.528536 seconds\n",
            "\u001b[0m\n",
            "\u001b[36m saving weight samples 5/90\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.155786, err = 0.042600\n",
            "\u001b[0m\n",
            "\u001b[34mbest test error\u001b[0m\n",
            "\u001b[36mWritting models_SGLD_MNIST_gaussian/theta_best.dat\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:08<00:00, 27.71it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 10/15, Jtr_pred = 0.182929, err = 0.046583, \u001b[31m   time: 8.482658 seconds\n",
            "\u001b[0m\n",
            "\u001b[36m saving weight samples 6/90\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.160553, err = 0.043100\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:08<00:00, 27.81it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 11/15, Jtr_pred = 0.177635, err = 0.045667, \u001b[31m   time: 8.454824 seconds\n",
            "\u001b[0m\n",
            "\u001b[36m saving weight samples 7/90\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.181840, err = 0.047000\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:08<00:00, 28.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 12/15, Jtr_pred = 0.181149, err = 0.046967, \u001b[31m   time: 8.404781 seconds\n",
            "\u001b[0m\n",
            "\u001b[36m saving weight samples 8/90\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.172243, err = 0.045300\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:08<00:00, 27.69it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 13/15, Jtr_pred = 0.188860, err = 0.048700, \u001b[31m   time: 8.490197 seconds\n",
            "\u001b[0m\n",
            "\u001b[36m saving weight samples 9/90\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m    Jdev = 0.169348, err = 0.042600\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [00:08<00:00, 27.17it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 14/15, Jtr_pred = 0.173972, err = 0.046467, \u001b[31m   time: 8.656815 seconds\n",
            "\u001b[0m\n",
            "\u001b[36m saving weight samples 10/90\u001b[0m\n",
            "\u001b[32m    Jdev = 0.250675, err = 0.058500\n",
            "\u001b[0m\n",
            "\u001b[31m   average time: 9.821137 seconds\n",
            "\u001b[0m\n",
            "\u001b[36mWritting models_SGLD_MNIST_gaussian/theta_last.dat\n",
            "\u001b[0m\n",
            "\u001b[36m\n",
            "RESULTS:\u001b[0m\n",
            "  cost_dev: 0.155786 (cost_train 0.173972)\n",
            "  err_dev: 0.042600\n",
            "  nb_parameters: 26010 (25.40KB)\n",
            "  time_per_it: 9.821137s\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixl5us09Bohh"
      },
      "source": [
        "# Hetero Regression "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAYPKSFrG8AF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78d10901-7ee3-4f26-c552-6b8d84cfa2a1"
      },
      "source": [
        "!git clone https://github.com/JavierAntoran/Bayesian-Neural-Networks.git\n",
        "!pip3 install http://download.pytorch.org/whl/cu92/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision\n",
        "!pip3 install GPy\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import GPy\n",
        "import time\n",
        "import copy\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.sgd import SGD\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm, trange\n",
        "from google.colab import files\n",
        "%config InlineBackend.figure_format = 'svg'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Bayesian-Neural-Networks'...\n",
            "remote: Enumerating objects: 369, done.\u001b[K\n",
            "remote: Total 369 (delta 0), reused 0 (delta 0), pack-reused 369\u001b[K\n",
            "Receiving objects: 100% (369/369), 15.98 MiB | 17.50 MiB/s, done.\n",
            "Resolving deltas: 100% (191/191), done.\n",
            "\u001b[31mERROR: torch-0.4.1-cp36-cp36m-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.10.0+cu102)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.9.0+cu102)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchvision) (3.7.4.3)\n",
            "Collecting GPy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/4a/43d6f07b8b493bc216ecf1d5c447809e8c9d0b1b18b0b9db496dfadd87ea/GPy-1.10.0.tar.gz (959kB)\n",
            "\u001b[K     |████████████████████████████████| 962kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from GPy) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from GPy) (1.15.0)\n",
            "Collecting paramz>=0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/37/4abbeb78d30f20d3402887f46e6e9f3ef32034a9dea65d243654c82c8553/paramz-0.9.5.tar.gz (71kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython>=0.29 in /usr/local/lib/python3.7/dist-packages (from GPy) (0.29.23)\n",
            "Requirement already satisfied: scipy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from GPy) (1.4.1)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.7/dist-packages (from paramz>=0.9.0->GPy) (4.4.2)\n",
            "Building wheels for collected packages: GPy, paramz\n",
            "  Building wheel for GPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPy: filename=GPy-1.10.0-cp37-cp37m-linux_x86_64.whl size=2564977 sha256=f05beea25382bcde59dcb855afb9e8f61c0b9417623ff560ae8225fdbee92bb2\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/99/8d/d0c3dee7db4af58190cde6abdb45e6a7ded2f9f01ff528dd0f\n",
            "  Building wheel for paramz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for paramz: filename=paramz-0.9.5-cp37-none-any.whl size=102566 sha256=629db8efc0b508ede3d64964e18ef48ca5e4be0b68ec6b65bc148d4ba57f799a\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/4a/0e/6e0dc85541825f991c431619e25b870d4b812c911214690cf8\n",
            "Successfully built GPy paramz\n",
            "Installing collected packages: paramz, GPy\n",
            "Successfully installed GPy-1.10.0 paramz-0.9.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKaB_wRACJfS"
      },
      "source": [
        "## Non-DP Version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1S5kt0omQ-N"
      },
      "source": [
        "def to_variable(var=(), cuda=True, volatile=False):\n",
        "    out = []\n",
        "    for v in var:\n",
        "        \n",
        "        if isinstance(v, np.ndarray):\n",
        "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
        "\n",
        "        if not v.is_cuda and cuda:\n",
        "            v = v.cuda()\n",
        "\n",
        "        if not isinstance(v, Variable):\n",
        "            v = Variable(v, volatile=volatile)\n",
        "\n",
        "        out.append(v)\n",
        "    return out\n",
        "class Langevin_SGD(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr, weight_decay=0, nesterov=False):\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
        "        \n",
        "        super(Langevin_SGD, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(SGD, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('nesterov', False)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            \n",
        "            weight_decay = group['weight_decay']\n",
        "            \n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad.data\n",
        "                \n",
        "                if len(p.shape) == 1 and p.shape[0] == 1:\n",
        "                    p.data.add_(-group['lr'], d_p)\n",
        "                    \n",
        "                else:\n",
        "                    if weight_decay != 0:\n",
        "                        d_p.add_(weight_decay, p.data)\n",
        "\n",
        "                    unit_noise = Variable(p.data.new(p.size()).normal_())\n",
        "\n",
        "                    p.data.add_(-group['lr'], 0.5*d_p + unit_noise/group['lr']**0.5)\n",
        "\n",
        "        return loss\n",
        "def log_gaussian_loss(output, target, sigma, no_dim):\n",
        "    exponent = -0.5*(target - output)**2/sigma**2\n",
        "    log_coeff = -no_dim*torch.log(sigma)\n",
        "    \n",
        "    return - (log_coeff + exponent).sum()\n",
        "\n",
        "\n",
        "def get_kl_divergence(weights, prior, varpost):\n",
        "    prior_loglik = prior.loglik(weights)\n",
        "    \n",
        "    varpost_loglik = varpost.loglik(weights)\n",
        "    varpost_lik = varpost_loglik.exp()\n",
        "    \n",
        "    return (varpost_lik*(varpost_loglik - prior_loglik)).sum()\n",
        "\n",
        "\n",
        "class gaussian:\n",
        "    def __init__(self, mu, sigma):\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        \n",
        "    def loglik(self, weights):\n",
        "        exponent = -0.5*(weights - self.mu)**2/self.sigma**2\n",
        "        log_coeff = -0.5*(np.log(2*np.pi) + 2*np.log(self.sigma))\n",
        "        \n",
        "        return (exponent + log_coeff).sum()\n",
        "class Langevin_Model(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_units):\n",
        "        super(Langevin_Model, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        \n",
        "        # network with two hidden and one output layer\n",
        "        self.layer1 = nn.Linear(input_dim, num_units)\n",
        "        self.layer2 = nn.Linear(num_units, num_units)\n",
        "        self.layer3 = nn.Linear(num_units, 2*output_dim)\n",
        "        \n",
        "        self.activation = nn.ReLU(inplace = True)\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = x.view(-1, self.input_dim)\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        x = self.activation(x)\n",
        "      \n",
        "        \n",
        "        x = self.layer2(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        y = self.layer3(x)\n",
        "        \n",
        "        return y\n",
        "class Langevin_Wrapper:\n",
        "    def __init__(self, network, learn_rate, batch_size, no_batches, weight_decay):\n",
        "        \n",
        "        self.learn_rate = learn_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.no_batches = no_batches\n",
        "        \n",
        "        self.network = network\n",
        "        self.network.cuda()\n",
        "        \n",
        "        self.optimizer = Langevin_SGD(self.network.parameters(), lr=self.learn_rate, weight_decay=weight_decay)\n",
        "        self.loss_func = log_gaussian_loss\n",
        "    \n",
        "    def fit(self, x, y):\n",
        "        x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "        # reset gradient and total loss\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        output = self.network(x)\n",
        "        loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)\n",
        "        \n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss,output[:, :1]\n",
        "    \n",
        "    \n",
        "    def test_loss(self, x, y):\n",
        "        x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "        output = self.network(x)\n",
        "        loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)\n",
        "\n",
        "        return loss\n",
        "def eval_ensemble(x, y, ensemble):\n",
        "    \n",
        "    x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "    means, stds = [], []\n",
        "    for net in ensemble:\n",
        "        output = net(x)\n",
        "        means.append(output[:, :1, None])\n",
        "        stds.append(output[:, 1:, None].exp())\n",
        "            \n",
        "    means, stds = torch.cat(means, 2), torch.cat(stds, 2)\n",
        "    mean = means.mean(dim=2)\n",
        "    std = (means.var(dim=2) + stds.mean(dim=2)**2)**0.5\n",
        "    loss = log_gaussian_loss(mean, y, std, 1)/len(x)\n",
        "    \n",
        "    rmse = ((mean - y)**2).mean()**0.5\n",
        "\n",
        "    return loss, rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ym6HBK-s8GnO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3cbb0e21-c879-4b64-ab5f-33e0a1367b8e"
      },
      "source": [
        "np.random.seed(5)\n",
        "average_mse = []\n",
        "for iter in range(20):\n",
        "  no_points = 400\n",
        "  lengthscale = 1\n",
        "  variance = 1.0\n",
        "  sig_noise = 0.3\n",
        "  x = np.random.uniform(-3, 3, no_points)[:, None]\n",
        "  x.sort(axis=0)\n",
        "\n",
        "\n",
        "  k = GPy.kern.RBF(input_dim=1, variance=variance, lengthscale=lengthscale)\n",
        "  C = k.K(x, x) + np.eye(no_points)*(x + 2)**2*sig_noise**2\n",
        "\n",
        "  y = np.random.multivariate_normal(np.zeros((no_points)), C)[:, None]\n",
        "  y = (y - y.mean())\n",
        "  x_train = x[75:325]\n",
        "  y_train = y[75:325]\n",
        "\n",
        "  \n",
        "  best_net, best_loss = None, float('inf')\n",
        "  num_nets, nets, losses = 1, [], []\n",
        "  mix_epochs, burnin_epochs = 50, 150\n",
        "  num_epochs = mix_epochs*num_nets + burnin_epochs + 1\n",
        "\n",
        "  batch_size, nb_train = len(x_train), len(x_train)\n",
        "\n",
        "  net = Langevin_Wrapper(network=Langevin_Model(input_dim=1, output_dim=1, num_units=200),\n",
        "                        learn_rate=2.5e-4, batch_size=batch_size, no_batches=1, weight_decay=50)\n",
        "\n",
        "  for i in range(num_epochs):\n",
        "      \n",
        "      loss,y_hat = net.fit(x_train, y_train)\n",
        "      \n",
        "      if i % mix_epochs == 0:\n",
        "          print('Epoch: %4d, Train loss = %8.3f' % (i, loss.cpu().data.numpy()))\n",
        "          \n",
        "      if i % 100 == 0 and i > burnin_epochs: nets.append(copy.deepcopy(net.network))\n",
        "  average_mse.append(np.power((y_hat.cpu().data.numpy()-y_train),2).mean())\n",
        "  if iter==0:\n",
        "      print(\"Using %d networks for prediction\" % len(nets))\n",
        "      samples = []\n",
        "      noises = []\n",
        "      for network in nets:\n",
        "          preds = network.forward(torch.linspace(-5, 5, 200).cuda()).cpu().data.numpy()\n",
        "          samples.append(preds[:, 0])\n",
        "          noises.append(np.exp(preds[:, 1]))\n",
        "          \n",
        "      samples = np.array(samples)\n",
        "      means = (samples.mean(axis = 0)).reshape(-1)\n",
        "\n",
        "      noises = np.array(noises)\n",
        "      aleatoric = (noises**2).mean(axis = 0)**0.5\n",
        "      epistemic = samples.var(axis = 0)**0.5\n",
        "      total_unc = (aleatoric**2 + epistemic**2)**0.5\n",
        "\n",
        "      c = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',\n",
        "          '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
        "\n",
        "      plt.figure(figsize = (6, 5))\n",
        "      plt.style.use('default')\n",
        "      plt.scatter(x_train, y_train, s = 10, marker = 'x', color = 'black', alpha = 0.5)\n",
        "      plt.fill_between(np.linspace(-5, 5, 200), means + aleatoric, means + total_unc, color = c[0], alpha = 0.3, label = 'Epistemic + Aleatoric')\n",
        "      plt.fill_between(np.linspace(-5, 5, 200), means - total_unc, means - aleatoric, color = c[0], alpha = 0.3)\n",
        "      plt.fill_between(np.linspace(-5, 5, 200), means - aleatoric, means + aleatoric, color = c[1], alpha = 0.4, label = 'Aleatoric')\n",
        "      plt.plot(np.linspace(-5, 5, 200), means, color = 'black', linewidth = 1)\n",
        "      plt.xlim([-5, 5])\n",
        "      plt.ylim([-5, 7])\n",
        "      plt.xlabel('$x$', fontsize=16)\n",
        "      plt.ylabel('$y$', fontsize=16)\n",
        "      #plt.tick_params(labelsize=30)\n",
        "      plt.xticks(np.arange(-4, 5, 2),fontsize=14)\n",
        "      plt.yticks(np.arange(-4, 7, 2),fontsize=14)\n",
        "      #plt.gca().set_yticklabels([])\n",
        "      plt.gca().yaxis.grid(alpha=0.3)\n",
        "      plt.gca().xaxis.grid(alpha=0.3)\n",
        "      plt.savefig('sgld_hetero.pdf', bbox_inches = 'tight')\n",
        "\n",
        "      #files.download(\"sgld_hetero.pdf\")\n",
        "\n",
        "      plt.show()\n",
        "print(average_mse,np.median(average_mse))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =  161.501\n",
            "Epoch:   50, Train loss =  441.799\n",
            "Epoch:  100, Train loss =  379.827\n",
            "Epoch:  150, Train loss =  138.985\n",
            "Epoch:  200, Train loss =  124.910\n",
            "Using 1 networks for prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"331.634375pt\" version=\"1.1\" viewBox=\"0 0 396.324063 331.634375\" width=\"396.324063pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 331.634375 \nL 396.324063 331.634375 \nL 396.324063 0 \nL 0 0 \nz\n\" style=\"fill:#ffffff;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 54.324063 284.4 \nL 389.124063 284.4 \nL 389.124063 7.2 \nL 54.324063 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"PathCollection_1\">\n    <defs>\n     <path d=\"M -1.581139 1.581139 \nL 1.581139 -1.581139 \nM -1.581139 -1.581139 \nL 1.581139 1.581139 \n\" id=\"m06e28193ea\" style=\"stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\"/>\n    </defs>\n    <g clip-path=\"url(#pcc99a7a769)\">\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"162.766321\" xlink:href=\"#m06e28193ea\" y=\"186.474353\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"162.809806\" xlink:href=\"#m06e28193ea\" y=\"188.470501\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"162.975168\" xlink:href=\"#m06e28193ea\" y=\"186.858613\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"163.698067\" xlink:href=\"#m06e28193ea\" y=\"186.611183\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"164.080535\" xlink:href=\"#m06e28193ea\" y=\"185.95654\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"165.175897\" xlink:href=\"#m06e28193ea\" y=\"185.301945\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"165.212848\" xlink:href=\"#m06e28193ea\" y=\"191.241443\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"165.532307\" xlink:href=\"#m06e28193ea\" y=\"184.277114\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"165.775519\" xlink:href=\"#m06e28193ea\" y=\"186.858444\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"165.878051\" xlink:href=\"#m06e28193ea\" y=\"184.920269\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"166.218523\" xlink:href=\"#m06e28193ea\" y=\"186.596162\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"167.693946\" xlink:href=\"#m06e28193ea\" y=\"185.609247\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"168.624174\" xlink:href=\"#m06e28193ea\" y=\"176.287675\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"169.423034\" xlink:href=\"#m06e28193ea\" y=\"182.659116\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"169.868709\" xlink:href=\"#m06e28193ea\" y=\"183.779569\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"169.918652\" xlink:href=\"#m06e28193ea\" y=\"188.05034\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"170.357212\" xlink:href=\"#m06e28193ea\" y=\"185.706423\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"170.626417\" xlink:href=\"#m06e28193ea\" y=\"189.278576\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"171.754181\" xlink:href=\"#m06e28193ea\" y=\"176.557464\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"172.224861\" xlink:href=\"#m06e28193ea\" y=\"175.58798\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"172.988034\" xlink:href=\"#m06e28193ea\" y=\"181.535598\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"173.148543\" xlink:href=\"#m06e28193ea\" y=\"179.245545\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"173.331588\" xlink:href=\"#m06e28193ea\" y=\"178.747641\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"174.364267\" xlink:href=\"#m06e28193ea\" y=\"180.350961\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"174.539895\" xlink:href=\"#m06e28193ea\" y=\"177.063499\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"174.563514\" xlink:href=\"#m06e28193ea\" y=\"181.780912\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"174.681807\" xlink:href=\"#m06e28193ea\" y=\"180.700013\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"176.342551\" xlink:href=\"#m06e28193ea\" y=\"181.88653\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"176.492798\" xlink:href=\"#m06e28193ea\" y=\"176.361755\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"177.160026\" xlink:href=\"#m06e28193ea\" y=\"173.067936\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"177.325097\" xlink:href=\"#m06e28193ea\" y=\"176.347715\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"178.471762\" xlink:href=\"#m06e28193ea\" y=\"173.874377\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"178.571235\" xlink:href=\"#m06e28193ea\" y=\"175.38302\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"180.037537\" xlink:href=\"#m06e28193ea\" y=\"171.90254\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"180.513635\" xlink:href=\"#m06e28193ea\" y=\"178.325526\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"180.582127\" xlink:href=\"#m06e28193ea\" y=\"184.856101\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"180.744026\" xlink:href=\"#m06e28193ea\" y=\"175.701818\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"180.760599\" xlink:href=\"#m06e28193ea\" y=\"170.548931\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"180.905347\" xlink:href=\"#m06e28193ea\" y=\"174.395862\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"181.34374\" xlink:href=\"#m06e28193ea\" y=\"176.435081\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"181.647673\" xlink:href=\"#m06e28193ea\" y=\"178.603472\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"181.926942\" xlink:href=\"#m06e28193ea\" y=\"166.469536\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"182.053901\" xlink:href=\"#m06e28193ea\" y=\"167.706484\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"182.662651\" xlink:href=\"#m06e28193ea\" y=\"170.551302\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"183.40234\" xlink:href=\"#m06e28193ea\" y=\"171.326404\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"185.103679\" xlink:href=\"#m06e28193ea\" y=\"164.55677\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"185.293759\" xlink:href=\"#m06e28193ea\" y=\"178.060779\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"185.775328\" xlink:href=\"#m06e28193ea\" y=\"172.695726\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"187.085108\" xlink:href=\"#m06e28193ea\" y=\"161.344532\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"187.625336\" xlink:href=\"#m06e28193ea\" y=\"161.4077\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"187.715963\" xlink:href=\"#m06e28193ea\" y=\"164.166146\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"187.985895\" xlink:href=\"#m06e28193ea\" y=\"182.026139\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"189.24953\" xlink:href=\"#m06e28193ea\" y=\"158.119838\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"189.716591\" xlink:href=\"#m06e28193ea\" y=\"171.566358\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"190.147341\" xlink:href=\"#m06e28193ea\" y=\"156.598023\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"190.410675\" xlink:href=\"#m06e28193ea\" y=\"150.26627\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"190.794198\" xlink:href=\"#m06e28193ea\" y=\"151.025965\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"191.135898\" xlink:href=\"#m06e28193ea\" y=\"169.490221\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"191.256994\" xlink:href=\"#m06e28193ea\" y=\"162.10405\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"191.356545\" xlink:href=\"#m06e28193ea\" y=\"154.14338\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"191.628136\" xlink:href=\"#m06e28193ea\" y=\"154.871793\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"192.012893\" xlink:href=\"#m06e28193ea\" y=\"158.159838\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"192.553366\" xlink:href=\"#m06e28193ea\" y=\"147.587196\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"192.761501\" xlink:href=\"#m06e28193ea\" y=\"151.101855\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"193.597487\" xlink:href=\"#m06e28193ea\" y=\"149.513181\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"193.811469\" xlink:href=\"#m06e28193ea\" y=\"158.033078\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"194.110635\" xlink:href=\"#m06e28193ea\" y=\"168.955005\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"194.32399\" xlink:href=\"#m06e28193ea\" y=\"142.475188\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"194.547043\" xlink:href=\"#m06e28193ea\" y=\"149.176675\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"194.662806\" xlink:href=\"#m06e28193ea\" y=\"147.074923\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"194.672159\" xlink:href=\"#m06e28193ea\" y=\"163.16191\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"194.701237\" xlink:href=\"#m06e28193ea\" y=\"148.430837\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"195.256596\" xlink:href=\"#m06e28193ea\" y=\"152.776275\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"195.782975\" xlink:href=\"#m06e28193ea\" y=\"147.055608\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"195.891888\" xlink:href=\"#m06e28193ea\" y=\"161.123052\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"195.913517\" xlink:href=\"#m06e28193ea\" y=\"132.806104\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"196.098294\" xlink:href=\"#m06e28193ea\" y=\"154.674221\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"196.355835\" xlink:href=\"#m06e28193ea\" y=\"153.375495\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"196.821374\" xlink:href=\"#m06e28193ea\" y=\"144.616826\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"197.432175\" xlink:href=\"#m06e28193ea\" y=\"149.758614\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"197.573945\" xlink:href=\"#m06e28193ea\" y=\"140.986321\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"197.75447\" xlink:href=\"#m06e28193ea\" y=\"151.465375\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"198.303276\" xlink:href=\"#m06e28193ea\" y=\"141.328631\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"199.28439\" xlink:href=\"#m06e28193ea\" y=\"156.585889\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"201.424095\" xlink:href=\"#m06e28193ea\" y=\"160.568159\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"203.058083\" xlink:href=\"#m06e28193ea\" y=\"138.315821\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"203.179972\" xlink:href=\"#m06e28193ea\" y=\"148.061981\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"203.922803\" xlink:href=\"#m06e28193ea\" y=\"143.968545\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"204.495593\" xlink:href=\"#m06e28193ea\" y=\"138.996\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"205.383298\" xlink:href=\"#m06e28193ea\" y=\"135.078235\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"205.966156\" xlink:href=\"#m06e28193ea\" y=\"132.273375\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"206.426971\" xlink:href=\"#m06e28193ea\" y=\"153.4158\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"206.78609\" xlink:href=\"#m06e28193ea\" y=\"149.073959\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"206.787432\" xlink:href=\"#m06e28193ea\" y=\"158.766424\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"208.251159\" xlink:href=\"#m06e28193ea\" y=\"118.031632\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"208.343022\" xlink:href=\"#m06e28193ea\" y=\"147.94209\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"208.975324\" xlink:href=\"#m06e28193ea\" y=\"139.840018\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"209.30343\" xlink:href=\"#m06e28193ea\" y=\"144.606783\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"209.92443\" xlink:href=\"#m06e28193ea\" y=\"117.935648\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"209.934259\" xlink:href=\"#m06e28193ea\" y=\"143.554163\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"210.375291\" xlink:href=\"#m06e28193ea\" y=\"141.315852\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"210.834852\" xlink:href=\"#m06e28193ea\" y=\"128.623237\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"210.998643\" xlink:href=\"#m06e28193ea\" y=\"125.707832\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"211.139726\" xlink:href=\"#m06e28193ea\" y=\"148.301856\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"211.510147\" xlink:href=\"#m06e28193ea\" y=\"158.767415\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"211.628548\" xlink:href=\"#m06e28193ea\" y=\"134.024652\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"212.16596\" xlink:href=\"#m06e28193ea\" y=\"137.585383\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"213.557406\" xlink:href=\"#m06e28193ea\" y=\"115.779355\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"214.022633\" xlink:href=\"#m06e28193ea\" y=\"150.045378\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"215.411108\" xlink:href=\"#m06e28193ea\" y=\"147.614076\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"215.594887\" xlink:href=\"#m06e28193ea\" y=\"145.781856\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"215.630666\" xlink:href=\"#m06e28193ea\" y=\"138.641893\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"216.255883\" xlink:href=\"#m06e28193ea\" y=\"142.889677\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"216.265782\" xlink:href=\"#m06e28193ea\" y=\"123.745154\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"217.292913\" xlink:href=\"#m06e28193ea\" y=\"126.997613\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"217.37281\" xlink:href=\"#m06e28193ea\" y=\"136.454669\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"218.63465\" xlink:href=\"#m06e28193ea\" y=\"131.663618\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"218.697017\" xlink:href=\"#m06e28193ea\" y=\"149.089504\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"218.790546\" xlink:href=\"#m06e28193ea\" y=\"140.01072\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"219.396102\" xlink:href=\"#m06e28193ea\" y=\"142.336829\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"220.401278\" xlink:href=\"#m06e28193ea\" y=\"148.552719\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"221.060256\" xlink:href=\"#m06e28193ea\" y=\"134.002508\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"221.123502\" xlink:href=\"#m06e28193ea\" y=\"129.229907\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"221.275702\" xlink:href=\"#m06e28193ea\" y=\"131.224335\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"222.592349\" xlink:href=\"#m06e28193ea\" y=\"148.400478\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"223.627419\" xlink:href=\"#m06e28193ea\" y=\"160.991253\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"223.804134\" xlink:href=\"#m06e28193ea\" y=\"116.997649\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"224.650085\" xlink:href=\"#m06e28193ea\" y=\"119.911894\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"224.684625\" xlink:href=\"#m06e28193ea\" y=\"133.624021\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"224.832201\" xlink:href=\"#m06e28193ea\" y=\"112.523887\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"225.423868\" xlink:href=\"#m06e28193ea\" y=\"122.985382\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"225.694755\" xlink:href=\"#m06e28193ea\" y=\"136.217899\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"226.836354\" xlink:href=\"#m06e28193ea\" y=\"147.00049\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"227.091087\" xlink:href=\"#m06e28193ea\" y=\"135.733469\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"227.183746\" xlink:href=\"#m06e28193ea\" y=\"161.661185\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"230.179184\" xlink:href=\"#m06e28193ea\" y=\"162.648409\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"230.609458\" xlink:href=\"#m06e28193ea\" y=\"149.579116\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"230.624037\" xlink:href=\"#m06e28193ea\" y=\"154.974258\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"231.036528\" xlink:href=\"#m06e28193ea\" y=\"128.600624\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"231.056244\" xlink:href=\"#m06e28193ea\" y=\"128.781399\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"231.512181\" xlink:href=\"#m06e28193ea\" y=\"143.208382\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"231.660282\" xlink:href=\"#m06e28193ea\" y=\"124.721284\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"233.960585\" xlink:href=\"#m06e28193ea\" y=\"160.216876\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"234.998614\" xlink:href=\"#m06e28193ea\" y=\"131.902698\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"235.981564\" xlink:href=\"#m06e28193ea\" y=\"164.778215\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"236.02242\" xlink:href=\"#m06e28193ea\" y=\"152.436283\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"236.311697\" xlink:href=\"#m06e28193ea\" y=\"147.696253\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"236.865325\" xlink:href=\"#m06e28193ea\" y=\"172.335145\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"237.324977\" xlink:href=\"#m06e28193ea\" y=\"150.771571\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"237.761882\" xlink:href=\"#m06e28193ea\" y=\"115.85224\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"237.978976\" xlink:href=\"#m06e28193ea\" y=\"164.023157\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"238.379187\" xlink:href=\"#m06e28193ea\" y=\"174.371627\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"238.419727\" xlink:href=\"#m06e28193ea\" y=\"125.972843\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"238.497491\" xlink:href=\"#m06e28193ea\" y=\"148.459493\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"239.004383\" xlink:href=\"#m06e28193ea\" y=\"143.359242\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"239.216209\" xlink:href=\"#m06e28193ea\" y=\"125.229124\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"239.441601\" xlink:href=\"#m06e28193ea\" y=\"128.368035\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"239.455453\" xlink:href=\"#m06e28193ea\" y=\"151.47648\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"239.501956\" xlink:href=\"#m06e28193ea\" y=\"142.027902\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"239.677487\" xlink:href=\"#m06e28193ea\" y=\"131.631302\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"240.0488\" xlink:href=\"#m06e28193ea\" y=\"156.685115\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"240.864939\" xlink:href=\"#m06e28193ea\" y=\"145.712988\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"241.79784\" xlink:href=\"#m06e28193ea\" y=\"136.370762\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"241.81069\" xlink:href=\"#m06e28193ea\" y=\"141.761688\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"242.081474\" xlink:href=\"#m06e28193ea\" y=\"133.010141\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"242.612397\" xlink:href=\"#m06e28193ea\" y=\"138.622972\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"242.736776\" xlink:href=\"#m06e28193ea\" y=\"191.995405\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"242.817329\" xlink:href=\"#m06e28193ea\" y=\"156.842272\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"244.17117\" xlink:href=\"#m06e28193ea\" y=\"173.49815\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"244.988686\" xlink:href=\"#m06e28193ea\" y=\"140.301658\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"246.671051\" xlink:href=\"#m06e28193ea\" y=\"167.129566\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"246.78474\" xlink:href=\"#m06e28193ea\" y=\"134.853487\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"247.594978\" xlink:href=\"#m06e28193ea\" y=\"143.43477\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"248.193227\" xlink:href=\"#m06e28193ea\" y=\"138.599113\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"248.422128\" xlink:href=\"#m06e28193ea\" y=\"144.676935\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"248.585028\" xlink:href=\"#m06e28193ea\" y=\"166.641867\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"248.685026\" xlink:href=\"#m06e28193ea\" y=\"118.947289\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"248.914417\" xlink:href=\"#m06e28193ea\" y=\"134.829281\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"249.472596\" xlink:href=\"#m06e28193ea\" y=\"144.688809\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"249.621811\" xlink:href=\"#m06e28193ea\" y=\"173.036393\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"249.806117\" xlink:href=\"#m06e28193ea\" y=\"139.452536\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"250.141506\" xlink:href=\"#m06e28193ea\" y=\"133.085439\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"250.319238\" xlink:href=\"#m06e28193ea\" y=\"153.241773\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"250.57851\" xlink:href=\"#m06e28193ea\" y=\"153.305586\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"252.312218\" xlink:href=\"#m06e28193ea\" y=\"177.174903\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"252.424529\" xlink:href=\"#m06e28193ea\" y=\"140.987872\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"252.491662\" xlink:href=\"#m06e28193ea\" y=\"156.146052\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"253.091413\" xlink:href=\"#m06e28193ea\" y=\"174.233734\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"253.103784\" xlink:href=\"#m06e28193ea\" y=\"139.54645\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"253.33178\" xlink:href=\"#m06e28193ea\" y=\"164.983422\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"253.747806\" xlink:href=\"#m06e28193ea\" y=\"159.397444\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"254.034484\" xlink:href=\"#m06e28193ea\" y=\"124.353619\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"255.655394\" xlink:href=\"#m06e28193ea\" y=\"139.618176\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"257.718279\" xlink:href=\"#m06e28193ea\" y=\"134.403398\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"261.868648\" xlink:href=\"#m06e28193ea\" y=\"184.574757\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"262.211996\" xlink:href=\"#m06e28193ea\" y=\"165.251611\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"262.491067\" xlink:href=\"#m06e28193ea\" y=\"167.759021\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"263.411931\" xlink:href=\"#m06e28193ea\" y=\"190.851678\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"263.918715\" xlink:href=\"#m06e28193ea\" y=\"187.65762\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"264.704158\" xlink:href=\"#m06e28193ea\" y=\"168.872111\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"264.77618\" xlink:href=\"#m06e28193ea\" y=\"205.084641\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"265.797111\" xlink:href=\"#m06e28193ea\" y=\"171.50628\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"266.216718\" xlink:href=\"#m06e28193ea\" y=\"140.298054\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"267.281511\" xlink:href=\"#m06e28193ea\" y=\"182.170448\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"268.045236\" xlink:href=\"#m06e28193ea\" y=\"182.089716\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"268.164304\" xlink:href=\"#m06e28193ea\" y=\"203.160578\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"269.621949\" xlink:href=\"#m06e28193ea\" y=\"215.437253\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"269.78475\" xlink:href=\"#m06e28193ea\" y=\"194.866528\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"270.487831\" xlink:href=\"#m06e28193ea\" y=\"224.695125\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"271.657971\" xlink:href=\"#m06e28193ea\" y=\"175.169792\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"271.726132\" xlink:href=\"#m06e28193ea\" y=\"199.25046\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"271.780277\" xlink:href=\"#m06e28193ea\" y=\"156.017906\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"273.121989\" xlink:href=\"#m06e28193ea\" y=\"188.543059\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"273.300002\" xlink:href=\"#m06e28193ea\" y=\"147.253516\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"273.316386\" xlink:href=\"#m06e28193ea\" y=\"183.761987\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"273.594411\" xlink:href=\"#m06e28193ea\" y=\"203.410936\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"273.771893\" xlink:href=\"#m06e28193ea\" y=\"146.295246\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"274.059638\" xlink:href=\"#m06e28193ea\" y=\"186.540167\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"274.763346\" xlink:href=\"#m06e28193ea\" y=\"169.32996\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"274.877443\" xlink:href=\"#m06e28193ea\" y=\"195.422909\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"275.139633\" xlink:href=\"#m06e28193ea\" y=\"207.18272\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"275.353625\" xlink:href=\"#m06e28193ea\" y=\"140.185789\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"275.42559\" xlink:href=\"#m06e28193ea\" y=\"158.29396\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"275.948139\" xlink:href=\"#m06e28193ea\" y=\"155.729807\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"276.013892\" xlink:href=\"#m06e28193ea\" y=\"235.287044\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"276.027825\" xlink:href=\"#m06e28193ea\" y=\"187.728176\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"276.133232\" xlink:href=\"#m06e28193ea\" y=\"191.493948\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"276.9687\" xlink:href=\"#m06e28193ea\" y=\"224.932117\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"277.394799\" xlink:href=\"#m06e28193ea\" y=\"197.06885\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"277.444558\" xlink:href=\"#m06e28193ea\" y=\"171.62949\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"277.872952\" xlink:href=\"#m06e28193ea\" y=\"174.046112\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"279.20133\" xlink:href=\"#m06e28193ea\" y=\"151.111998\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"279.780414\" xlink:href=\"#m06e28193ea\" y=\"218.615916\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"281.001234\" xlink:href=\"#m06e28193ea\" y=\"171.822343\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"281.213212\" xlink:href=\"#m06e28193ea\" y=\"198.6055\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"281.744249\" xlink:href=\"#m06e28193ea\" y=\"192.692168\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"282.083766\" xlink:href=\"#m06e28193ea\" y=\"204.231765\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"282.269602\" xlink:href=\"#m06e28193ea\" y=\"157.511959\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"282.489637\" xlink:href=\"#m06e28193ea\" y=\"194.389695\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"282.833998\" xlink:href=\"#m06e28193ea\" y=\"193.115556\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"283.131996\" xlink:href=\"#m06e28193ea\" y=\"182.946887\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"283.31983\" xlink:href=\"#m06e28193ea\" y=\"176.545275\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"283.550394\" xlink:href=\"#m06e28193ea\" y=\"153.920245\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"283.746578\" xlink:href=\"#m06e28193ea\" y=\"170.300761\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"284.05422\" xlink:href=\"#m06e28193ea\" y=\"181.102067\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"284.059985\" xlink:href=\"#m06e28193ea\" y=\"224.453322\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"284.241129\" xlink:href=\"#m06e28193ea\" y=\"188.170668\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"284.364524\" xlink:href=\"#m06e28193ea\" y=\"210.038279\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"284.379015\" xlink:href=\"#m06e28193ea\" y=\"146.337313\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"284.387412\" xlink:href=\"#m06e28193ea\" y=\"162.225272\"/>\n    </g>\n   </g>\n   <g id=\"PolyCollection_1\">\n    <defs>\n     <path d=\"M 54.324062 -107.664254 \nL 54.324062 -107.664254 \nL 56.006475 -108.397106 \nL 57.688887 -109.131653 \nL 59.371299 -109.867385 \nL 61.053711 -110.603822 \nL 62.736123 -111.341024 \nL 64.418535 -112.07897 \nL 66.100947 -112.823073 \nL 67.783359 -113.568471 \nL 69.465771 -114.315537 \nL 71.148183 -115.063583 \nL 72.830595 -115.81245 \nL 74.513007 -116.562146 \nL 76.195419 -117.312746 \nL 77.877831 -118.064196 \nL 79.560243 -118.816574 \nL 81.242655 -119.569858 \nL 82.925068 -120.324111 \nL 84.60748 -121.079318 \nL 86.289892 -121.83579 \nL 87.972304 -122.595951 \nL 89.654716 -123.357207 \nL 91.337128 -124.119589 \nL 93.01954 -124.88727 \nL 94.701952 -125.657349 \nL 96.384364 -126.428645 \nL 98.066776 -127.185184 \nL 99.749188 -127.942756 \nL 101.4316 -128.701806 \nL 103.114012 -129.462333 \nL 104.796424 -130.22439 \nL 106.478836 -131.008836 \nL 108.161248 -131.797772 \nL 109.84366 -132.588367 \nL 111.526073 -133.380658 \nL 113.208485 -134.174683 \nL 114.890897 -134.944352 \nL 116.573309 -135.6913 \nL 118.255721 -136.439553 \nL 119.938133 -137.1897 \nL 121.620545 -137.942486 \nL 123.302957 -138.695864 \nL 124.985369 -139.450632 \nL 126.667781 -140.206835 \nL 128.350193 -140.964516 \nL 130.032605 -141.723993 \nL 131.715017 -142.504273 \nL 133.397429 -143.290255 \nL 135.079841 -144.13224 \nL 136.762253 -144.978595 \nL 138.444666 -145.82566 \nL 140.127078 -146.680448 \nL 141.80949 -147.531601 \nL 143.491902 -148.37936 \nL 145.174314 -149.228108 \nL 146.856726 -150.07785 \nL 148.539138 -150.924935 \nL 150.22155 -151.757551 \nL 151.903962 -152.597385 \nL 153.586374 -153.437817 \nL 155.268786 -154.279579 \nL 156.951198 -155.122713 \nL 158.63361 -155.975343 \nL 160.316022 -156.874999 \nL 161.998434 -157.829371 \nL 163.680846 -158.742385 \nL 165.363258 -159.64626 \nL 167.045671 -160.551229 \nL 168.728083 -161.548861 \nL 170.410495 -162.616867 \nL 172.092907 -163.799416 \nL 173.775319 -165.035701 \nL 175.457731 -166.214585 \nL 177.140143 -167.420188 \nL 178.822555 -168.725107 \nL 180.504967 -170.103957 \nL 182.187379 -171.510954 \nL 183.869791 -172.773326 \nL 185.552203 -173.982776 \nL 187.234615 -175.187024 \nL 188.917027 -176.49102 \nL 190.599439 -177.806755 \nL 192.281851 -179.114626 \nL 193.964264 -180.355064 \nL 195.646676 -181.461125 \nL 197.329088 -182.641186 \nL 199.0115 -183.945418 \nL 200.693912 -185.173556 \nL 202.376324 -186.13085 \nL 204.058736 -187.060833 \nL 205.741148 -188.126854 \nL 207.42356 -189.164287 \nL 209.105972 -190.220895 \nL 210.788384 -191.253771 \nL 212.470796 -192.240977 \nL 214.153208 -193.300193 \nL 215.83562 -194.226411 \nL 217.518032 -195.014073 \nL 219.200444 -195.792749 \nL 220.882856 -196.62373 \nL 222.565269 -196.963442 \nL 224.247681 -197.079283 \nL 225.930093 -197.129484 \nL 227.612505 -197.032605 \nL 229.294917 -196.770422 \nL 230.977329 -196.298591 \nL 232.659741 -195.780462 \nL 234.342153 -195.364246 \nL 236.024565 -195.092453 \nL 237.706977 -194.803737 \nL 239.389389 -194.459292 \nL 241.071801 -193.964677 \nL 242.754213 -193.462137 \nL 244.436625 -193.043173 \nL 246.119037 -192.335193 \nL 247.801449 -191.506589 \nL 249.483861 -190.459132 \nL 251.166274 -189.405969 \nL 252.848686 -188.220172 \nL 254.531098 -187.033811 \nL 256.21351 -186.006271 \nL 257.895922 -185.121686 \nL 259.578334 -184.303958 \nL 261.260746 -183.362895 \nL 262.943158 -182.384012 \nL 264.62557 -181.361842 \nL 266.307982 -180.463885 \nL 267.990394 -179.556276 \nL 269.672806 -178.644307 \nL 271.355218 -177.823225 \nL 273.03763 -176.99016 \nL 274.720042 -176.110883 \nL 276.402454 -175.231609 \nL 278.084867 -174.352337 \nL 279.767279 -173.473061 \nL 281.449691 -172.593796 \nL 283.132103 -171.714537 \nL 284.814515 -170.842468 \nL 286.496927 -169.979972 \nL 288.179339 -169.127 \nL 289.861751 -168.281045 \nL 291.544163 -167.429745 \nL 293.226575 -166.532164 \nL 294.908987 -165.669487 \nL 296.591399 -164.818904 \nL 298.273811 -163.956631 \nL 299.956223 -163.080024 \nL 301.638635 -162.206173 \nL 303.321047 -161.332907 \nL 305.003459 -160.445909 \nL 306.685872 -159.558765 \nL 308.368284 -158.671628 \nL 310.050696 -157.799622 \nL 311.733108 -156.927864 \nL 313.41552 -156.056347 \nL 315.097932 -155.185045 \nL 316.780344 -154.313999 \nL 318.462756 -153.44318 \nL 320.145168 -152.631374 \nL 321.82758 -151.834844 \nL 323.509992 -151.03858 \nL 325.192404 -150.18506 \nL 326.874816 -149.310277 \nL 328.557228 -148.420581 \nL 330.23964 -147.531158 \nL 331.922052 -146.642003 \nL 333.604465 -145.768239 \nL 335.286877 -144.900797 \nL 336.969289 -144.028164 \nL 338.651701 -143.156969 \nL 340.334113 -142.308298 \nL 342.016525 -141.53044 \nL 343.698937 -140.816575 \nL 345.381349 -140.098489 \nL 347.063761 -139.380795 \nL 348.746173 -138.67347 \nL 350.428585 -138.004412 \nL 352.110997 -137.33542 \nL 353.793409 -136.667044 \nL 355.475821 -136.007952 \nL 357.158233 -135.348925 \nL 358.840645 -134.689433 \nL 360.523057 -134.022685 \nL 362.20547 -133.356058 \nL 363.887882 -132.689533 \nL 365.570294 -132.023121 \nL 367.252706 -131.356831 \nL 368.935118 -130.690622 \nL 370.61753 -130.024532 \nL 372.299942 -129.358547 \nL 373.982354 -128.692664 \nL 375.664766 -128.026899 \nL 377.347178 -127.359531 \nL 379.02959 -126.691836 \nL 380.712002 -126.024284 \nL 382.394414 -125.356842 \nL 384.076826 -124.689549 \nL 385.759238 -124.022347 \nL 387.44165 -123.355265 \nL 389.124063 -122.680168 \nL 389.124063 -122.680168 \nL 389.124063 -122.680168 \nL 387.44165 -123.355265 \nL 385.759238 -124.022347 \nL 384.076826 -124.689549 \nL 382.394414 -125.356842 \nL 380.712002 -126.024284 \nL 379.02959 -126.691836 \nL 377.347178 -127.359531 \nL 375.664766 -128.026899 \nL 373.982354 -128.692664 \nL 372.299942 -129.358547 \nL 370.61753 -130.024532 \nL 368.935118 -130.690622 \nL 367.252706 -131.356831 \nL 365.570294 -132.023121 \nL 363.887882 -132.689533 \nL 362.20547 -133.356058 \nL 360.523057 -134.022685 \nL 358.840645 -134.689433 \nL 357.158233 -135.348925 \nL 355.475821 -136.007952 \nL 353.793409 -136.667044 \nL 352.110997 -137.33542 \nL 350.428585 -138.004412 \nL 348.746173 -138.67347 \nL 347.063761 -139.380795 \nL 345.381349 -140.098489 \nL 343.698937 -140.816575 \nL 342.016525 -141.53044 \nL 340.334113 -142.308298 \nL 338.651701 -143.156969 \nL 336.969289 -144.028164 \nL 335.286877 -144.900797 \nL 333.604465 -145.768239 \nL 331.922052 -146.642003 \nL 330.23964 -147.531158 \nL 328.557228 -148.420581 \nL 326.874816 -149.310277 \nL 325.192404 -150.18506 \nL 323.509992 -151.03858 \nL 321.82758 -151.834844 \nL 320.145168 -152.631374 \nL 318.462756 -153.44318 \nL 316.780344 -154.313999 \nL 315.097932 -155.185045 \nL 313.41552 -156.056347 \nL 311.733108 -156.927864 \nL 310.050696 -157.799622 \nL 308.368284 -158.671628 \nL 306.685872 -159.558765 \nL 305.003459 -160.445909 \nL 303.321047 -161.332907 \nL 301.638635 -162.206173 \nL 299.956223 -163.080024 \nL 298.273811 -163.956631 \nL 296.591399 -164.818904 \nL 294.908987 -165.669487 \nL 293.226575 -166.532164 \nL 291.544163 -167.429745 \nL 289.861751 -168.281045 \nL 288.179339 -169.127 \nL 286.496927 -169.979972 \nL 284.814515 -170.842468 \nL 283.132103 -171.714537 \nL 281.449691 -172.593796 \nL 279.767279 -173.473061 \nL 278.084867 -174.352337 \nL 276.402454 -175.231609 \nL 274.720042 -176.110883 \nL 273.03763 -176.99016 \nL 271.355218 -177.823225 \nL 269.672806 -178.644307 \nL 267.990394 -179.556276 \nL 266.307982 -180.463885 \nL 264.62557 -181.361842 \nL 262.943158 -182.384012 \nL 261.260746 -183.362895 \nL 259.578334 -184.303958 \nL 257.895922 -185.121686 \nL 256.21351 -186.006271 \nL 254.531098 -187.033811 \nL 252.848686 -188.220172 \nL 251.166274 -189.405969 \nL 249.483861 -190.459132 \nL 247.801449 -191.506589 \nL 246.119037 -192.335193 \nL 244.436625 -193.043173 \nL 242.754213 -193.462137 \nL 241.071801 -193.964677 \nL 239.389389 -194.459292 \nL 237.706977 -194.803737 \nL 236.024565 -195.092453 \nL 234.342153 -195.364246 \nL 232.659741 -195.780462 \nL 230.977329 -196.298591 \nL 229.294917 -196.770422 \nL 227.612505 -197.032605 \nL 225.930093 -197.129484 \nL 224.247681 -197.079283 \nL 222.565269 -196.963442 \nL 220.882856 -196.62373 \nL 219.200444 -195.792749 \nL 217.518032 -195.014073 \nL 215.83562 -194.226411 \nL 214.153208 -193.300193 \nL 212.470796 -192.240977 \nL 210.788384 -191.253771 \nL 209.105972 -190.220895 \nL 207.42356 -189.164287 \nL 205.741148 -188.126854 \nL 204.058736 -187.060833 \nL 202.376324 -186.13085 \nL 200.693912 -185.173556 \nL 199.0115 -183.945418 \nL 197.329088 -182.641186 \nL 195.646676 -181.461125 \nL 193.964264 -180.355064 \nL 192.281851 -179.114626 \nL 190.599439 -177.806755 \nL 188.917027 -176.49102 \nL 187.234615 -175.187024 \nL 185.552203 -173.982776 \nL 183.869791 -172.773326 \nL 182.187379 -171.510954 \nL 180.504967 -170.103957 \nL 178.822555 -168.725107 \nL 177.140143 -167.420188 \nL 175.457731 -166.214585 \nL 173.775319 -165.035701 \nL 172.092907 -163.799416 \nL 170.410495 -162.616867 \nL 168.728083 -161.548861 \nL 167.045671 -160.551229 \nL 165.363258 -159.64626 \nL 163.680846 -158.742385 \nL 161.998434 -157.829371 \nL 160.316022 -156.874999 \nL 158.63361 -155.975343 \nL 156.951198 -155.122713 \nL 155.268786 -154.279579 \nL 153.586374 -153.437817 \nL 151.903962 -152.597385 \nL 150.22155 -151.757551 \nL 148.539138 -150.924935 \nL 146.856726 -150.07785 \nL 145.174314 -149.228108 \nL 143.491902 -148.37936 \nL 141.80949 -147.531601 \nL 140.127078 -146.680448 \nL 138.444666 -145.82566 \nL 136.762253 -144.978595 \nL 135.079841 -144.13224 \nL 133.397429 -143.290255 \nL 131.715017 -142.504273 \nL 130.032605 -141.723993 \nL 128.350193 -140.964516 \nL 126.667781 -140.206835 \nL 124.985369 -139.450632 \nL 123.302957 -138.695864 \nL 121.620545 -137.942486 \nL 119.938133 -137.1897 \nL 118.255721 -136.439553 \nL 116.573309 -135.6913 \nL 114.890897 -134.944352 \nL 113.208485 -134.174683 \nL 111.526073 -133.380658 \nL 109.84366 -132.588367 \nL 108.161248 -131.797772 \nL 106.478836 -131.008836 \nL 104.796424 -130.22439 \nL 103.114012 -129.462333 \nL 101.4316 -128.701806 \nL 99.749188 -127.942756 \nL 98.066776 -127.185184 \nL 96.384364 -126.428645 \nL 94.701952 -125.657349 \nL 93.01954 -124.88727 \nL 91.337128 -124.119589 \nL 89.654716 -123.357207 \nL 87.972304 -122.595951 \nL 86.289892 -121.83579 \nL 84.60748 -121.079318 \nL 82.925068 -120.324111 \nL 81.242655 -119.569858 \nL 79.560243 -118.816574 \nL 77.877831 -118.064196 \nL 76.195419 -117.312746 \nL 74.513007 -116.562146 \nL 72.830595 -115.81245 \nL 71.148183 -115.063583 \nL 69.465771 -114.315537 \nL 67.783359 -113.568471 \nL 66.100947 -112.823073 \nL 64.418535 -112.07897 \nL 62.736123 -111.341024 \nL 61.053711 -110.603822 \nL 59.371299 -109.867385 \nL 57.688887 -109.131653 \nL 56.006475 -108.397106 \nL 54.324062 -107.664254 \nz\n\" id=\"mb5683dd5b5\" style=\"stroke:#1f77b4;stroke-opacity:0.3;\"/>\n    </defs>\n    <g clip-path=\"url(#pcc99a7a769)\">\n     <use style=\"fill:#1f77b4;fill-opacity:0.3;stroke:#1f77b4;stroke-opacity:0.3;\" x=\"0\" xlink:href=\"#mb5683dd5b5\" y=\"331.634375\"/>\n    </g>\n   </g>\n   <g id=\"PolyCollection_2\">\n    <defs>\n     <path d=\"M 54.324062 -105.267613 \nL 54.324062 -105.267613 \nL 56.006475 -105.94235 \nL 57.688887 -106.617681 \nL 59.371299 -107.29293 \nL 61.053711 -107.96743 \nL 62.736123 -108.641208 \nL 64.418535 -109.314199 \nL 66.100947 -109.992443 \nL 67.783359 -110.670462 \nL 69.465771 -111.349093 \nL 71.148183 -112.027183 \nL 72.830595 -112.704453 \nL 74.513007 -113.380859 \nL 76.195419 -114.056443 \nL 77.877831 -114.731108 \nL 79.560243 -115.404887 \nL 81.242655 -116.077723 \nL 82.925068 -116.749626 \nL 84.60748 -117.420546 \nL 86.289892 -118.090552 \nL 87.972304 -118.760257 \nL 89.654716 -119.428872 \nL 91.337128 -120.096378 \nL 93.01954 -120.766697 \nL 94.701952 -121.437003 \nL 96.384364 -122.106103 \nL 98.066776 -122.752123 \nL 99.749188 -123.396219 \nL 101.4316 -124.038886 \nL 103.114012 -124.680049 \nL 104.796424 -125.319692 \nL 106.478836 -125.978341 \nL 108.161248 -126.638221 \nL 109.84366 -127.296449 \nL 111.526073 -127.952981 \nL 113.208485 -128.607761 \nL 114.890897 -129.246996 \nL 116.573309 -129.872119 \nL 118.255721 -130.495936 \nL 119.938133 -131.119197 \nL 121.620545 -131.742871 \nL 123.302957 -132.364251 \nL 124.985369 -132.984213 \nL 126.667781 -133.602746 \nL 128.350193 -134.21982 \nL 130.032605 -134.835965 \nL 131.715017 -135.486755 \nL 133.397429 -136.143163 \nL 135.079841 -136.879316 \nL 136.762253 -137.624267 \nL 138.444666 -138.368514 \nL 140.127078 -139.109387 \nL 141.80949 -139.840643 \nL 143.491902 -140.565542 \nL 145.174314 -141.289466 \nL 146.856726 -142.012387 \nL 148.539138 -142.73199 \nL 150.22155 -143.443343 \nL 151.903962 -144.160879 \nL 153.586374 -144.851556 \nL 155.268786 -145.539883 \nL 156.951198 -146.226839 \nL 158.63361 -146.922176 \nL 160.316022 -147.683025 \nL 161.998434 -148.496741 \nL 163.680846 -149.261445 \nL 165.363258 -150.011984 \nL 167.045671 -150.760919 \nL 168.728083 -151.609057 \nL 170.410495 -152.544303 \nL 172.092907 -153.568735 \nL 173.775319 -154.65611 \nL 175.457731 -155.616779 \nL 177.140143 -156.607906 \nL 178.822555 -157.762247 \nL 180.504967 -159.001456 \nL 182.187379 -160.252058 \nL 183.869791 -161.371716 \nL 185.552203 -162.440023 \nL 187.234615 -163.499303 \nL 188.917027 -164.544031 \nL 190.599439 -165.573394 \nL 192.281851 -166.594417 \nL 193.964264 -167.514086 \nL 195.646676 -168.284205 \nL 197.329088 -169.113552 \nL 199.0115 -170.012348 \nL 200.693912 -170.809076 \nL 202.376324 -171.502356 \nL 204.058736 -172.222681 \nL 205.741148 -172.995932 \nL 207.42356 -173.71791 \nL 209.105972 -174.435541 \nL 210.788384 -175.131016 \nL 212.470796 -175.804759 \nL 214.153208 -176.511995 \nL 215.83562 -177.09121 \nL 217.518032 -177.425899 \nL 219.200444 -177.654601 \nL 220.882856 -177.968759 \nL 222.565269 -177.762835 \nL 224.247681 -177.504064 \nL 225.930093 -177.166973 \nL 227.612505 -176.732619 \nL 229.294917 -176.20768 \nL 230.977329 -175.485595 \nL 232.659741 -174.592784 \nL 234.342153 -173.7734 \nL 236.024565 -173.113676 \nL 237.706977 -172.710245 \nL 239.389389 -172.425888 \nL 241.071801 -172.028914 \nL 242.754213 -171.59824 \nL 244.436625 -171.151464 \nL 246.119037 -170.41422 \nL 247.801449 -169.557218 \nL 249.483861 -168.414253 \nL 251.166274 -167.245746 \nL 252.848686 -166.079025 \nL 254.531098 -164.928653 \nL 256.21351 -163.8667 \nL 257.895922 -162.816269 \nL 259.578334 -161.75924 \nL 261.260746 -160.646371 \nL 262.943158 -159.526269 \nL 264.62557 -158.374776 \nL 266.307982 -157.403968 \nL 267.990394 -156.425877 \nL 269.672806 -155.456451 \nL 271.355218 -154.558007 \nL 273.03763 -153.66827 \nL 274.720042 -152.799617 \nL 276.402454 -151.930966 \nL 278.084867 -151.062305 \nL 279.767279 -150.193634 \nL 281.449691 -149.324982 \nL 283.132103 -148.456316 \nL 284.814515 -147.587739 \nL 286.496927 -146.719291 \nL 288.179339 -145.849966 \nL 289.861751 -144.987915 \nL 291.544163 -144.127657 \nL 293.226575 -143.27845 \nL 294.908987 -142.497433 \nL 296.591399 -141.731888 \nL 298.273811 -140.981728 \nL 299.956223 -140.262733 \nL 301.638635 -139.55263 \nL 303.321047 -138.841933 \nL 305.003459 -138.08413 \nL 306.685872 -137.326789 \nL 308.368284 -136.551718 \nL 310.050696 -135.78239 \nL 311.733108 -135.012824 \nL 313.41552 -134.243036 \nL 315.097932 -133.472987 \nL 316.780344 -132.702724 \nL 318.462756 -131.932223 \nL 320.145168 -131.227245 \nL 321.82758 -130.538787 \nL 323.509992 -129.850048 \nL 325.192404 -129.113341 \nL 326.874816 -128.358366 \nL 328.557228 -127.575196 \nL 330.23964 -126.791753 \nL 331.922052 -126.008037 \nL 333.604465 -125.20005 \nL 335.286877 -124.379958 \nL 336.969289 -123.540223 \nL 338.651701 -122.698957 \nL 340.334113 -121.873258 \nL 342.016525 -121.118189 \nL 343.698937 -120.423777 \nL 345.381349 -119.741099 \nL 347.063761 -119.059539 \nL 348.746173 -118.390199 \nL 350.428585 -117.76612 \nL 352.110997 -117.142009 \nL 353.793409 -116.518613 \nL 355.475821 -115.907834 \nL 357.158233 -115.297001 \nL 358.840645 -114.686801 \nL 360.523057 -114.086404 \nL 362.20547 -113.485903 \nL 363.887882 -112.885297 \nL 365.570294 -112.284564 \nL 367.252706 -111.683743 \nL 368.935118 -111.082779 \nL 370.61753 -110.481749 \nL 372.299942 -109.88057 \nL 373.982354 -109.279298 \nL 375.664766 -108.677926 \nL 377.347178 -108.078444 \nL 379.02959 -107.47927 \nL 380.712002 -106.87998 \nL 382.394414 -106.280547 \nL 384.076826 -105.681037 \nL 385.759238 -105.081378 \nL 387.44165 -104.481609 \nL 389.124063 -103.884203 \nL 389.124063 -103.884203 \nL 389.124063 -103.884203 \nL 387.44165 -104.481609 \nL 385.759238 -105.081378 \nL 384.076826 -105.681037 \nL 382.394414 -106.280547 \nL 380.712002 -106.87998 \nL 379.02959 -107.47927 \nL 377.347178 -108.078444 \nL 375.664766 -108.677926 \nL 373.982354 -109.279298 \nL 372.299942 -109.88057 \nL 370.61753 -110.481749 \nL 368.935118 -111.082779 \nL 367.252706 -111.683743 \nL 365.570294 -112.284564 \nL 363.887882 -112.885297 \nL 362.20547 -113.485903 \nL 360.523057 -114.086404 \nL 358.840645 -114.686801 \nL 357.158233 -115.297001 \nL 355.475821 -115.907834 \nL 353.793409 -116.518613 \nL 352.110997 -117.142009 \nL 350.428585 -117.76612 \nL 348.746173 -118.390199 \nL 347.063761 -119.059539 \nL 345.381349 -119.741099 \nL 343.698937 -120.423777 \nL 342.016525 -121.118189 \nL 340.334113 -121.873258 \nL 338.651701 -122.698957 \nL 336.969289 -123.540223 \nL 335.286877 -124.379958 \nL 333.604465 -125.20005 \nL 331.922052 -126.008037 \nL 330.23964 -126.791753 \nL 328.557228 -127.575196 \nL 326.874816 -128.358366 \nL 325.192404 -129.113341 \nL 323.509992 -129.850048 \nL 321.82758 -130.538787 \nL 320.145168 -131.227245 \nL 318.462756 -131.932223 \nL 316.780344 -132.702724 \nL 315.097932 -133.472987 \nL 313.41552 -134.243036 \nL 311.733108 -135.012824 \nL 310.050696 -135.78239 \nL 308.368284 -136.551718 \nL 306.685872 -137.326789 \nL 305.003459 -138.08413 \nL 303.321047 -138.841933 \nL 301.638635 -139.55263 \nL 299.956223 -140.262733 \nL 298.273811 -140.981728 \nL 296.591399 -141.731888 \nL 294.908987 -142.497433 \nL 293.226575 -143.27845 \nL 291.544163 -144.127657 \nL 289.861751 -144.987915 \nL 288.179339 -145.849966 \nL 286.496927 -146.719291 \nL 284.814515 -147.587739 \nL 283.132103 -148.456316 \nL 281.449691 -149.324982 \nL 279.767279 -150.193634 \nL 278.084867 -151.062305 \nL 276.402454 -151.930966 \nL 274.720042 -152.799617 \nL 273.03763 -153.66827 \nL 271.355218 -154.558007 \nL 269.672806 -155.456451 \nL 267.990394 -156.425877 \nL 266.307982 -157.403968 \nL 264.62557 -158.374776 \nL 262.943158 -159.526269 \nL 261.260746 -160.646371 \nL 259.578334 -161.75924 \nL 257.895922 -162.816269 \nL 256.21351 -163.8667 \nL 254.531098 -164.928653 \nL 252.848686 -166.079025 \nL 251.166274 -167.245746 \nL 249.483861 -168.414253 \nL 247.801449 -169.557218 \nL 246.119037 -170.41422 \nL 244.436625 -171.151464 \nL 242.754213 -171.59824 \nL 241.071801 -172.028914 \nL 239.389389 -172.425888 \nL 237.706977 -172.710245 \nL 236.024565 -173.113676 \nL 234.342153 -173.7734 \nL 232.659741 -174.592784 \nL 230.977329 -175.485595 \nL 229.294917 -176.20768 \nL 227.612505 -176.732619 \nL 225.930093 -177.166973 \nL 224.247681 -177.504064 \nL 222.565269 -177.762835 \nL 220.882856 -177.968759 \nL 219.200444 -177.654601 \nL 217.518032 -177.425899 \nL 215.83562 -177.09121 \nL 214.153208 -176.511995 \nL 212.470796 -175.804759 \nL 210.788384 -175.131016 \nL 209.105972 -174.435541 \nL 207.42356 -173.71791 \nL 205.741148 -172.995932 \nL 204.058736 -172.222681 \nL 202.376324 -171.502356 \nL 200.693912 -170.809076 \nL 199.0115 -170.012348 \nL 197.329088 -169.113552 \nL 195.646676 -168.284205 \nL 193.964264 -167.514086 \nL 192.281851 -166.594417 \nL 190.599439 -165.573394 \nL 188.917027 -164.544031 \nL 187.234615 -163.499303 \nL 185.552203 -162.440023 \nL 183.869791 -161.371716 \nL 182.187379 -160.252058 \nL 180.504967 -159.001456 \nL 178.822555 -157.762247 \nL 177.140143 -156.607906 \nL 175.457731 -155.616779 \nL 173.775319 -154.65611 \nL 172.092907 -153.568735 \nL 170.410495 -152.544303 \nL 168.728083 -151.609057 \nL 167.045671 -150.760919 \nL 165.363258 -150.011984 \nL 163.680846 -149.261445 \nL 161.998434 -148.496741 \nL 160.316022 -147.683025 \nL 158.63361 -146.922176 \nL 156.951198 -146.226839 \nL 155.268786 -145.539883 \nL 153.586374 -144.851556 \nL 151.903962 -144.160879 \nL 150.22155 -143.443343 \nL 148.539138 -142.73199 \nL 146.856726 -142.012387 \nL 145.174314 -141.289466 \nL 143.491902 -140.565542 \nL 141.80949 -139.840643 \nL 140.127078 -139.109387 \nL 138.444666 -138.368514 \nL 136.762253 -137.624267 \nL 135.079841 -136.879316 \nL 133.397429 -136.143163 \nL 131.715017 -135.486755 \nL 130.032605 -134.835965 \nL 128.350193 -134.21982 \nL 126.667781 -133.602746 \nL 124.985369 -132.984213 \nL 123.302957 -132.364251 \nL 121.620545 -131.742871 \nL 119.938133 -131.119197 \nL 118.255721 -130.495936 \nL 116.573309 -129.872119 \nL 114.890897 -129.246996 \nL 113.208485 -128.607761 \nL 111.526073 -127.952981 \nL 109.84366 -127.296449 \nL 108.161248 -126.638221 \nL 106.478836 -125.978341 \nL 104.796424 -125.319692 \nL 103.114012 -124.680049 \nL 101.4316 -124.038886 \nL 99.749188 -123.396219 \nL 98.066776 -122.752123 \nL 96.384364 -122.106103 \nL 94.701952 -121.437003 \nL 93.01954 -120.766697 \nL 91.337128 -120.096378 \nL 89.654716 -119.428872 \nL 87.972304 -118.760257 \nL 86.289892 -118.090552 \nL 84.60748 -117.420546 \nL 82.925068 -116.749626 \nL 81.242655 -116.077723 \nL 79.560243 -115.404887 \nL 77.877831 -114.731108 \nL 76.195419 -114.056443 \nL 74.513007 -113.380859 \nL 72.830595 -112.704453 \nL 71.148183 -112.027183 \nL 69.465771 -111.349093 \nL 67.783359 -110.670462 \nL 66.100947 -109.992443 \nL 64.418535 -109.314199 \nL 62.736123 -108.641208 \nL 61.053711 -107.96743 \nL 59.371299 -107.29293 \nL 57.688887 -106.617681 \nL 56.006475 -105.94235 \nL 54.324062 -105.267613 \nz\n\" id=\"m837797e44c\" style=\"stroke:#1f77b4;stroke-opacity:0.3;\"/>\n    </defs>\n    <g clip-path=\"url(#pcc99a7a769)\">\n     <use style=\"fill:#1f77b4;fill-opacity:0.3;stroke:#1f77b4;stroke-opacity:0.3;\" x=\"0\" xlink:href=\"#m837797e44c\" y=\"331.634375\"/>\n    </g>\n   </g>\n   <g id=\"PolyCollection_3\">\n    <defs>\n     <path d=\"M 54.324062 -107.664254 \nL 54.324062 -105.267613 \nL 56.006475 -105.94235 \nL 57.688887 -106.617681 \nL 59.371299 -107.29293 \nL 61.053711 -107.96743 \nL 62.736123 -108.641208 \nL 64.418535 -109.314199 \nL 66.100947 -109.992443 \nL 67.783359 -110.670462 \nL 69.465771 -111.349093 \nL 71.148183 -112.027183 \nL 72.830595 -112.704453 \nL 74.513007 -113.380859 \nL 76.195419 -114.056443 \nL 77.877831 -114.731108 \nL 79.560243 -115.404887 \nL 81.242655 -116.077723 \nL 82.925068 -116.749626 \nL 84.60748 -117.420546 \nL 86.289892 -118.090552 \nL 87.972304 -118.760257 \nL 89.654716 -119.428872 \nL 91.337128 -120.096378 \nL 93.01954 -120.766697 \nL 94.701952 -121.437003 \nL 96.384364 -122.106103 \nL 98.066776 -122.752123 \nL 99.749188 -123.396219 \nL 101.4316 -124.038886 \nL 103.114012 -124.680049 \nL 104.796424 -125.319692 \nL 106.478836 -125.978341 \nL 108.161248 -126.638221 \nL 109.84366 -127.296449 \nL 111.526073 -127.952981 \nL 113.208485 -128.607761 \nL 114.890897 -129.246996 \nL 116.573309 -129.872119 \nL 118.255721 -130.495936 \nL 119.938133 -131.119197 \nL 121.620545 -131.742871 \nL 123.302957 -132.364251 \nL 124.985369 -132.984213 \nL 126.667781 -133.602746 \nL 128.350193 -134.21982 \nL 130.032605 -134.835965 \nL 131.715017 -135.486755 \nL 133.397429 -136.143163 \nL 135.079841 -136.879316 \nL 136.762253 -137.624267 \nL 138.444666 -138.368514 \nL 140.127078 -139.109387 \nL 141.80949 -139.840643 \nL 143.491902 -140.565542 \nL 145.174314 -141.289466 \nL 146.856726 -142.012387 \nL 148.539138 -142.73199 \nL 150.22155 -143.443343 \nL 151.903962 -144.160879 \nL 153.586374 -144.851556 \nL 155.268786 -145.539883 \nL 156.951198 -146.226839 \nL 158.63361 -146.922176 \nL 160.316022 -147.683025 \nL 161.998434 -148.496741 \nL 163.680846 -149.261445 \nL 165.363258 -150.011984 \nL 167.045671 -150.760919 \nL 168.728083 -151.609057 \nL 170.410495 -152.544303 \nL 172.092907 -153.568735 \nL 173.775319 -154.65611 \nL 175.457731 -155.616779 \nL 177.140143 -156.607906 \nL 178.822555 -157.762247 \nL 180.504967 -159.001456 \nL 182.187379 -160.252058 \nL 183.869791 -161.371716 \nL 185.552203 -162.440023 \nL 187.234615 -163.499303 \nL 188.917027 -164.544031 \nL 190.599439 -165.573394 \nL 192.281851 -166.594417 \nL 193.964264 -167.514086 \nL 195.646676 -168.284205 \nL 197.329088 -169.113552 \nL 199.0115 -170.012348 \nL 200.693912 -170.809076 \nL 202.376324 -171.502356 \nL 204.058736 -172.222681 \nL 205.741148 -172.995932 \nL 207.42356 -173.71791 \nL 209.105972 -174.435541 \nL 210.788384 -175.131016 \nL 212.470796 -175.804759 \nL 214.153208 -176.511995 \nL 215.83562 -177.09121 \nL 217.518032 -177.425899 \nL 219.200444 -177.654601 \nL 220.882856 -177.968759 \nL 222.565269 -177.762835 \nL 224.247681 -177.504064 \nL 225.930093 -177.166973 \nL 227.612505 -176.732619 \nL 229.294917 -176.20768 \nL 230.977329 -175.485595 \nL 232.659741 -174.592784 \nL 234.342153 -173.7734 \nL 236.024565 -173.113676 \nL 237.706977 -172.710245 \nL 239.389389 -172.425888 \nL 241.071801 -172.028914 \nL 242.754213 -171.59824 \nL 244.436625 -171.151464 \nL 246.119037 -170.41422 \nL 247.801449 -169.557218 \nL 249.483861 -168.414253 \nL 251.166274 -167.245746 \nL 252.848686 -166.079025 \nL 254.531098 -164.928653 \nL 256.21351 -163.8667 \nL 257.895922 -162.816269 \nL 259.578334 -161.75924 \nL 261.260746 -160.646371 \nL 262.943158 -159.526269 \nL 264.62557 -158.374776 \nL 266.307982 -157.403968 \nL 267.990394 -156.425877 \nL 269.672806 -155.456451 \nL 271.355218 -154.558007 \nL 273.03763 -153.66827 \nL 274.720042 -152.799617 \nL 276.402454 -151.930966 \nL 278.084867 -151.062305 \nL 279.767279 -150.193634 \nL 281.449691 -149.324982 \nL 283.132103 -148.456316 \nL 284.814515 -147.587739 \nL 286.496927 -146.719291 \nL 288.179339 -145.849966 \nL 289.861751 -144.987915 \nL 291.544163 -144.127657 \nL 293.226575 -143.27845 \nL 294.908987 -142.497433 \nL 296.591399 -141.731888 \nL 298.273811 -140.981728 \nL 299.956223 -140.262733 \nL 301.638635 -139.55263 \nL 303.321047 -138.841933 \nL 305.003459 -138.08413 \nL 306.685872 -137.326789 \nL 308.368284 -136.551718 \nL 310.050696 -135.78239 \nL 311.733108 -135.012824 \nL 313.41552 -134.243036 \nL 315.097932 -133.472987 \nL 316.780344 -132.702724 \nL 318.462756 -131.932223 \nL 320.145168 -131.227245 \nL 321.82758 -130.538787 \nL 323.509992 -129.850048 \nL 325.192404 -129.113341 \nL 326.874816 -128.358366 \nL 328.557228 -127.575196 \nL 330.23964 -126.791753 \nL 331.922052 -126.008037 \nL 333.604465 -125.20005 \nL 335.286877 -124.379958 \nL 336.969289 -123.540223 \nL 338.651701 -122.698957 \nL 340.334113 -121.873258 \nL 342.016525 -121.118189 \nL 343.698937 -120.423777 \nL 345.381349 -119.741099 \nL 347.063761 -119.059539 \nL 348.746173 -118.390199 \nL 350.428585 -117.76612 \nL 352.110997 -117.142009 \nL 353.793409 -116.518613 \nL 355.475821 -115.907834 \nL 357.158233 -115.297001 \nL 358.840645 -114.686801 \nL 360.523057 -114.086404 \nL 362.20547 -113.485903 \nL 363.887882 -112.885297 \nL 365.570294 -112.284564 \nL 367.252706 -111.683743 \nL 368.935118 -111.082779 \nL 370.61753 -110.481749 \nL 372.299942 -109.88057 \nL 373.982354 -109.279298 \nL 375.664766 -108.677926 \nL 377.347178 -108.078444 \nL 379.02959 -107.47927 \nL 380.712002 -106.87998 \nL 382.394414 -106.280547 \nL 384.076826 -105.681037 \nL 385.759238 -105.081378 \nL 387.44165 -104.481609 \nL 389.124063 -103.884203 \nL 389.124063 -122.680168 \nL 389.124063 -122.680168 \nL 387.44165 -123.355265 \nL 385.759238 -124.022347 \nL 384.076826 -124.689549 \nL 382.394414 -125.356842 \nL 380.712002 -126.024284 \nL 379.02959 -126.691836 \nL 377.347178 -127.359531 \nL 375.664766 -128.026899 \nL 373.982354 -128.692664 \nL 372.299942 -129.358547 \nL 370.61753 -130.024532 \nL 368.935118 -130.690622 \nL 367.252706 -131.356831 \nL 365.570294 -132.023121 \nL 363.887882 -132.689533 \nL 362.20547 -133.356058 \nL 360.523057 -134.022685 \nL 358.840645 -134.689433 \nL 357.158233 -135.348925 \nL 355.475821 -136.007952 \nL 353.793409 -136.667044 \nL 352.110997 -137.33542 \nL 350.428585 -138.004412 \nL 348.746173 -138.67347 \nL 347.063761 -139.380795 \nL 345.381349 -140.098489 \nL 343.698937 -140.816575 \nL 342.016525 -141.53044 \nL 340.334113 -142.308298 \nL 338.651701 -143.156969 \nL 336.969289 -144.028164 \nL 335.286877 -144.900797 \nL 333.604465 -145.768239 \nL 331.922052 -146.642003 \nL 330.23964 -147.531158 \nL 328.557228 -148.420581 \nL 326.874816 -149.310277 \nL 325.192404 -150.18506 \nL 323.509992 -151.03858 \nL 321.82758 -151.834844 \nL 320.145168 -152.631374 \nL 318.462756 -153.44318 \nL 316.780344 -154.313999 \nL 315.097932 -155.185045 \nL 313.41552 -156.056347 \nL 311.733108 -156.927864 \nL 310.050696 -157.799622 \nL 308.368284 -158.671628 \nL 306.685872 -159.558765 \nL 305.003459 -160.445909 \nL 303.321047 -161.332907 \nL 301.638635 -162.206173 \nL 299.956223 -163.080024 \nL 298.273811 -163.956631 \nL 296.591399 -164.818904 \nL 294.908987 -165.669487 \nL 293.226575 -166.532164 \nL 291.544163 -167.429745 \nL 289.861751 -168.281045 \nL 288.179339 -169.127 \nL 286.496927 -169.979972 \nL 284.814515 -170.842468 \nL 283.132103 -171.714537 \nL 281.449691 -172.593796 \nL 279.767279 -173.473061 \nL 278.084867 -174.352337 \nL 276.402454 -175.231609 \nL 274.720042 -176.110883 \nL 273.03763 -176.99016 \nL 271.355218 -177.823225 \nL 269.672806 -178.644307 \nL 267.990394 -179.556276 \nL 266.307982 -180.463885 \nL 264.62557 -181.361842 \nL 262.943158 -182.384012 \nL 261.260746 -183.362895 \nL 259.578334 -184.303958 \nL 257.895922 -185.121686 \nL 256.21351 -186.006271 \nL 254.531098 -187.033811 \nL 252.848686 -188.220172 \nL 251.166274 -189.405969 \nL 249.483861 -190.459132 \nL 247.801449 -191.506589 \nL 246.119037 -192.335193 \nL 244.436625 -193.043173 \nL 242.754213 -193.462137 \nL 241.071801 -193.964677 \nL 239.389389 -194.459292 \nL 237.706977 -194.803737 \nL 236.024565 -195.092453 \nL 234.342153 -195.364246 \nL 232.659741 -195.780462 \nL 230.977329 -196.298591 \nL 229.294917 -196.770422 \nL 227.612505 -197.032605 \nL 225.930093 -197.129484 \nL 224.247681 -197.079283 \nL 222.565269 -196.963442 \nL 220.882856 -196.62373 \nL 219.200444 -195.792749 \nL 217.518032 -195.014073 \nL 215.83562 -194.226411 \nL 214.153208 -193.300193 \nL 212.470796 -192.240977 \nL 210.788384 -191.253771 \nL 209.105972 -190.220895 \nL 207.42356 -189.164287 \nL 205.741148 -188.126854 \nL 204.058736 -187.060833 \nL 202.376324 -186.13085 \nL 200.693912 -185.173556 \nL 199.0115 -183.945418 \nL 197.329088 -182.641186 \nL 195.646676 -181.461125 \nL 193.964264 -180.355064 \nL 192.281851 -179.114626 \nL 190.599439 -177.806755 \nL 188.917027 -176.49102 \nL 187.234615 -175.187024 \nL 185.552203 -173.982776 \nL 183.869791 -172.773326 \nL 182.187379 -171.510954 \nL 180.504967 -170.103957 \nL 178.822555 -168.725107 \nL 177.140143 -167.420188 \nL 175.457731 -166.214585 \nL 173.775319 -165.035701 \nL 172.092907 -163.799416 \nL 170.410495 -162.616867 \nL 168.728083 -161.548861 \nL 167.045671 -160.551229 \nL 165.363258 -159.64626 \nL 163.680846 -158.742385 \nL 161.998434 -157.829371 \nL 160.316022 -156.874999 \nL 158.63361 -155.975343 \nL 156.951198 -155.122713 \nL 155.268786 -154.279579 \nL 153.586374 -153.437817 \nL 151.903962 -152.597385 \nL 150.22155 -151.757551 \nL 148.539138 -150.924935 \nL 146.856726 -150.07785 \nL 145.174314 -149.228108 \nL 143.491902 -148.37936 \nL 141.80949 -147.531601 \nL 140.127078 -146.680448 \nL 138.444666 -145.82566 \nL 136.762253 -144.978595 \nL 135.079841 -144.13224 \nL 133.397429 -143.290255 \nL 131.715017 -142.504273 \nL 130.032605 -141.723993 \nL 128.350193 -140.964516 \nL 126.667781 -140.206835 \nL 124.985369 -139.450632 \nL 123.302957 -138.695864 \nL 121.620545 -137.942486 \nL 119.938133 -137.1897 \nL 118.255721 -136.439553 \nL 116.573309 -135.6913 \nL 114.890897 -134.944352 \nL 113.208485 -134.174683 \nL 111.526073 -133.380658 \nL 109.84366 -132.588367 \nL 108.161248 -131.797772 \nL 106.478836 -131.008836 \nL 104.796424 -130.22439 \nL 103.114012 -129.462333 \nL 101.4316 -128.701806 \nL 99.749188 -127.942756 \nL 98.066776 -127.185184 \nL 96.384364 -126.428645 \nL 94.701952 -125.657349 \nL 93.01954 -124.88727 \nL 91.337128 -124.119589 \nL 89.654716 -123.357207 \nL 87.972304 -122.595951 \nL 86.289892 -121.83579 \nL 84.60748 -121.079318 \nL 82.925068 -120.324111 \nL 81.242655 -119.569858 \nL 79.560243 -118.816574 \nL 77.877831 -118.064196 \nL 76.195419 -117.312746 \nL 74.513007 -116.562146 \nL 72.830595 -115.81245 \nL 71.148183 -115.063583 \nL 69.465771 -114.315537 \nL 67.783359 -113.568471 \nL 66.100947 -112.823073 \nL 64.418535 -112.07897 \nL 62.736123 -111.341024 \nL 61.053711 -110.603822 \nL 59.371299 -109.867385 \nL 57.688887 -109.131653 \nL 56.006475 -108.397106 \nL 54.324062 -107.664254 \nz\n\" id=\"m457211c8db\" style=\"stroke:#ff7f0e;stroke-opacity:0.4;\"/>\n    </defs>\n    <g clip-path=\"url(#pcc99a7a769)\">\n     <use style=\"fill:#ff7f0e;fill-opacity:0.4;stroke:#ff7f0e;stroke-opacity:0.4;\" x=\"0\" xlink:href=\"#m457211c8db\" y=\"331.634375\"/>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#pcc99a7a769)\" d=\"M 87.804062 284.4 \nL 87.804062 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m0e79a379e5\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"87.804062\" xlink:href=\"#m0e79a379e5\" y=\"284.4\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- −4 -->\n      <defs>\n       <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-8722\"/>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(77.484531 302.037812)scale(0.14 -0.14)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#pcc99a7a769)\" d=\"M 154.764062 284.4 \nL 154.764062 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"154.764062\" xlink:href=\"#m0e79a379e5\" y=\"284.4\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- −2 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(144.444531 302.037812)scale(0.14 -0.14)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#pcc99a7a769)\" d=\"M 221.724063 284.4 \nL 221.724063 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"221.724063\" xlink:href=\"#m0e79a379e5\" y=\"284.4\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(217.270312 302.037812)scale(0.14 -0.14)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#pcc99a7a769)\" d=\"M 288.684062 284.4 \nL 288.684062 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"288.684062\" xlink:href=\"#m0e79a379e5\" y=\"284.4\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 2 -->\n      <g transform=\"translate(284.230312 302.037812)scale(0.14 -0.14)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#pcc99a7a769)\" d=\"M 355.644063 284.4 \nL 355.644063 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"355.644063\" xlink:href=\"#m0e79a379e5\" y=\"284.4\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 4 -->\n      <g transform=\"translate(351.190313 302.037812)scale(0.14 -0.14)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- $x$ -->\n     <defs>\n      <path d=\"M 60.015625 54.6875 \nL 34.90625 27.875 \nL 50.296875 0 \nL 39.984375 0 \nL 28.421875 21.6875 \nL 8.296875 0 \nL -2.59375 0 \nL 24.3125 28.8125 \nL 10.015625 54.6875 \nL 20.3125 54.6875 \nL 30.8125 34.90625 \nL 49.125 54.6875 \nz\n\" id=\"DejaVuSans-Oblique-120\"/>\n     </defs>\n     <g transform=\"translate(216.924062 321.106875)scale(0.16 -0.16)\">\n      <use transform=\"translate(0 0.3125)\" xlink:href=\"#DejaVuSans-Oblique-120\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#pcc99a7a769)\" d=\"M 54.324063 261.3 \nL 389.124063 261.3 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m5b6df21d61\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"54.324063\" xlink:href=\"#m5b6df21d61\" y=\"261.3\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- −4 -->\n      <g transform=\"translate(26.685 266.618906)scale(0.14 -0.14)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#pcc99a7a769)\" d=\"M 54.324063 215.1 \nL 389.124063 215.1 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"54.324063\" xlink:href=\"#m5b6df21d61\" y=\"215.1\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- −2 -->\n      <g transform=\"translate(26.685 220.418906)scale(0.14 -0.14)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#pcc99a7a769)\" d=\"M 54.324063 168.9 \nL 389.124063 168.9 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"54.324063\" xlink:href=\"#m5b6df21d61\" y=\"168.9\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0 -->\n      <g transform=\"translate(38.416563 174.218906)scale(0.14 -0.14)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#pcc99a7a769)\" d=\"M 54.324063 122.7 \nL 389.124063 122.7 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"54.324063\" xlink:href=\"#m5b6df21d61\" y=\"122.7\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 2 -->\n      <g transform=\"translate(38.416563 128.018906)scale(0.14 -0.14)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#pcc99a7a769)\" d=\"M 54.324063 76.5 \nL 389.124063 76.5 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"54.324063\" xlink:href=\"#m5b6df21d61\" y=\"76.5\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 4 -->\n      <g transform=\"translate(38.416563 81.818906)scale(0.14 -0.14)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#pcc99a7a769)\" d=\"M 54.324063 30.3 \nL 389.124063 30.3 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"54.324063\" xlink:href=\"#m5b6df21d61\" y=\"30.3\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 6 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(38.416563 35.618906)scale(0.14 -0.14)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_13\">\n     <!-- $y$ -->\n     <defs>\n      <path d=\"M 24.8125 -5.078125 \nQ 18.5625 -15.578125 14.625 -18.1875 \nQ 10.6875 -20.796875 4.59375 -20.796875 \nL -2.484375 -20.796875 \nL -0.984375 -13.28125 \nL 4.203125 -13.28125 \nQ 7.953125 -13.28125 10.59375 -11.234375 \nQ 13.234375 -9.1875 16.5 -3.21875 \nL 19.28125 2 \nL 7.171875 54.6875 \nL 16.703125 54.6875 \nL 25.78125 12.796875 \nL 50.875 54.6875 \nL 60.296875 54.6875 \nz\n\" id=\"DejaVuSans-Oblique-121\"/>\n     </defs>\n     <g transform=\"translate(19.325 150.6)rotate(-90)scale(0.16 -0.16)\">\n      <use transform=\"translate(0 0.3125)\" xlink:href=\"#DejaVuSans-Oblique-121\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_23\">\n    <path clip-path=\"url(#pcc99a7a769)\" d=\"M 54.324062 225.168441 \nL 84.60748 212.384443 \nL 121.620545 196.791696 \nL 133.397429 191.917666 \nL 165.363258 176.805253 \nL 168.728083 175.055416 \nL 172.092907 172.9503 \nL 175.457731 170.718693 \nL 178.822555 168.390698 \nL 183.869791 164.561854 \nL 193.964264 157.6998 \nL 197.329088 155.757006 \nL 200.693912 153.643059 \nL 205.741148 151.072983 \nL 214.153208 146.728281 \nL 215.83562 145.975564 \nL 220.882856 144.33813 \nL 222.565269 144.271237 \nL 225.930093 144.486146 \nL 227.612505 144.751763 \nL 229.294917 145.145324 \nL 232.659741 146.447753 \nL 234.342153 147.065552 \nL 236.024565 147.531311 \nL 241.071801 148.637579 \nL 244.436625 149.537056 \nL 247.801449 151.102472 \nL 252.848686 154.484776 \nL 256.21351 156.697889 \nL 262.943158 160.679235 \nL 264.62557 161.766066 \nL 271.355218 165.443759 \nL 299.956223 179.962997 \nL 308.368284 184.022702 \nL 320.145168 189.705065 \nL 325.192404 191.985174 \nL 335.286877 196.993997 \nL 342.016525 200.310061 \nL 350.428585 203.749109 \nL 368.935118 210.747673 \nL 389.124063 218.352188 \nL 389.124063 218.352188 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 54.324063 284.4 \nL 54.324063 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 389.124063 284.4 \nL 389.124063 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 54.324062 284.4 \nL 389.124063 284.4 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 54.324062 7.2 \nL 389.124063 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pcc99a7a769\">\n   <rect height=\"277.2\" width=\"334.8\" x=\"54.324063\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =  119.498\n",
            "Epoch:   50, Train loss =  119.577\n",
            "Epoch:  100, Train loss =  188.062\n",
            "Epoch:  150, Train loss =   93.119\n",
            "Epoch:  200, Train loss =   67.953\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =  152.654\n",
            "Epoch:   50, Train loss =  277.070\n",
            "Epoch:  100, Train loss =  144.046\n",
            "Epoch:  150, Train loss =  217.475\n",
            "Epoch:  200, Train loss =  253.908\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =   88.830\n",
            "Epoch:   50, Train loss =  283.986\n",
            "Epoch:  100, Train loss =   27.472\n",
            "Epoch:  150, Train loss =  100.697\n",
            "Epoch:  200, Train loss =   58.470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =  281.967\n",
            "Epoch:   50, Train loss =  224.534\n",
            "Epoch:  100, Train loss =   78.909\n",
            "Epoch:  150, Train loss =  188.399\n",
            "Epoch:  200, Train loss =  100.196\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =  137.419\n",
            "Epoch:   50, Train loss =  163.088\n",
            "Epoch:  100, Train loss =  140.677\n",
            "Epoch:  150, Train loss =  138.349\n",
            "Epoch:  200, Train loss =  103.798\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =  131.897\n",
            "Epoch:   50, Train loss =   65.643\n",
            "Epoch:  100, Train loss =   24.426\n",
            "Epoch:  150, Train loss =   87.767\n",
            "Epoch:  200, Train loss =  247.774\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =  173.229\n",
            "Epoch:   50, Train loss = 1140.646\n",
            "Epoch:  100, Train loss =  -35.170\n",
            "Epoch:  150, Train loss =    3.685\n",
            "Epoch:  200, Train loss =   21.171\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =  147.834\n",
            "Epoch:   50, Train loss =   47.296\n",
            "Epoch:  100, Train loss =  118.928\n",
            "Epoch:  150, Train loss =   -3.778\n",
            "Epoch:  200, Train loss =    7.373\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =  128.733\n",
            "Epoch:   50, Train loss =  -13.534\n",
            "Epoch:  100, Train loss =   -2.300\n",
            "Epoch:  150, Train loss =   10.541\n",
            "Epoch:  200, Train loss =  224.708\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =  109.097\n",
            "Epoch:   50, Train loss =   27.591\n",
            "Epoch:  100, Train loss =   24.957\n",
            "Epoch:  150, Train loss =  265.813\n",
            "Epoch:  200, Train loss =   74.477\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =   88.412\n",
            "Epoch:   50, Train loss =  256.049\n",
            "Epoch:  100, Train loss =  131.086\n",
            "Epoch:  150, Train loss =   14.957\n",
            "Epoch:  200, Train loss =   81.297\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =  103.715\n",
            "Epoch:   50, Train loss =  106.804\n",
            "Epoch:  100, Train loss =   37.088\n",
            "Epoch:  150, Train loss =  152.145\n",
            "Epoch:  200, Train loss =  146.847\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =   97.937\n",
            "Epoch:   50, Train loss =  148.834\n",
            "Epoch:  100, Train loss =   71.968\n",
            "Epoch:  150, Train loss =   99.570\n",
            "Epoch:  200, Train loss =  153.151\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =  166.457\n",
            "Epoch:   50, Train loss =  105.654\n",
            "Epoch:  100, Train loss =   67.137\n",
            "Epoch:  150, Train loss =   63.787\n",
            "Epoch:  200, Train loss =   33.960\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =  101.939\n",
            "Epoch:   50, Train loss =  345.121\n",
            "Epoch:  100, Train loss =  509.707\n",
            "Epoch:  150, Train loss =  110.702\n",
            "Epoch:  200, Train loss =   35.407\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =   94.653\n",
            "Epoch:   50, Train loss =   60.737\n",
            "Epoch:  100, Train loss =  -51.720\n",
            "Epoch:  150, Train loss =  163.913\n",
            "Epoch:  200, Train loss =   23.949\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =  100.546\n",
            "Epoch:   50, Train loss =   65.183\n",
            "Epoch:  100, Train loss =  236.580\n",
            "Epoch:  150, Train loss =  170.371\n",
            "Epoch:  200, Train loss =   67.713\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =   70.436\n",
            "Epoch:   50, Train loss =   36.061\n",
            "Epoch:  100, Train loss =   50.817\n",
            "Epoch:  150, Train loss =  -14.123\n",
            "Epoch:  200, Train loss =  187.459\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =   98.629\n",
            "Epoch:   50, Train loss =  101.678\n",
            "Epoch:  100, Train loss =   99.767\n",
            "Epoch:  150, Train loss =  126.301\n",
            "Epoch:  200, Train loss =  267.256\n",
            "[0.5591582784238387, 0.6463556174110711, 0.47465668192602917, 0.5595836060736236, 0.7353491688271475, 0.7036338409786606, 0.6744924087158123, 0.47335565238396027, 0.5335592892034159, 0.36153918558833265, 0.4565184393422906, 0.4839785530566475, 0.49608448048866965, 0.6212603097081199, 0.6329991905560236, 0.6607787641364467, 0.44963941233928245, 0.5134331382461689, 0.48077166613866806, 0.49202027405663123] 0.5234962137247924\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aIrQPMwCgeA"
      },
      "source": [
        "## DP Version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5_Xv6nyCmg7"
      },
      "source": [
        "class Langevin_SGD(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr, weight_decay=0, nesterov=False):\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
        "        \n",
        "        super(Langevin_SGD, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(SGD, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('nesterov', False)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            \n",
        "            weight_decay = group['weight_decay']\n",
        "            \n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad.data\n",
        "                \n",
        "                if len(p.shape) == 1 and p.shape[0] == 1:\n",
        "                    p.data.add_(-group['lr'], d_p)\n",
        "                    \n",
        "                else:\n",
        "                    if weight_decay != 0:\n",
        "                        d_p.add_(weight_decay, p.data)\n",
        "\n",
        "                    unit_noise = Variable(p.data.new(p.size()).normal_())\n",
        "\n",
        "                    p.data.add_(-group['lr'], 0.5*d_p + unit_noise/group['lr']**0.5)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzgTr0nSEXni",
        "outputId": "5b406c4c-f6d9-47d1-8cd1-db569fe2b9ed"
      },
      "source": [
        "!pip install opacus\n",
        "from opacus import PrivacyEngine"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opacus in /usr/local/lib/python3.7/dist-packages (0.14.0)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.7/dist-packages (from opacus) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from opacus) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from opacus) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3->opacus) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2SqWhNgDG0Z"
      },
      "source": [
        "class Langevin_Wrapper:\n",
        "    def __init__(self, network, learn_rate, batch_size, no_batches, weight_decay):\n",
        "        \n",
        "        self.learn_rate = learn_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.no_batches = no_batches\n",
        "        \n",
        "        self.network = network\n",
        "        self.network.cuda()\n",
        "        \n",
        "        self.optimizer = Langevin_SGD(self.network.parameters(), lr=self.learn_rate, weight_decay=weight_decay)\n",
        "        privacy_engine = PrivacyEngine(\n",
        "                self.network,\n",
        "                sample_rate=batch_size/250,\n",
        "                alphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64)),\n",
        "                noise_multiplier=noise_multiplier,\n",
        "                max_grad_norm=2/self.learn_rate/noise_multiplier)\n",
        "\n",
        "        # clipping_norm = 100\n",
        "        # self.optimizer = torch.optim.SGD(self.network.parameters(), lr=self.learn_rate, weight_decay=weight_decay)\n",
        "        # privacy_engine = PrivacyEngine(\n",
        "        #         self.network,\n",
        "        #         sample_rate=batch_size/250,\n",
        "        #         alphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64)),\n",
        "        #         noise_multiplier=1/np.sqrt(self.learn_rate/self.batch_size)/clipping_norm,\n",
        "        #         max_grad_norm=clipping_norm)\n",
        "\n",
        "        privacy_engine.attach(self.optimizer)\n",
        "\n",
        "        self.loss_func = log_gaussian_loss\n",
        "    \n",
        "    def fit(self, x, y):\n",
        "        x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "        # reset gradient and total loss\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        output = self.network(x)\n",
        "        loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)\n",
        "        \n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss,output[:, :1]\n",
        "    \n",
        "    \n",
        "    def test_loss(self, x, y):\n",
        "        x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "        output = self.network(x)\n",
        "        loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2Qy5NbYQEkGL",
        "outputId": "785c3321-1523-44f9-ff0c-5a9dc4e10551"
      },
      "source": [
        "np.random.seed(5)\n",
        "average_mse = []\n",
        "for iter in range(20):\n",
        "  no_points = 400\n",
        "  lengthscale = 1\n",
        "  variance = 1.0\n",
        "  sig_noise = 0.3\n",
        "  x = np.random.uniform(-3, 3, no_points)[:, None]\n",
        "  x.sort(axis=0)\n",
        "\n",
        "\n",
        "  k = GPy.kern.RBF(input_dim=1, variance=variance, lengthscale=lengthscale)\n",
        "  C = k.K(x, x) + np.eye(no_points)*(x + 2)**2*sig_noise**2\n",
        "\n",
        "  y = np.random.multivariate_normal(np.zeros((no_points)), C)[:, None]\n",
        "  y = (y - y.mean())\n",
        "  x_train = x[75:325]\n",
        "  y_train = y[75:325]\n",
        "\n",
        "\n",
        "  best_net, best_loss = None, float('inf')\n",
        "  num_nets, nets, losses = 1, [], []\n",
        "  mix_epochs, burnin_epochs = 50, 150\n",
        "  num_epochs = mix_epochs*num_nets + burnin_epochs + 1\n",
        "\n",
        "  batch_size, nb_train = len(x_train), len(x_train)\n",
        "  noise_multiplier=10\n",
        "  net = Langevin_Wrapper(network=Langevin_Model(input_dim=1, output_dim=1, num_units=200),learn_rate=2.5e-4, batch_size=batch_size, no_batches=1, weight_decay=50)\n",
        "  \n",
        "  for i in range(num_epochs):\n",
        "      \n",
        "      loss,y_hat = net.fit(x_train, y_train)\n",
        "      \n",
        "      if i % mix_epochs == 0:\n",
        "          print('Epoch: %4d, Train loss = %8.3f' % (i, loss.cpu().data.numpy()))\n",
        "          \n",
        "      if i % 100 == 0 and i > burnin_epochs: nets.append(copy.deepcopy(net.network))\n",
        "  average_mse.append(np.power((y_hat.cpu().data.numpy()-y_train),2).mean())\n",
        "  if iter==0:\n",
        "      print(\"Using %d networks for prediction\" % len(nets))\n",
        "      samples = []\n",
        "      noises = []\n",
        "      for network in nets:\n",
        "          preds = network.forward(torch.linspace(-5, 5, 200).cuda()).cpu().data.numpy()\n",
        "          samples.append(preds[:, 0])\n",
        "          noises.append(np.exp(preds[:, 1]))\n",
        "          \n",
        "      samples = np.array(samples)\n",
        "\n",
        "      means = (samples.mean(axis = 0)).reshape(-1)\n",
        "      noises = np.array(noises)\n",
        "      aleatoric = (noises**2).mean(axis = 0)**0.5\n",
        "      epistemic = samples.var(axis = 0)**0.5\n",
        "      total_unc = (aleatoric**2 + epistemic**2)**0.5\n",
        "\n",
        "      c = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',\n",
        "          '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
        "\n",
        "      plt.figure(figsize = (6, 5))\n",
        "      plt.style.use('default')\n",
        "      plt.scatter(x_train, y_train, s = 10, marker = 'x', color = 'black', alpha = 0.5)\n",
        "      plt.fill_between(np.linspace(-5, 5, 200), means + aleatoric, means + total_unc, color = c[0], alpha = 0.3, label = 'Epistemic + Aleatoric')\n",
        "      plt.fill_between(np.linspace(-5, 5, 200), means - total_unc, means - aleatoric, color = c[0], alpha = 0.3)\n",
        "      plt.fill_between(np.linspace(-5, 5, 200), means - aleatoric, means + aleatoric, color = c[1], alpha = 0.4, label = 'Aleatoric')\n",
        "      plt.plot(np.linspace(-5, 5, 200), means, color = 'black', linewidth = 1)\n",
        "      plt.xlim([-5, 5])\n",
        "      plt.ylim([-5, 7])\n",
        "      plt.xlabel('$x$', fontsize=16)\n",
        "      plt.ylabel('$y$', fontsize=16)\n",
        "      #plt.tick_params(labelsize=30)\n",
        "      plt.xticks(np.arange(-4, 5, 2),fontsize=14)\n",
        "      plt.yticks(np.arange(-4, 7, 2),fontsize=14)\n",
        "      #plt.gca().set_yticklabels([])\n",
        "      plt.gca().yaxis.grid(alpha=0.3)\n",
        "      plt.gca().xaxis.grid(alpha=0.3)\n",
        "      plt.savefig('dpsgld_hetero.pdf', bbox_inches = 'tight')\n",
        "\n",
        "      #files.download(\"dpsgld_hetero.pdf\")\n",
        "\n",
        "      plt.show()\n",
        "\n",
        "print(average_mse,np.median(average_mse))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =  149.608\n",
            "Epoch:   50, Train loss =  130.751\n",
            "Epoch:  100, Train loss =  112.261\n",
            "Epoch:  150, Train loss =   98.225\n",
            "Epoch:  200, Train loss =  178.389\n",
            "Using 1 networks for prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"331.634375pt\" version=\"1.1\" viewBox=\"0 0 396.324063 331.634375\" width=\"396.324063pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 331.634375 \nL 396.324063 331.634375 \nL 396.324063 0 \nL 0 0 \nz\n\" style=\"fill:#ffffff;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 54.324063 284.4 \nL 389.124063 284.4 \nL 389.124063 7.2 \nL 54.324063 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"PathCollection_1\">\n    <defs>\n     <path d=\"M -1.581139 1.581139 \nL 1.581139 -1.581139 \nM -1.581139 -1.581139 \nL 1.581139 1.581139 \n\" id=\"mb50882704d\" style=\"stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\"/>\n    </defs>\n    <g clip-path=\"url(#pe0488430e2)\">\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"162.766321\" xlink:href=\"#mb50882704d\" y=\"186.474353\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"162.809806\" xlink:href=\"#mb50882704d\" y=\"188.470501\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"162.975168\" xlink:href=\"#mb50882704d\" y=\"186.858613\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"163.698067\" xlink:href=\"#mb50882704d\" y=\"186.611183\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"164.080535\" xlink:href=\"#mb50882704d\" y=\"185.95654\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"165.175897\" xlink:href=\"#mb50882704d\" y=\"185.301945\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"165.212848\" xlink:href=\"#mb50882704d\" y=\"191.241443\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"165.532307\" xlink:href=\"#mb50882704d\" y=\"184.277114\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"165.775519\" xlink:href=\"#mb50882704d\" y=\"186.858444\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"165.878051\" xlink:href=\"#mb50882704d\" y=\"184.920269\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"166.218523\" xlink:href=\"#mb50882704d\" y=\"186.596162\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"167.693946\" xlink:href=\"#mb50882704d\" y=\"185.609247\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"168.624174\" xlink:href=\"#mb50882704d\" y=\"176.287675\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"169.423034\" xlink:href=\"#mb50882704d\" y=\"182.659116\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"169.868709\" xlink:href=\"#mb50882704d\" y=\"183.779569\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"169.918652\" xlink:href=\"#mb50882704d\" y=\"188.05034\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"170.357212\" xlink:href=\"#mb50882704d\" y=\"185.706423\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"170.626417\" xlink:href=\"#mb50882704d\" y=\"189.278576\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"171.754181\" xlink:href=\"#mb50882704d\" y=\"176.557464\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"172.224861\" xlink:href=\"#mb50882704d\" y=\"175.58798\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"172.988034\" xlink:href=\"#mb50882704d\" y=\"181.535598\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"173.148543\" xlink:href=\"#mb50882704d\" y=\"179.245545\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"173.331588\" xlink:href=\"#mb50882704d\" y=\"178.747641\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"174.364267\" xlink:href=\"#mb50882704d\" y=\"180.350961\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"174.539895\" xlink:href=\"#mb50882704d\" y=\"177.063499\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"174.563514\" xlink:href=\"#mb50882704d\" y=\"181.780912\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"174.681807\" xlink:href=\"#mb50882704d\" y=\"180.700013\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"176.342551\" xlink:href=\"#mb50882704d\" y=\"181.88653\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"176.492798\" xlink:href=\"#mb50882704d\" y=\"176.361755\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"177.160026\" xlink:href=\"#mb50882704d\" y=\"173.067936\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"177.325097\" xlink:href=\"#mb50882704d\" y=\"176.347715\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"178.471762\" xlink:href=\"#mb50882704d\" y=\"173.874377\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"178.571235\" xlink:href=\"#mb50882704d\" y=\"175.38302\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"180.037537\" xlink:href=\"#mb50882704d\" y=\"171.90254\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"180.513635\" xlink:href=\"#mb50882704d\" y=\"178.325526\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"180.582127\" xlink:href=\"#mb50882704d\" y=\"184.856101\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"180.744026\" xlink:href=\"#mb50882704d\" y=\"175.701818\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"180.760599\" xlink:href=\"#mb50882704d\" y=\"170.548931\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"180.905347\" xlink:href=\"#mb50882704d\" y=\"174.395862\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"181.34374\" xlink:href=\"#mb50882704d\" y=\"176.435081\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"181.647673\" xlink:href=\"#mb50882704d\" y=\"178.603472\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"181.926942\" xlink:href=\"#mb50882704d\" y=\"166.469536\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"182.053901\" xlink:href=\"#mb50882704d\" y=\"167.706484\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"182.662651\" xlink:href=\"#mb50882704d\" y=\"170.551302\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"183.40234\" xlink:href=\"#mb50882704d\" y=\"171.326404\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"185.103679\" xlink:href=\"#mb50882704d\" y=\"164.55677\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"185.293759\" xlink:href=\"#mb50882704d\" y=\"178.060779\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"185.775328\" xlink:href=\"#mb50882704d\" y=\"172.695726\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"187.085108\" xlink:href=\"#mb50882704d\" y=\"161.344532\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"187.625336\" xlink:href=\"#mb50882704d\" y=\"161.4077\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"187.715963\" xlink:href=\"#mb50882704d\" y=\"164.166146\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"187.985895\" xlink:href=\"#mb50882704d\" y=\"182.026139\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"189.24953\" xlink:href=\"#mb50882704d\" y=\"158.119838\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"189.716591\" xlink:href=\"#mb50882704d\" y=\"171.566358\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"190.147341\" xlink:href=\"#mb50882704d\" y=\"156.598023\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"190.410675\" xlink:href=\"#mb50882704d\" y=\"150.26627\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"190.794198\" xlink:href=\"#mb50882704d\" y=\"151.025965\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"191.135898\" xlink:href=\"#mb50882704d\" y=\"169.490221\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"191.256994\" xlink:href=\"#mb50882704d\" y=\"162.10405\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"191.356545\" xlink:href=\"#mb50882704d\" y=\"154.14338\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"191.628136\" xlink:href=\"#mb50882704d\" y=\"154.871793\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"192.012893\" xlink:href=\"#mb50882704d\" y=\"158.159838\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"192.553366\" xlink:href=\"#mb50882704d\" y=\"147.587196\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"192.761501\" xlink:href=\"#mb50882704d\" y=\"151.101855\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"193.597487\" xlink:href=\"#mb50882704d\" y=\"149.513181\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"193.811469\" xlink:href=\"#mb50882704d\" y=\"158.033078\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"194.110635\" xlink:href=\"#mb50882704d\" y=\"168.955005\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"194.32399\" xlink:href=\"#mb50882704d\" y=\"142.475188\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"194.547043\" xlink:href=\"#mb50882704d\" y=\"149.176675\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"194.662806\" xlink:href=\"#mb50882704d\" y=\"147.074923\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"194.672159\" xlink:href=\"#mb50882704d\" y=\"163.16191\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"194.701237\" xlink:href=\"#mb50882704d\" y=\"148.430837\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"195.256596\" xlink:href=\"#mb50882704d\" y=\"152.776275\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"195.782975\" xlink:href=\"#mb50882704d\" y=\"147.055608\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"195.891888\" xlink:href=\"#mb50882704d\" y=\"161.123052\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"195.913517\" xlink:href=\"#mb50882704d\" y=\"132.806104\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"196.098294\" xlink:href=\"#mb50882704d\" y=\"154.674221\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"196.355835\" xlink:href=\"#mb50882704d\" y=\"153.375495\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"196.821374\" xlink:href=\"#mb50882704d\" y=\"144.616826\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"197.432175\" xlink:href=\"#mb50882704d\" y=\"149.758614\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"197.573945\" xlink:href=\"#mb50882704d\" y=\"140.986321\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"197.75447\" xlink:href=\"#mb50882704d\" y=\"151.465375\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"198.303276\" xlink:href=\"#mb50882704d\" y=\"141.328631\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"199.28439\" xlink:href=\"#mb50882704d\" y=\"156.585889\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"201.424095\" xlink:href=\"#mb50882704d\" y=\"160.568159\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"203.058083\" xlink:href=\"#mb50882704d\" y=\"138.315821\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"203.179972\" xlink:href=\"#mb50882704d\" y=\"148.061981\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"203.922803\" xlink:href=\"#mb50882704d\" y=\"143.968545\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"204.495593\" xlink:href=\"#mb50882704d\" y=\"138.996\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"205.383298\" xlink:href=\"#mb50882704d\" y=\"135.078235\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"205.966156\" xlink:href=\"#mb50882704d\" y=\"132.273375\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"206.426971\" xlink:href=\"#mb50882704d\" y=\"153.4158\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"206.78609\" xlink:href=\"#mb50882704d\" y=\"149.073959\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"206.787432\" xlink:href=\"#mb50882704d\" y=\"158.766424\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"208.251159\" xlink:href=\"#mb50882704d\" y=\"118.031632\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"208.343022\" xlink:href=\"#mb50882704d\" y=\"147.94209\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"208.975324\" xlink:href=\"#mb50882704d\" y=\"139.840018\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"209.30343\" xlink:href=\"#mb50882704d\" y=\"144.606783\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"209.92443\" xlink:href=\"#mb50882704d\" y=\"117.935648\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"209.934259\" xlink:href=\"#mb50882704d\" y=\"143.554163\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"210.375291\" xlink:href=\"#mb50882704d\" y=\"141.315852\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"210.834852\" xlink:href=\"#mb50882704d\" y=\"128.623237\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"210.998643\" xlink:href=\"#mb50882704d\" y=\"125.707832\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"211.139726\" xlink:href=\"#mb50882704d\" y=\"148.301856\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"211.510147\" xlink:href=\"#mb50882704d\" y=\"158.767415\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"211.628548\" xlink:href=\"#mb50882704d\" y=\"134.024652\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"212.16596\" xlink:href=\"#mb50882704d\" y=\"137.585383\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"213.557406\" xlink:href=\"#mb50882704d\" y=\"115.779355\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"214.022633\" xlink:href=\"#mb50882704d\" y=\"150.045378\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"215.411108\" xlink:href=\"#mb50882704d\" y=\"147.614076\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"215.594887\" xlink:href=\"#mb50882704d\" y=\"145.781856\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"215.630666\" xlink:href=\"#mb50882704d\" y=\"138.641893\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"216.255883\" xlink:href=\"#mb50882704d\" y=\"142.889677\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"216.265782\" xlink:href=\"#mb50882704d\" y=\"123.745154\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"217.292913\" xlink:href=\"#mb50882704d\" y=\"126.997613\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"217.37281\" xlink:href=\"#mb50882704d\" y=\"136.454669\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"218.63465\" xlink:href=\"#mb50882704d\" y=\"131.663618\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"218.697017\" xlink:href=\"#mb50882704d\" y=\"149.089504\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"218.790546\" xlink:href=\"#mb50882704d\" y=\"140.01072\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"219.396102\" xlink:href=\"#mb50882704d\" y=\"142.336829\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"220.401278\" xlink:href=\"#mb50882704d\" y=\"148.552719\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"221.060256\" xlink:href=\"#mb50882704d\" y=\"134.002508\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"221.123502\" xlink:href=\"#mb50882704d\" y=\"129.229907\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"221.275702\" xlink:href=\"#mb50882704d\" y=\"131.224335\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"222.592349\" xlink:href=\"#mb50882704d\" y=\"148.400478\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"223.627419\" xlink:href=\"#mb50882704d\" y=\"160.991253\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"223.804134\" xlink:href=\"#mb50882704d\" y=\"116.997649\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"224.650085\" xlink:href=\"#mb50882704d\" y=\"119.911894\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"224.684625\" xlink:href=\"#mb50882704d\" y=\"133.624021\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"224.832201\" xlink:href=\"#mb50882704d\" y=\"112.523887\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"225.423868\" xlink:href=\"#mb50882704d\" y=\"122.985382\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"225.694755\" xlink:href=\"#mb50882704d\" y=\"136.217899\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"226.836354\" xlink:href=\"#mb50882704d\" y=\"147.00049\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"227.091087\" xlink:href=\"#mb50882704d\" y=\"135.733469\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"227.183746\" xlink:href=\"#mb50882704d\" y=\"161.661185\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"230.179184\" xlink:href=\"#mb50882704d\" y=\"162.648409\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"230.609458\" xlink:href=\"#mb50882704d\" y=\"149.579116\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"230.624037\" xlink:href=\"#mb50882704d\" y=\"154.974258\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"231.036528\" xlink:href=\"#mb50882704d\" y=\"128.600624\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"231.056244\" xlink:href=\"#mb50882704d\" y=\"128.781399\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"231.512181\" xlink:href=\"#mb50882704d\" y=\"143.208382\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"231.660282\" xlink:href=\"#mb50882704d\" y=\"124.721284\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"233.960585\" xlink:href=\"#mb50882704d\" y=\"160.216876\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"234.998614\" xlink:href=\"#mb50882704d\" y=\"131.902698\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"235.981564\" xlink:href=\"#mb50882704d\" y=\"164.778215\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"236.02242\" xlink:href=\"#mb50882704d\" y=\"152.436283\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"236.311697\" xlink:href=\"#mb50882704d\" y=\"147.696253\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"236.865325\" xlink:href=\"#mb50882704d\" y=\"172.335145\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"237.324977\" xlink:href=\"#mb50882704d\" y=\"150.771571\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"237.761882\" xlink:href=\"#mb50882704d\" y=\"115.85224\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"237.978976\" xlink:href=\"#mb50882704d\" y=\"164.023157\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"238.379187\" xlink:href=\"#mb50882704d\" y=\"174.371627\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"238.419727\" xlink:href=\"#mb50882704d\" y=\"125.972843\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"238.497491\" xlink:href=\"#mb50882704d\" y=\"148.459493\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"239.004383\" xlink:href=\"#mb50882704d\" y=\"143.359242\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"239.216209\" xlink:href=\"#mb50882704d\" y=\"125.229124\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"239.441601\" xlink:href=\"#mb50882704d\" y=\"128.368035\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"239.455453\" xlink:href=\"#mb50882704d\" y=\"151.47648\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"239.501956\" xlink:href=\"#mb50882704d\" y=\"142.027902\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"239.677487\" xlink:href=\"#mb50882704d\" y=\"131.631302\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"240.0488\" xlink:href=\"#mb50882704d\" y=\"156.685115\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"240.864939\" xlink:href=\"#mb50882704d\" y=\"145.712988\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"241.79784\" xlink:href=\"#mb50882704d\" y=\"136.370762\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"241.81069\" xlink:href=\"#mb50882704d\" y=\"141.761688\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"242.081474\" xlink:href=\"#mb50882704d\" y=\"133.010141\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"242.612397\" xlink:href=\"#mb50882704d\" y=\"138.622972\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"242.736776\" xlink:href=\"#mb50882704d\" y=\"191.995405\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"242.817329\" xlink:href=\"#mb50882704d\" y=\"156.842272\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"244.17117\" xlink:href=\"#mb50882704d\" y=\"173.49815\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"244.988686\" xlink:href=\"#mb50882704d\" y=\"140.301658\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"246.671051\" xlink:href=\"#mb50882704d\" y=\"167.129566\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"246.78474\" xlink:href=\"#mb50882704d\" y=\"134.853487\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"247.594978\" xlink:href=\"#mb50882704d\" y=\"143.43477\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"248.193227\" xlink:href=\"#mb50882704d\" y=\"138.599113\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"248.422128\" xlink:href=\"#mb50882704d\" y=\"144.676935\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"248.585028\" xlink:href=\"#mb50882704d\" y=\"166.641867\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"248.685026\" xlink:href=\"#mb50882704d\" y=\"118.947289\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"248.914417\" xlink:href=\"#mb50882704d\" y=\"134.829281\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"249.472596\" xlink:href=\"#mb50882704d\" y=\"144.688809\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"249.621811\" xlink:href=\"#mb50882704d\" y=\"173.036393\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"249.806117\" xlink:href=\"#mb50882704d\" y=\"139.452536\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"250.141506\" xlink:href=\"#mb50882704d\" y=\"133.085439\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"250.319238\" xlink:href=\"#mb50882704d\" y=\"153.241773\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"250.57851\" xlink:href=\"#mb50882704d\" y=\"153.305586\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"252.312218\" xlink:href=\"#mb50882704d\" y=\"177.174903\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"252.424529\" xlink:href=\"#mb50882704d\" y=\"140.987872\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"252.491662\" xlink:href=\"#mb50882704d\" y=\"156.146052\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"253.091413\" xlink:href=\"#mb50882704d\" y=\"174.233734\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"253.103784\" xlink:href=\"#mb50882704d\" y=\"139.54645\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"253.33178\" xlink:href=\"#mb50882704d\" y=\"164.983422\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"253.747806\" xlink:href=\"#mb50882704d\" y=\"159.397444\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"254.034484\" xlink:href=\"#mb50882704d\" y=\"124.353619\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"255.655394\" xlink:href=\"#mb50882704d\" y=\"139.618176\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"257.718279\" xlink:href=\"#mb50882704d\" y=\"134.403398\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"261.868648\" xlink:href=\"#mb50882704d\" y=\"184.574757\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"262.211996\" xlink:href=\"#mb50882704d\" y=\"165.251611\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"262.491067\" xlink:href=\"#mb50882704d\" y=\"167.759021\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"263.411931\" xlink:href=\"#mb50882704d\" y=\"190.851678\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"263.918715\" xlink:href=\"#mb50882704d\" y=\"187.65762\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"264.704158\" xlink:href=\"#mb50882704d\" y=\"168.872111\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"264.77618\" xlink:href=\"#mb50882704d\" y=\"205.084641\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"265.797111\" xlink:href=\"#mb50882704d\" y=\"171.50628\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"266.216718\" xlink:href=\"#mb50882704d\" y=\"140.298054\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"267.281511\" xlink:href=\"#mb50882704d\" y=\"182.170448\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"268.045236\" xlink:href=\"#mb50882704d\" y=\"182.089716\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"268.164304\" xlink:href=\"#mb50882704d\" y=\"203.160578\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"269.621949\" xlink:href=\"#mb50882704d\" y=\"215.437253\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"269.78475\" xlink:href=\"#mb50882704d\" y=\"194.866528\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"270.487831\" xlink:href=\"#mb50882704d\" y=\"224.695125\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"271.657971\" xlink:href=\"#mb50882704d\" y=\"175.169792\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"271.726132\" xlink:href=\"#mb50882704d\" y=\"199.25046\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"271.780277\" xlink:href=\"#mb50882704d\" y=\"156.017906\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"273.121989\" xlink:href=\"#mb50882704d\" y=\"188.543059\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"273.300002\" xlink:href=\"#mb50882704d\" y=\"147.253516\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"273.316386\" xlink:href=\"#mb50882704d\" y=\"183.761987\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"273.594411\" xlink:href=\"#mb50882704d\" y=\"203.410936\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"273.771893\" xlink:href=\"#mb50882704d\" y=\"146.295246\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"274.059638\" xlink:href=\"#mb50882704d\" y=\"186.540167\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"274.763346\" xlink:href=\"#mb50882704d\" y=\"169.32996\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"274.877443\" xlink:href=\"#mb50882704d\" y=\"195.422909\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"275.139633\" xlink:href=\"#mb50882704d\" y=\"207.18272\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"275.353625\" xlink:href=\"#mb50882704d\" y=\"140.185789\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"275.42559\" xlink:href=\"#mb50882704d\" y=\"158.29396\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"275.948139\" xlink:href=\"#mb50882704d\" y=\"155.729807\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"276.013892\" xlink:href=\"#mb50882704d\" y=\"235.287044\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"276.027825\" xlink:href=\"#mb50882704d\" y=\"187.728176\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"276.133232\" xlink:href=\"#mb50882704d\" y=\"191.493948\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"276.9687\" xlink:href=\"#mb50882704d\" y=\"224.932117\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"277.394799\" xlink:href=\"#mb50882704d\" y=\"197.06885\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"277.444558\" xlink:href=\"#mb50882704d\" y=\"171.62949\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"277.872952\" xlink:href=\"#mb50882704d\" y=\"174.046112\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"279.20133\" xlink:href=\"#mb50882704d\" y=\"151.111998\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"279.780414\" xlink:href=\"#mb50882704d\" y=\"218.615916\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"281.001234\" xlink:href=\"#mb50882704d\" y=\"171.822343\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"281.213212\" xlink:href=\"#mb50882704d\" y=\"198.6055\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"281.744249\" xlink:href=\"#mb50882704d\" y=\"192.692168\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"282.083766\" xlink:href=\"#mb50882704d\" y=\"204.231765\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"282.269602\" xlink:href=\"#mb50882704d\" y=\"157.511959\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"282.489637\" xlink:href=\"#mb50882704d\" y=\"194.389695\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"282.833998\" xlink:href=\"#mb50882704d\" y=\"193.115556\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"283.131996\" xlink:href=\"#mb50882704d\" y=\"182.946887\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"283.31983\" xlink:href=\"#mb50882704d\" y=\"176.545275\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"283.550394\" xlink:href=\"#mb50882704d\" y=\"153.920245\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"283.746578\" xlink:href=\"#mb50882704d\" y=\"170.300761\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"284.05422\" xlink:href=\"#mb50882704d\" y=\"181.102067\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"284.059985\" xlink:href=\"#mb50882704d\" y=\"224.453322\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"284.241129\" xlink:href=\"#mb50882704d\" y=\"188.170668\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"284.364524\" xlink:href=\"#mb50882704d\" y=\"210.038279\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"284.379015\" xlink:href=\"#mb50882704d\" y=\"146.337313\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"284.387412\" xlink:href=\"#mb50882704d\" y=\"162.225272\"/>\n    </g>\n   </g>\n   <g id=\"PolyCollection_1\">\n    <defs>\n     <path d=\"M 54.324062 -105.871215 \nL 54.324062 -105.871215 \nL 56.006475 -106.591835 \nL 57.688887 -107.313429 \nL 59.371299 -108.035513 \nL 61.053711 -108.766371 \nL 62.736123 -109.500335 \nL 64.418535 -110.234844 \nL 66.100947 -110.969893 \nL 67.783359 -111.701339 \nL 69.465771 -112.433469 \nL 71.148183 -113.234938 \nL 72.830595 -114.036953 \nL 74.513007 -114.839495 \nL 76.195419 -115.642661 \nL 77.877831 -116.446404 \nL 79.560243 -117.250745 \nL 81.242655 -118.054067 \nL 82.925068 -118.856831 \nL 84.60748 -119.66026 \nL 86.289892 -120.464436 \nL 87.972304 -121.273577 \nL 89.654716 -122.085929 \nL 91.337128 -122.899013 \nL 93.01954 -123.710844 \nL 94.701952 -124.525168 \nL 96.384364 -125.339039 \nL 98.066776 -126.152195 \nL 99.749188 -126.966218 \nL 101.4316 -127.781159 \nL 103.114012 -128.596234 \nL 104.796424 -129.396706 \nL 106.478836 -130.198238 \nL 108.161248 -131.001767 \nL 109.84366 -131.806405 \nL 111.526073 -132.615678 \nL 113.208485 -133.427843 \nL 114.890897 -134.241213 \nL 116.573309 -135.059006 \nL 118.255721 -135.880693 \nL 119.938133 -136.703763 \nL 121.620545 -137.511785 \nL 123.302957 -138.318624 \nL 124.985369 -139.125895 \nL 126.667781 -139.946212 \nL 128.350193 -140.782985 \nL 130.032605 -141.616092 \nL 131.715017 -142.412309 \nL 133.397429 -143.205502 \nL 135.079841 -144.022291 \nL 136.762253 -144.863088 \nL 138.444666 -145.707855 \nL 140.127078 -146.589606 \nL 141.80949 -147.485575 \nL 143.491902 -148.379487 \nL 145.174314 -149.193618 \nL 146.856726 -150.045038 \nL 148.539138 -150.913464 \nL 150.22155 -151.781273 \nL 151.903962 -152.6351 \nL 153.586374 -153.524079 \nL 155.268786 -154.428488 \nL 156.951198 -155.366619 \nL 158.63361 -156.311473 \nL 160.316022 -157.214941 \nL 161.998434 -158.096653 \nL 163.680846 -158.98285 \nL 165.363258 -159.943093 \nL 167.045671 -160.933788 \nL 168.728083 -161.953456 \nL 170.410495 -163.078915 \nL 172.092907 -164.270745 \nL 173.775319 -165.449232 \nL 175.457731 -166.635219 \nL 177.140143 -167.789102 \nL 178.822555 -168.922883 \nL 180.504967 -170.050534 \nL 182.187379 -171.170495 \nL 183.869791 -172.27814 \nL 185.552203 -173.430623 \nL 187.234615 -174.673084 \nL 188.917027 -175.870806 \nL 190.599439 -177.002032 \nL 192.281851 -178.25352 \nL 193.964264 -179.536143 \nL 195.646676 -180.881778 \nL 197.329088 -182.241203 \nL 199.0115 -183.83808 \nL 200.693912 -185.319362 \nL 202.376324 -186.597242 \nL 204.058736 -187.435088 \nL 205.741148 -188.226831 \nL 207.42356 -188.792216 \nL 209.105972 -189.277248 \nL 210.788384 -189.965519 \nL 212.470796 -190.835642 \nL 214.153208 -191.810701 \nL 215.83562 -192.437638 \nL 217.518032 -193.013284 \nL 219.200444 -193.6251 \nL 220.882856 -194.015742 \nL 222.565269 -194.090864 \nL 224.247681 -193.925852 \nL 225.930093 -193.618965 \nL 227.612505 -193.437747 \nL 229.294917 -193.334193 \nL 230.977329 -193.027267 \nL 232.659741 -192.523086 \nL 234.342153 -192.318913 \nL 236.024565 -192.249786 \nL 237.706977 -192.145494 \nL 239.389389 -192.005825 \nL 241.071801 -191.581568 \nL 242.754213 -191.118527 \nL 244.436625 -190.689297 \nL 246.119037 -190.2448 \nL 247.801449 -189.803671 \nL 249.483861 -189.440137 \nL 251.166274 -189.106723 \nL 252.848686 -188.77956 \nL 254.531098 -188.446705 \nL 256.21351 -188.048851 \nL 257.895922 -187.672625 \nL 259.578334 -187.404934 \nL 261.260746 -187.165605 \nL 262.943158 -186.947688 \nL 264.62557 -186.784661 \nL 266.307982 -186.533667 \nL 267.990394 -186.219256 \nL 269.672806 -185.782299 \nL 271.355218 -185.30118 \nL 273.03763 -184.82476 \nL 274.720042 -184.391959 \nL 276.402454 -183.945244 \nL 278.084867 -183.503394 \nL 279.767279 -183.029 \nL 281.449691 -182.534809 \nL 283.132103 -182.217185 \nL 284.814515 -181.824641 \nL 286.496927 -181.422632 \nL 288.179339 -181.030251 \nL 289.861751 -180.712325 \nL 291.544163 -180.399956 \nL 293.226575 -180.120424 \nL 294.908987 -179.830121 \nL 296.591399 -179.54216 \nL 298.273811 -179.333331 \nL 299.956223 -179.14834 \nL 301.638635 -179.012446 \nL 303.321047 -178.907177 \nL 305.003459 -178.803021 \nL 306.685872 -178.699993 \nL 308.368284 -178.595012 \nL 310.050696 -178.512188 \nL 311.733108 -178.430968 \nL 313.41552 -178.350232 \nL 315.097932 -178.265186 \nL 316.780344 -178.196847 \nL 318.462756 -178.145737 \nL 320.145168 -178.151424 \nL 321.82758 -178.159206 \nL 323.509992 -178.169115 \nL 325.192404 -178.181177 \nL 326.874816 -178.195406 \nL 328.557228 -178.226166 \nL 330.23964 -178.25972 \nL 331.922052 -178.295498 \nL 333.604465 -178.333497 \nL 335.286877 -178.373747 \nL 336.969289 -178.416271 \nL 338.651701 -178.461112 \nL 340.334113 -178.507812 \nL 342.016525 -178.554832 \nL 343.698937 -178.603727 \nL 345.381349 -178.636724 \nL 347.063761 -178.65687 \nL 348.746173 -178.678117 \nL 350.428585 -178.68685 \nL 352.110997 -178.677212 \nL 353.793409 -178.668385 \nL 355.475821 -178.660376 \nL 357.158233 -178.653227 \nL 358.840645 -178.652379 \nL 360.523057 -178.664699 \nL 362.20547 -178.677912 \nL 363.887882 -178.691942 \nL 365.570294 -178.711375 \nL 367.252706 -178.761658 \nL 368.935118 -178.813082 \nL 370.61753 -178.865678 \nL 372.299942 -178.919445 \nL 373.982354 -178.976075 \nL 375.664766 -179.018626 \nL 377.347178 -179.04903 \nL 379.02959 -179.073213 \nL 380.712002 -179.090297 \nL 382.394414 -179.108307 \nL 384.076826 -179.127197 \nL 385.759238 -179.147019 \nL 387.44165 -179.167768 \nL 389.124063 -179.18944 \nL 389.124063 -179.18944 \nL 389.124063 -179.18944 \nL 387.44165 -179.167768 \nL 385.759238 -179.147019 \nL 384.076826 -179.127197 \nL 382.394414 -179.108307 \nL 380.712002 -179.090297 \nL 379.02959 -179.073213 \nL 377.347178 -179.04903 \nL 375.664766 -179.018626 \nL 373.982354 -178.976075 \nL 372.299942 -178.919445 \nL 370.61753 -178.865678 \nL 368.935118 -178.813082 \nL 367.252706 -178.761658 \nL 365.570294 -178.711375 \nL 363.887882 -178.691942 \nL 362.20547 -178.677912 \nL 360.523057 -178.664699 \nL 358.840645 -178.652379 \nL 357.158233 -178.653227 \nL 355.475821 -178.660376 \nL 353.793409 -178.668385 \nL 352.110997 -178.677212 \nL 350.428585 -178.68685 \nL 348.746173 -178.678117 \nL 347.063761 -178.65687 \nL 345.381349 -178.636724 \nL 343.698937 -178.603727 \nL 342.016525 -178.554832 \nL 340.334113 -178.507812 \nL 338.651701 -178.461112 \nL 336.969289 -178.416271 \nL 335.286877 -178.373747 \nL 333.604465 -178.333497 \nL 331.922052 -178.295498 \nL 330.23964 -178.25972 \nL 328.557228 -178.226166 \nL 326.874816 -178.195406 \nL 325.192404 -178.181177 \nL 323.509992 -178.169115 \nL 321.82758 -178.159206 \nL 320.145168 -178.151424 \nL 318.462756 -178.145737 \nL 316.780344 -178.196847 \nL 315.097932 -178.265186 \nL 313.41552 -178.350232 \nL 311.733108 -178.430968 \nL 310.050696 -178.512188 \nL 308.368284 -178.595012 \nL 306.685872 -178.699993 \nL 305.003459 -178.803021 \nL 303.321047 -178.907177 \nL 301.638635 -179.012446 \nL 299.956223 -179.14834 \nL 298.273811 -179.333331 \nL 296.591399 -179.54216 \nL 294.908987 -179.830121 \nL 293.226575 -180.120424 \nL 291.544163 -180.399956 \nL 289.861751 -180.712325 \nL 288.179339 -181.030251 \nL 286.496927 -181.422632 \nL 284.814515 -181.824641 \nL 283.132103 -182.217185 \nL 281.449691 -182.534809 \nL 279.767279 -183.029 \nL 278.084867 -183.503394 \nL 276.402454 -183.945244 \nL 274.720042 -184.391959 \nL 273.03763 -184.82476 \nL 271.355218 -185.30118 \nL 269.672806 -185.782299 \nL 267.990394 -186.219256 \nL 266.307982 -186.533667 \nL 264.62557 -186.784661 \nL 262.943158 -186.947688 \nL 261.260746 -187.165605 \nL 259.578334 -187.404934 \nL 257.895922 -187.672625 \nL 256.21351 -188.048851 \nL 254.531098 -188.446705 \nL 252.848686 -188.77956 \nL 251.166274 -189.106723 \nL 249.483861 -189.440137 \nL 247.801449 -189.803671 \nL 246.119037 -190.2448 \nL 244.436625 -190.689297 \nL 242.754213 -191.118527 \nL 241.071801 -191.581568 \nL 239.389389 -192.005825 \nL 237.706977 -192.145494 \nL 236.024565 -192.249786 \nL 234.342153 -192.318913 \nL 232.659741 -192.523086 \nL 230.977329 -193.027267 \nL 229.294917 -193.334193 \nL 227.612505 -193.437747 \nL 225.930093 -193.618965 \nL 224.247681 -193.925852 \nL 222.565269 -194.090864 \nL 220.882856 -194.015742 \nL 219.200444 -193.6251 \nL 217.518032 -193.013284 \nL 215.83562 -192.437638 \nL 214.153208 -191.810701 \nL 212.470796 -190.835642 \nL 210.788384 -189.965519 \nL 209.105972 -189.277248 \nL 207.42356 -188.792216 \nL 205.741148 -188.226831 \nL 204.058736 -187.435088 \nL 202.376324 -186.597242 \nL 200.693912 -185.319362 \nL 199.0115 -183.83808 \nL 197.329088 -182.241203 \nL 195.646676 -180.881778 \nL 193.964264 -179.536143 \nL 192.281851 -178.25352 \nL 190.599439 -177.002032 \nL 188.917027 -175.870806 \nL 187.234615 -174.673084 \nL 185.552203 -173.430623 \nL 183.869791 -172.27814 \nL 182.187379 -171.170495 \nL 180.504967 -170.050534 \nL 178.822555 -168.922883 \nL 177.140143 -167.789102 \nL 175.457731 -166.635219 \nL 173.775319 -165.449232 \nL 172.092907 -164.270745 \nL 170.410495 -163.078915 \nL 168.728083 -161.953456 \nL 167.045671 -160.933788 \nL 165.363258 -159.943093 \nL 163.680846 -158.98285 \nL 161.998434 -158.096653 \nL 160.316022 -157.214941 \nL 158.63361 -156.311473 \nL 156.951198 -155.366619 \nL 155.268786 -154.428488 \nL 153.586374 -153.524079 \nL 151.903962 -152.6351 \nL 150.22155 -151.781273 \nL 148.539138 -150.913464 \nL 146.856726 -150.045038 \nL 145.174314 -149.193618 \nL 143.491902 -148.379487 \nL 141.80949 -147.485575 \nL 140.127078 -146.589606 \nL 138.444666 -145.707855 \nL 136.762253 -144.863088 \nL 135.079841 -144.022291 \nL 133.397429 -143.205502 \nL 131.715017 -142.412309 \nL 130.032605 -141.616092 \nL 128.350193 -140.782985 \nL 126.667781 -139.946212 \nL 124.985369 -139.125895 \nL 123.302957 -138.318624 \nL 121.620545 -137.511785 \nL 119.938133 -136.703763 \nL 118.255721 -135.880693 \nL 116.573309 -135.059006 \nL 114.890897 -134.241213 \nL 113.208485 -133.427843 \nL 111.526073 -132.615678 \nL 109.84366 -131.806405 \nL 108.161248 -131.001767 \nL 106.478836 -130.198238 \nL 104.796424 -129.396706 \nL 103.114012 -128.596234 \nL 101.4316 -127.781159 \nL 99.749188 -126.966218 \nL 98.066776 -126.152195 \nL 96.384364 -125.339039 \nL 94.701952 -124.525168 \nL 93.01954 -123.710844 \nL 91.337128 -122.899013 \nL 89.654716 -122.085929 \nL 87.972304 -121.273577 \nL 86.289892 -120.464436 \nL 84.60748 -119.66026 \nL 82.925068 -118.856831 \nL 81.242655 -118.054067 \nL 79.560243 -117.250745 \nL 77.877831 -116.446404 \nL 76.195419 -115.642661 \nL 74.513007 -114.839495 \nL 72.830595 -114.036953 \nL 71.148183 -113.234938 \nL 69.465771 -112.433469 \nL 67.783359 -111.701339 \nL 66.100947 -110.969893 \nL 64.418535 -110.234844 \nL 62.736123 -109.500335 \nL 61.053711 -108.766371 \nL 59.371299 -108.035513 \nL 57.688887 -107.313429 \nL 56.006475 -106.591835 \nL 54.324062 -105.871215 \nz\n\" id=\"mfb867e74d0\" style=\"stroke:#1f77b4;stroke-opacity:0.3;\"/>\n    </defs>\n    <g clip-path=\"url(#pe0488430e2)\">\n     <use style=\"fill:#1f77b4;fill-opacity:0.3;stroke:#1f77b4;stroke-opacity:0.3;\" x=\"0\" xlink:href=\"#mfb867e74d0\" y=\"331.634375\"/>\n    </g>\n   </g>\n   <g id=\"PolyCollection_2\">\n    <defs>\n     <path d=\"M 54.324062 -105.028528 \nL 54.324062 -105.028528 \nL 56.006475 -105.721952 \nL 57.688887 -106.414918 \nL 59.371299 -107.107427 \nL 61.053711 -107.807542 \nL 62.736123 -108.509695 \nL 64.418535 -109.211325 \nL 66.100947 -109.912403 \nL 67.783359 -110.609175 \nL 69.465771 -111.306828 \nL 71.148183 -112.073358 \nL 72.830595 -112.839342 \nL 74.513007 -113.604754 \nL 76.195419 -114.36962 \nL 77.877831 -115.133886 \nL 79.560243 -115.897524 \nL 81.242655 -116.658519 \nL 82.925068 -117.417343 \nL 84.60748 -118.175458 \nL 86.289892 -118.932892 \nL 87.972304 -119.694536 \nL 89.654716 -120.458356 \nL 91.337128 -121.221421 \nL 93.01954 -121.981703 \nL 94.701952 -122.742791 \nL 96.384364 -123.501593 \nL 98.066776 -124.257725 \nL 99.749188 -125.012956 \nL 101.4316 -125.767281 \nL 103.114012 -126.519868 \nL 104.796424 -127.25446 \nL 106.478836 -127.987821 \nL 108.161248 -128.721058 \nL 109.84366 -129.453168 \nL 111.526073 -130.188052 \nL 113.208485 -130.923696 \nL 114.890897 -131.658139 \nL 116.573309 -132.393595 \nL 118.255721 -133.129569 \nL 119.938133 -133.864166 \nL 121.620545 -134.580314 \nL 123.302957 -135.292204 \nL 124.985369 -136.001866 \nL 126.667781 -136.723209 \nL 128.350193 -137.460698 \nL 130.032605 -138.192668 \nL 131.715017 -138.87723 \nL 133.397429 -139.5456 \nL 135.079841 -140.236683 \nL 136.762253 -140.952405 \nL 138.444666 -141.668058 \nL 140.127078 -142.41812 \nL 141.80949 -143.171842 \nL 143.491902 -143.919712 \nL 145.174314 -144.593256 \nL 146.856726 -145.306219 \nL 148.539138 -146.036319 \nL 150.22155 -146.762892 \nL 151.903962 -147.479489 \nL 153.586374 -148.219132 \nL 155.268786 -148.965257 \nL 156.951198 -149.725014 \nL 158.63361 -150.469638 \nL 160.316022 -151.154774 \nL 161.998434 -151.803325 \nL 163.680846 -152.447389 \nL 165.363258 -153.176909 \nL 167.045671 -153.93509 \nL 168.728083 -154.700375 \nL 170.410495 -155.533377 \nL 172.092907 -156.384609 \nL 173.775319 -157.219121 \nL 175.457731 -158.046127 \nL 177.140143 -158.862936 \nL 178.822555 -159.687678 \nL 180.504967 -160.527678 \nL 182.187379 -161.35649 \nL 183.869791 -162.171331 \nL 185.552203 -163.005277 \nL 187.234615 -163.876776 \nL 188.917027 -164.666769 \nL 190.599439 -165.351091 \nL 192.281851 -166.121573 \nL 193.964264 -166.848923 \nL 195.646676 -167.577904 \nL 197.329088 -168.348926 \nL 199.0115 -169.376729 \nL 200.693912 -170.272805 \nL 202.376324 -171.04866 \nL 204.058736 -171.573941 \nL 205.741148 -172.01086 \nL 207.42356 -172.269852 \nL 209.105972 -172.435478 \nL 210.788384 -172.783669 \nL 212.470796 -173.375375 \nL 214.153208 -173.979322 \nL 215.83562 -174.230918 \nL 217.518032 -174.420942 \nL 219.200444 -174.554885 \nL 220.882856 -174.647407 \nL 222.565269 -174.365712 \nL 224.247681 -174.03739 \nL 225.930093 -173.677937 \nL 227.612505 -173.320664 \nL 229.294917 -173.075311 \nL 230.977329 -172.57377 \nL 232.659741 -171.820852 \nL 234.342153 -171.232604 \nL 236.024565 -170.69893 \nL 237.706977 -170.107001 \nL 239.389389 -169.389737 \nL 241.071801 -168.386569 \nL 242.754213 -167.323841 \nL 244.436625 -166.265102 \nL 246.119037 -165.295898 \nL 247.801449 -164.334614 \nL 249.483861 -163.391617 \nL 251.166274 -162.459073 \nL 252.848686 -161.475121 \nL 254.531098 -160.487637 \nL 256.21351 -159.435462 \nL 257.895922 -158.430869 \nL 259.578334 -157.55244 \nL 261.260746 -156.764095 \nL 262.943158 -155.960308 \nL 264.62557 -155.331294 \nL 266.307982 -154.709075 \nL 267.990394 -154.050833 \nL 269.672806 -153.365773 \nL 271.355218 -152.660831 \nL 273.03763 -151.974262 \nL 274.720042 -151.255869 \nL 276.402454 -150.49449 \nL 278.084867 -149.761933 \nL 279.767279 -149.050118 \nL 281.449691 -148.340475 \nL 283.132103 -147.733376 \nL 284.814515 -147.259883 \nL 286.496927 -146.825977 \nL 288.179339 -146.333063 \nL 289.861751 -145.924776 \nL 291.544163 -145.527682 \nL 293.226575 -145.086421 \nL 294.908987 -144.660271 \nL 296.591399 -144.239657 \nL 298.273811 -143.869085 \nL 299.956223 -143.520872 \nL 301.638635 -143.148069 \nL 303.321047 -142.759561 \nL 305.003459 -142.369931 \nL 306.685872 -141.979176 \nL 308.368284 -141.572413 \nL 310.050696 -141.143775 \nL 311.733108 -140.713525 \nL 313.41552 -140.28111 \nL 315.097932 -139.844382 \nL 316.780344 -139.38492 \nL 318.462756 -138.930933 \nL 320.145168 -138.530469 \nL 321.82758 -138.1279 \nL 323.509992 -137.723203 \nL 325.192404 -137.316361 \nL 326.874816 -136.907352 \nL 328.557228 -136.513105 \nL 330.23964 -136.117388 \nL 331.922052 -135.719465 \nL 333.604465 -135.319317 \nL 335.286877 -134.916908 \nL 336.969289 -134.512217 \nL 338.651701 -134.105223 \nL 340.334113 -133.705843 \nL 342.016525 -133.342069 \nL 343.698937 -132.976401 \nL 345.381349 -132.667674 \nL 347.063761 -132.371158 \nL 348.746173 -132.073562 \nL 350.428585 -131.780038 \nL 352.110997 -131.492961 \nL 353.793409 -131.205072 \nL 355.475821 -130.916346 \nL 357.158233 -130.626807 \nL 358.840645 -130.341036 \nL 360.523057 -130.064754 \nL 362.20547 -129.787623 \nL 363.887882 -129.509614 \nL 365.570294 -129.229634 \nL 367.252706 -128.941296 \nL 368.935118 -128.651774 \nL 370.61753 -128.361106 \nL 372.299942 -128.069268 \nL 373.982354 -127.778851 \nL 375.664766 -127.482455 \nL 377.347178 -127.178077 \nL 379.02959 -126.880222 \nL 380.712002 -126.589764 \nL 382.394414 -126.298391 \nL 384.076826 -126.006115 \nL 385.759238 -125.712908 \nL 387.44165 -125.418807 \nL 389.124063 -125.123761 \nL 389.124063 -125.123761 \nL 389.124063 -125.123761 \nL 387.44165 -125.418807 \nL 385.759238 -125.712908 \nL 384.076826 -126.006115 \nL 382.394414 -126.298391 \nL 380.712002 -126.589764 \nL 379.02959 -126.880222 \nL 377.347178 -127.178077 \nL 375.664766 -127.482455 \nL 373.982354 -127.778851 \nL 372.299942 -128.069268 \nL 370.61753 -128.361106 \nL 368.935118 -128.651774 \nL 367.252706 -128.941296 \nL 365.570294 -129.229634 \nL 363.887882 -129.509614 \nL 362.20547 -129.787623 \nL 360.523057 -130.064754 \nL 358.840645 -130.341036 \nL 357.158233 -130.626807 \nL 355.475821 -130.916346 \nL 353.793409 -131.205072 \nL 352.110997 -131.492961 \nL 350.428585 -131.780038 \nL 348.746173 -132.073562 \nL 347.063761 -132.371158 \nL 345.381349 -132.667674 \nL 343.698937 -132.976401 \nL 342.016525 -133.342069 \nL 340.334113 -133.705843 \nL 338.651701 -134.105223 \nL 336.969289 -134.512217 \nL 335.286877 -134.916908 \nL 333.604465 -135.319317 \nL 331.922052 -135.719465 \nL 330.23964 -136.117388 \nL 328.557228 -136.513105 \nL 326.874816 -136.907352 \nL 325.192404 -137.316361 \nL 323.509992 -137.723203 \nL 321.82758 -138.1279 \nL 320.145168 -138.530469 \nL 318.462756 -138.930933 \nL 316.780344 -139.38492 \nL 315.097932 -139.844382 \nL 313.41552 -140.28111 \nL 311.733108 -140.713525 \nL 310.050696 -141.143775 \nL 308.368284 -141.572413 \nL 306.685872 -141.979176 \nL 305.003459 -142.369931 \nL 303.321047 -142.759561 \nL 301.638635 -143.148069 \nL 299.956223 -143.520872 \nL 298.273811 -143.869085 \nL 296.591399 -144.239657 \nL 294.908987 -144.660271 \nL 293.226575 -145.086421 \nL 291.544163 -145.527682 \nL 289.861751 -145.924776 \nL 288.179339 -146.333063 \nL 286.496927 -146.825977 \nL 284.814515 -147.259883 \nL 283.132103 -147.733376 \nL 281.449691 -148.340475 \nL 279.767279 -149.050118 \nL 278.084867 -149.761933 \nL 276.402454 -150.49449 \nL 274.720042 -151.255869 \nL 273.03763 -151.974262 \nL 271.355218 -152.660831 \nL 269.672806 -153.365773 \nL 267.990394 -154.050833 \nL 266.307982 -154.709075 \nL 264.62557 -155.331294 \nL 262.943158 -155.960308 \nL 261.260746 -156.764095 \nL 259.578334 -157.55244 \nL 257.895922 -158.430869 \nL 256.21351 -159.435462 \nL 254.531098 -160.487637 \nL 252.848686 -161.475121 \nL 251.166274 -162.459073 \nL 249.483861 -163.391617 \nL 247.801449 -164.334614 \nL 246.119037 -165.295898 \nL 244.436625 -166.265102 \nL 242.754213 -167.323841 \nL 241.071801 -168.386569 \nL 239.389389 -169.389737 \nL 237.706977 -170.107001 \nL 236.024565 -170.69893 \nL 234.342153 -171.232604 \nL 232.659741 -171.820852 \nL 230.977329 -172.57377 \nL 229.294917 -173.075311 \nL 227.612505 -173.320664 \nL 225.930093 -173.677937 \nL 224.247681 -174.03739 \nL 222.565269 -174.365712 \nL 220.882856 -174.647407 \nL 219.200444 -174.554885 \nL 217.518032 -174.420942 \nL 215.83562 -174.230918 \nL 214.153208 -173.979322 \nL 212.470796 -173.375375 \nL 210.788384 -172.783669 \nL 209.105972 -172.435478 \nL 207.42356 -172.269852 \nL 205.741148 -172.01086 \nL 204.058736 -171.573941 \nL 202.376324 -171.04866 \nL 200.693912 -170.272805 \nL 199.0115 -169.376729 \nL 197.329088 -168.348926 \nL 195.646676 -167.577904 \nL 193.964264 -166.848923 \nL 192.281851 -166.121573 \nL 190.599439 -165.351091 \nL 188.917027 -164.666769 \nL 187.234615 -163.876776 \nL 185.552203 -163.005277 \nL 183.869791 -162.171331 \nL 182.187379 -161.35649 \nL 180.504967 -160.527678 \nL 178.822555 -159.687678 \nL 177.140143 -158.862936 \nL 175.457731 -158.046127 \nL 173.775319 -157.219121 \nL 172.092907 -156.384609 \nL 170.410495 -155.533377 \nL 168.728083 -154.700375 \nL 167.045671 -153.93509 \nL 165.363258 -153.176909 \nL 163.680846 -152.447389 \nL 161.998434 -151.803325 \nL 160.316022 -151.154774 \nL 158.63361 -150.469638 \nL 156.951198 -149.725014 \nL 155.268786 -148.965257 \nL 153.586374 -148.219132 \nL 151.903962 -147.479489 \nL 150.22155 -146.762892 \nL 148.539138 -146.036319 \nL 146.856726 -145.306219 \nL 145.174314 -144.593256 \nL 143.491902 -143.919712 \nL 141.80949 -143.171842 \nL 140.127078 -142.41812 \nL 138.444666 -141.668058 \nL 136.762253 -140.952405 \nL 135.079841 -140.236683 \nL 133.397429 -139.5456 \nL 131.715017 -138.87723 \nL 130.032605 -138.192668 \nL 128.350193 -137.460698 \nL 126.667781 -136.723209 \nL 124.985369 -136.001866 \nL 123.302957 -135.292204 \nL 121.620545 -134.580314 \nL 119.938133 -133.864166 \nL 118.255721 -133.129569 \nL 116.573309 -132.393595 \nL 114.890897 -131.658139 \nL 113.208485 -130.923696 \nL 111.526073 -130.188052 \nL 109.84366 -129.453168 \nL 108.161248 -128.721058 \nL 106.478836 -127.987821 \nL 104.796424 -127.25446 \nL 103.114012 -126.519868 \nL 101.4316 -125.767281 \nL 99.749188 -125.012956 \nL 98.066776 -124.257725 \nL 96.384364 -123.501593 \nL 94.701952 -122.742791 \nL 93.01954 -121.981703 \nL 91.337128 -121.221421 \nL 89.654716 -120.458356 \nL 87.972304 -119.694536 \nL 86.289892 -118.932892 \nL 84.60748 -118.175458 \nL 82.925068 -117.417343 \nL 81.242655 -116.658519 \nL 79.560243 -115.897524 \nL 77.877831 -115.133886 \nL 76.195419 -114.36962 \nL 74.513007 -113.604754 \nL 72.830595 -112.839342 \nL 71.148183 -112.073358 \nL 69.465771 -111.306828 \nL 67.783359 -110.609175 \nL 66.100947 -109.912403 \nL 64.418535 -109.211325 \nL 62.736123 -108.509695 \nL 61.053711 -107.807542 \nL 59.371299 -107.107427 \nL 57.688887 -106.414918 \nL 56.006475 -105.721952 \nL 54.324062 -105.028528 \nz\n\" id=\"m648e50c899\" style=\"stroke:#1f77b4;stroke-opacity:0.3;\"/>\n    </defs>\n    <g clip-path=\"url(#pe0488430e2)\">\n     <use style=\"fill:#1f77b4;fill-opacity:0.3;stroke:#1f77b4;stroke-opacity:0.3;\" x=\"0\" xlink:href=\"#m648e50c899\" y=\"331.634375\"/>\n    </g>\n   </g>\n   <g id=\"PolyCollection_3\">\n    <defs>\n     <path d=\"M 54.324062 -105.871215 \nL 54.324062 -105.028528 \nL 56.006475 -105.721952 \nL 57.688887 -106.414918 \nL 59.371299 -107.107427 \nL 61.053711 -107.807542 \nL 62.736123 -108.509695 \nL 64.418535 -109.211325 \nL 66.100947 -109.912403 \nL 67.783359 -110.609175 \nL 69.465771 -111.306828 \nL 71.148183 -112.073358 \nL 72.830595 -112.839342 \nL 74.513007 -113.604754 \nL 76.195419 -114.36962 \nL 77.877831 -115.133886 \nL 79.560243 -115.897524 \nL 81.242655 -116.658519 \nL 82.925068 -117.417343 \nL 84.60748 -118.175458 \nL 86.289892 -118.932892 \nL 87.972304 -119.694536 \nL 89.654716 -120.458356 \nL 91.337128 -121.221421 \nL 93.01954 -121.981703 \nL 94.701952 -122.742791 \nL 96.384364 -123.501593 \nL 98.066776 -124.257725 \nL 99.749188 -125.012956 \nL 101.4316 -125.767281 \nL 103.114012 -126.519868 \nL 104.796424 -127.25446 \nL 106.478836 -127.987821 \nL 108.161248 -128.721058 \nL 109.84366 -129.453168 \nL 111.526073 -130.188052 \nL 113.208485 -130.923696 \nL 114.890897 -131.658139 \nL 116.573309 -132.393595 \nL 118.255721 -133.129569 \nL 119.938133 -133.864166 \nL 121.620545 -134.580314 \nL 123.302957 -135.292204 \nL 124.985369 -136.001866 \nL 126.667781 -136.723209 \nL 128.350193 -137.460698 \nL 130.032605 -138.192668 \nL 131.715017 -138.87723 \nL 133.397429 -139.5456 \nL 135.079841 -140.236683 \nL 136.762253 -140.952405 \nL 138.444666 -141.668058 \nL 140.127078 -142.41812 \nL 141.80949 -143.171842 \nL 143.491902 -143.919712 \nL 145.174314 -144.593256 \nL 146.856726 -145.306219 \nL 148.539138 -146.036319 \nL 150.22155 -146.762892 \nL 151.903962 -147.479489 \nL 153.586374 -148.219132 \nL 155.268786 -148.965257 \nL 156.951198 -149.725014 \nL 158.63361 -150.469638 \nL 160.316022 -151.154774 \nL 161.998434 -151.803325 \nL 163.680846 -152.447389 \nL 165.363258 -153.176909 \nL 167.045671 -153.93509 \nL 168.728083 -154.700375 \nL 170.410495 -155.533377 \nL 172.092907 -156.384609 \nL 173.775319 -157.219121 \nL 175.457731 -158.046127 \nL 177.140143 -158.862936 \nL 178.822555 -159.687678 \nL 180.504967 -160.527678 \nL 182.187379 -161.35649 \nL 183.869791 -162.171331 \nL 185.552203 -163.005277 \nL 187.234615 -163.876776 \nL 188.917027 -164.666769 \nL 190.599439 -165.351091 \nL 192.281851 -166.121573 \nL 193.964264 -166.848923 \nL 195.646676 -167.577904 \nL 197.329088 -168.348926 \nL 199.0115 -169.376729 \nL 200.693912 -170.272805 \nL 202.376324 -171.04866 \nL 204.058736 -171.573941 \nL 205.741148 -172.01086 \nL 207.42356 -172.269852 \nL 209.105972 -172.435478 \nL 210.788384 -172.783669 \nL 212.470796 -173.375375 \nL 214.153208 -173.979322 \nL 215.83562 -174.230918 \nL 217.518032 -174.420942 \nL 219.200444 -174.554885 \nL 220.882856 -174.647407 \nL 222.565269 -174.365712 \nL 224.247681 -174.03739 \nL 225.930093 -173.677937 \nL 227.612505 -173.320664 \nL 229.294917 -173.075311 \nL 230.977329 -172.57377 \nL 232.659741 -171.820852 \nL 234.342153 -171.232604 \nL 236.024565 -170.69893 \nL 237.706977 -170.107001 \nL 239.389389 -169.389737 \nL 241.071801 -168.386569 \nL 242.754213 -167.323841 \nL 244.436625 -166.265102 \nL 246.119037 -165.295898 \nL 247.801449 -164.334614 \nL 249.483861 -163.391617 \nL 251.166274 -162.459073 \nL 252.848686 -161.475121 \nL 254.531098 -160.487637 \nL 256.21351 -159.435462 \nL 257.895922 -158.430869 \nL 259.578334 -157.55244 \nL 261.260746 -156.764095 \nL 262.943158 -155.960308 \nL 264.62557 -155.331294 \nL 266.307982 -154.709075 \nL 267.990394 -154.050833 \nL 269.672806 -153.365773 \nL 271.355218 -152.660831 \nL 273.03763 -151.974262 \nL 274.720042 -151.255869 \nL 276.402454 -150.49449 \nL 278.084867 -149.761933 \nL 279.767279 -149.050118 \nL 281.449691 -148.340475 \nL 283.132103 -147.733376 \nL 284.814515 -147.259883 \nL 286.496927 -146.825977 \nL 288.179339 -146.333063 \nL 289.861751 -145.924776 \nL 291.544163 -145.527682 \nL 293.226575 -145.086421 \nL 294.908987 -144.660271 \nL 296.591399 -144.239657 \nL 298.273811 -143.869085 \nL 299.956223 -143.520872 \nL 301.638635 -143.148069 \nL 303.321047 -142.759561 \nL 305.003459 -142.369931 \nL 306.685872 -141.979176 \nL 308.368284 -141.572413 \nL 310.050696 -141.143775 \nL 311.733108 -140.713525 \nL 313.41552 -140.28111 \nL 315.097932 -139.844382 \nL 316.780344 -139.38492 \nL 318.462756 -138.930933 \nL 320.145168 -138.530469 \nL 321.82758 -138.1279 \nL 323.509992 -137.723203 \nL 325.192404 -137.316361 \nL 326.874816 -136.907352 \nL 328.557228 -136.513105 \nL 330.23964 -136.117388 \nL 331.922052 -135.719465 \nL 333.604465 -135.319317 \nL 335.286877 -134.916908 \nL 336.969289 -134.512217 \nL 338.651701 -134.105223 \nL 340.334113 -133.705843 \nL 342.016525 -133.342069 \nL 343.698937 -132.976401 \nL 345.381349 -132.667674 \nL 347.063761 -132.371158 \nL 348.746173 -132.073562 \nL 350.428585 -131.780038 \nL 352.110997 -131.492961 \nL 353.793409 -131.205072 \nL 355.475821 -130.916346 \nL 357.158233 -130.626807 \nL 358.840645 -130.341036 \nL 360.523057 -130.064754 \nL 362.20547 -129.787623 \nL 363.887882 -129.509614 \nL 365.570294 -129.229634 \nL 367.252706 -128.941296 \nL 368.935118 -128.651774 \nL 370.61753 -128.361106 \nL 372.299942 -128.069268 \nL 373.982354 -127.778851 \nL 375.664766 -127.482455 \nL 377.347178 -127.178077 \nL 379.02959 -126.880222 \nL 380.712002 -126.589764 \nL 382.394414 -126.298391 \nL 384.076826 -126.006115 \nL 385.759238 -125.712908 \nL 387.44165 -125.418807 \nL 389.124063 -125.123761 \nL 389.124063 -179.18944 \nL 389.124063 -179.18944 \nL 387.44165 -179.167768 \nL 385.759238 -179.147019 \nL 384.076826 -179.127197 \nL 382.394414 -179.108307 \nL 380.712002 -179.090297 \nL 379.02959 -179.073213 \nL 377.347178 -179.04903 \nL 375.664766 -179.018626 \nL 373.982354 -178.976075 \nL 372.299942 -178.919445 \nL 370.61753 -178.865678 \nL 368.935118 -178.813082 \nL 367.252706 -178.761658 \nL 365.570294 -178.711375 \nL 363.887882 -178.691942 \nL 362.20547 -178.677912 \nL 360.523057 -178.664699 \nL 358.840645 -178.652379 \nL 357.158233 -178.653227 \nL 355.475821 -178.660376 \nL 353.793409 -178.668385 \nL 352.110997 -178.677212 \nL 350.428585 -178.68685 \nL 348.746173 -178.678117 \nL 347.063761 -178.65687 \nL 345.381349 -178.636724 \nL 343.698937 -178.603727 \nL 342.016525 -178.554832 \nL 340.334113 -178.507812 \nL 338.651701 -178.461112 \nL 336.969289 -178.416271 \nL 335.286877 -178.373747 \nL 333.604465 -178.333497 \nL 331.922052 -178.295498 \nL 330.23964 -178.25972 \nL 328.557228 -178.226166 \nL 326.874816 -178.195406 \nL 325.192404 -178.181177 \nL 323.509992 -178.169115 \nL 321.82758 -178.159206 \nL 320.145168 -178.151424 \nL 318.462756 -178.145737 \nL 316.780344 -178.196847 \nL 315.097932 -178.265186 \nL 313.41552 -178.350232 \nL 311.733108 -178.430968 \nL 310.050696 -178.512188 \nL 308.368284 -178.595012 \nL 306.685872 -178.699993 \nL 305.003459 -178.803021 \nL 303.321047 -178.907177 \nL 301.638635 -179.012446 \nL 299.956223 -179.14834 \nL 298.273811 -179.333331 \nL 296.591399 -179.54216 \nL 294.908987 -179.830121 \nL 293.226575 -180.120424 \nL 291.544163 -180.399956 \nL 289.861751 -180.712325 \nL 288.179339 -181.030251 \nL 286.496927 -181.422632 \nL 284.814515 -181.824641 \nL 283.132103 -182.217185 \nL 281.449691 -182.534809 \nL 279.767279 -183.029 \nL 278.084867 -183.503394 \nL 276.402454 -183.945244 \nL 274.720042 -184.391959 \nL 273.03763 -184.82476 \nL 271.355218 -185.30118 \nL 269.672806 -185.782299 \nL 267.990394 -186.219256 \nL 266.307982 -186.533667 \nL 264.62557 -186.784661 \nL 262.943158 -186.947688 \nL 261.260746 -187.165605 \nL 259.578334 -187.404934 \nL 257.895922 -187.672625 \nL 256.21351 -188.048851 \nL 254.531098 -188.446705 \nL 252.848686 -188.77956 \nL 251.166274 -189.106723 \nL 249.483861 -189.440137 \nL 247.801449 -189.803671 \nL 246.119037 -190.2448 \nL 244.436625 -190.689297 \nL 242.754213 -191.118527 \nL 241.071801 -191.581568 \nL 239.389389 -192.005825 \nL 237.706977 -192.145494 \nL 236.024565 -192.249786 \nL 234.342153 -192.318913 \nL 232.659741 -192.523086 \nL 230.977329 -193.027267 \nL 229.294917 -193.334193 \nL 227.612505 -193.437747 \nL 225.930093 -193.618965 \nL 224.247681 -193.925852 \nL 222.565269 -194.090864 \nL 220.882856 -194.015742 \nL 219.200444 -193.6251 \nL 217.518032 -193.013284 \nL 215.83562 -192.437638 \nL 214.153208 -191.810701 \nL 212.470796 -190.835642 \nL 210.788384 -189.965519 \nL 209.105972 -189.277248 \nL 207.42356 -188.792216 \nL 205.741148 -188.226831 \nL 204.058736 -187.435088 \nL 202.376324 -186.597242 \nL 200.693912 -185.319362 \nL 199.0115 -183.83808 \nL 197.329088 -182.241203 \nL 195.646676 -180.881778 \nL 193.964264 -179.536143 \nL 192.281851 -178.25352 \nL 190.599439 -177.002032 \nL 188.917027 -175.870806 \nL 187.234615 -174.673084 \nL 185.552203 -173.430623 \nL 183.869791 -172.27814 \nL 182.187379 -171.170495 \nL 180.504967 -170.050534 \nL 178.822555 -168.922883 \nL 177.140143 -167.789102 \nL 175.457731 -166.635219 \nL 173.775319 -165.449232 \nL 172.092907 -164.270745 \nL 170.410495 -163.078915 \nL 168.728083 -161.953456 \nL 167.045671 -160.933788 \nL 165.363258 -159.943093 \nL 163.680846 -158.98285 \nL 161.998434 -158.096653 \nL 160.316022 -157.214941 \nL 158.63361 -156.311473 \nL 156.951198 -155.366619 \nL 155.268786 -154.428488 \nL 153.586374 -153.524079 \nL 151.903962 -152.6351 \nL 150.22155 -151.781273 \nL 148.539138 -150.913464 \nL 146.856726 -150.045038 \nL 145.174314 -149.193618 \nL 143.491902 -148.379487 \nL 141.80949 -147.485575 \nL 140.127078 -146.589606 \nL 138.444666 -145.707855 \nL 136.762253 -144.863088 \nL 135.079841 -144.022291 \nL 133.397429 -143.205502 \nL 131.715017 -142.412309 \nL 130.032605 -141.616092 \nL 128.350193 -140.782985 \nL 126.667781 -139.946212 \nL 124.985369 -139.125895 \nL 123.302957 -138.318624 \nL 121.620545 -137.511785 \nL 119.938133 -136.703763 \nL 118.255721 -135.880693 \nL 116.573309 -135.059006 \nL 114.890897 -134.241213 \nL 113.208485 -133.427843 \nL 111.526073 -132.615678 \nL 109.84366 -131.806405 \nL 108.161248 -131.001767 \nL 106.478836 -130.198238 \nL 104.796424 -129.396706 \nL 103.114012 -128.596234 \nL 101.4316 -127.781159 \nL 99.749188 -126.966218 \nL 98.066776 -126.152195 \nL 96.384364 -125.339039 \nL 94.701952 -124.525168 \nL 93.01954 -123.710844 \nL 91.337128 -122.899013 \nL 89.654716 -122.085929 \nL 87.972304 -121.273577 \nL 86.289892 -120.464436 \nL 84.60748 -119.66026 \nL 82.925068 -118.856831 \nL 81.242655 -118.054067 \nL 79.560243 -117.250745 \nL 77.877831 -116.446404 \nL 76.195419 -115.642661 \nL 74.513007 -114.839495 \nL 72.830595 -114.036953 \nL 71.148183 -113.234938 \nL 69.465771 -112.433469 \nL 67.783359 -111.701339 \nL 66.100947 -110.969893 \nL 64.418535 -110.234844 \nL 62.736123 -109.500335 \nL 61.053711 -108.766371 \nL 59.371299 -108.035513 \nL 57.688887 -107.313429 \nL 56.006475 -106.591835 \nL 54.324062 -105.871215 \nz\n\" id=\"m46eb457028\" style=\"stroke:#ff7f0e;stroke-opacity:0.4;\"/>\n    </defs>\n    <g clip-path=\"url(#pe0488430e2)\">\n     <use style=\"fill:#ff7f0e;fill-opacity:0.4;stroke:#ff7f0e;stroke-opacity:0.4;\" x=\"0\" xlink:href=\"#m46eb457028\" y=\"331.634375\"/>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#pe0488430e2)\" d=\"M 87.804062 284.4 \nL 87.804062 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m56ea45adad\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"87.804062\" xlink:href=\"#m56ea45adad\" y=\"284.4\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- −4 -->\n      <defs>\n       <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-8722\"/>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(77.484531 302.037812)scale(0.14 -0.14)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#pe0488430e2)\" d=\"M 154.764062 284.4 \nL 154.764062 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"154.764062\" xlink:href=\"#m56ea45adad\" y=\"284.4\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- −2 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(144.444531 302.037812)scale(0.14 -0.14)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#pe0488430e2)\" d=\"M 221.724063 284.4 \nL 221.724063 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"221.724063\" xlink:href=\"#m56ea45adad\" y=\"284.4\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(217.270312 302.037812)scale(0.14 -0.14)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#pe0488430e2)\" d=\"M 288.684062 284.4 \nL 288.684062 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"288.684062\" xlink:href=\"#m56ea45adad\" y=\"284.4\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 2 -->\n      <g transform=\"translate(284.230312 302.037812)scale(0.14 -0.14)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#pe0488430e2)\" d=\"M 355.644063 284.4 \nL 355.644063 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"355.644063\" xlink:href=\"#m56ea45adad\" y=\"284.4\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 4 -->\n      <g transform=\"translate(351.190313 302.037812)scale(0.14 -0.14)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- $x$ -->\n     <defs>\n      <path d=\"M 60.015625 54.6875 \nL 34.90625 27.875 \nL 50.296875 0 \nL 39.984375 0 \nL 28.421875 21.6875 \nL 8.296875 0 \nL -2.59375 0 \nL 24.3125 28.8125 \nL 10.015625 54.6875 \nL 20.3125 54.6875 \nL 30.8125 34.90625 \nL 49.125 54.6875 \nz\n\" id=\"DejaVuSans-Oblique-120\"/>\n     </defs>\n     <g transform=\"translate(216.924062 321.106875)scale(0.16 -0.16)\">\n      <use transform=\"translate(0 0.3125)\" xlink:href=\"#DejaVuSans-Oblique-120\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#pe0488430e2)\" d=\"M 54.324063 261.3 \nL 389.124063 261.3 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m5c002686b7\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"54.324063\" xlink:href=\"#m5c002686b7\" y=\"261.3\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- −4 -->\n      <g transform=\"translate(26.685 266.618906)scale(0.14 -0.14)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#pe0488430e2)\" d=\"M 54.324063 215.1 \nL 389.124063 215.1 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"54.324063\" xlink:href=\"#m5c002686b7\" y=\"215.1\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- −2 -->\n      <g transform=\"translate(26.685 220.418906)scale(0.14 -0.14)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#pe0488430e2)\" d=\"M 54.324063 168.9 \nL 389.124063 168.9 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"54.324063\" xlink:href=\"#m5c002686b7\" y=\"168.9\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0 -->\n      <g transform=\"translate(38.416563 174.218906)scale(0.14 -0.14)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#pe0488430e2)\" d=\"M 54.324063 122.7 \nL 389.124063 122.7 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"54.324063\" xlink:href=\"#m5c002686b7\" y=\"122.7\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 2 -->\n      <g transform=\"translate(38.416563 128.018906)scale(0.14 -0.14)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#pe0488430e2)\" d=\"M 54.324063 76.5 \nL 389.124063 76.5 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"54.324063\" xlink:href=\"#m5c002686b7\" y=\"76.5\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 4 -->\n      <g transform=\"translate(38.416563 81.818906)scale(0.14 -0.14)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#pe0488430e2)\" d=\"M 54.324063 30.3 \nL 389.124063 30.3 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"54.324063\" xlink:href=\"#m5c002686b7\" y=\"30.3\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 6 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(38.416563 35.618906)scale(0.14 -0.14)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_13\">\n     <!-- $y$ -->\n     <defs>\n      <path d=\"M 24.8125 -5.078125 \nQ 18.5625 -15.578125 14.625 -18.1875 \nQ 10.6875 -20.796875 4.59375 -20.796875 \nL -2.484375 -20.796875 \nL -0.984375 -13.28125 \nL 4.203125 -13.28125 \nQ 7.953125 -13.28125 10.59375 -11.234375 \nQ 13.234375 -9.1875 16.5 -3.21875 \nL 19.28125 2 \nL 7.171875 54.6875 \nL 16.703125 54.6875 \nL 25.78125 12.796875 \nL 50.875 54.6875 \nL 60.296875 54.6875 \nz\n\" id=\"DejaVuSans-Oblique-121\"/>\n     </defs>\n     <g transform=\"translate(19.325 150.6)rotate(-90)scale(0.16 -0.16)\">\n      <use transform=\"translate(0 0.3125)\" xlink:href=\"#DejaVuSans-Oblique-121\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_23\">\n    <path clip-path=\"url(#pe0488430e2)\" d=\"M 54.324062 226.184503 \nL 69.465771 219.764226 \nL 119.938133 196.35041 \nL 140.127078 187.130512 \nL 145.174314 184.740938 \nL 155.268786 179.937502 \nL 160.316022 177.449518 \nL 165.363258 175.074374 \nL 170.410495 172.328229 \nL 178.822555 167.329095 \nL 195.646676 157.404534 \nL 197.329088 156.339311 \nL 200.693912 153.838292 \nL 202.376324 152.811424 \nL 205.741148 151.51553 \nL 210.788384 150.259781 \nL 214.153208 148.739364 \nL 217.518032 147.917261 \nL 219.200444 147.544382 \nL 220.882856 147.302801 \nL 222.565269 147.406087 \nL 229.294917 148.429622 \nL 230.977329 148.833855 \nL 232.659741 149.462405 \nL 236.024565 150.160017 \nL 239.389389 150.936593 \nL 251.166274 155.851477 \nL 261.260746 159.669525 \nL 264.62557 160.576398 \nL 267.990394 161.499331 \nL 276.402454 164.414508 \nL 283.132103 166.659095 \nL 289.861751 168.315824 \nL 298.273811 170.033167 \nL 311.733108 172.062129 \nL 321.82758 173.490823 \nL 336.969289 175.170131 \nL 345.381349 175.982176 \nL 389.124063 179.477775 \nL 389.124063 179.477775 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 54.324063 284.4 \nL 54.324063 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 389.124063 284.4 \nL 389.124063 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 54.324062 284.4 \nL 389.124063 284.4 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 54.324062 7.2 \nL 389.124063 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pe0488430e2\">\n   <rect height=\"277.2\" width=\"334.8\" x=\"54.324063\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =  164.548\n",
            "Epoch:   50, Train loss =  124.741\n",
            "Epoch:  100, Train loss =  183.712\n",
            "Epoch:  150, Train loss =  215.045\n",
            "Epoch:  200, Train loss =  151.875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =  165.675\n",
            "Epoch:   50, Train loss =  209.055\n",
            "Epoch:  100, Train loss =  314.166\n",
            "Epoch:  150, Train loss =  442.780\n",
            "Epoch:  200, Train loss =  283.678\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =  116.373\n",
            "Epoch:   50, Train loss =  218.168\n",
            "Epoch:  100, Train loss =  111.178\n",
            "Epoch:  150, Train loss =   95.163\n",
            "Epoch:  200, Train loss =   48.433\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =  218.827\n",
            "Epoch:   50, Train loss =  333.548\n",
            "Epoch:  100, Train loss =   73.062\n",
            "Epoch:  150, Train loss =   91.845\n",
            "Epoch:  200, Train loss =  448.712\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =  153.697\n",
            "Epoch:   50, Train loss =  141.025\n",
            "Epoch:  100, Train loss =   72.338\n",
            "Epoch:  150, Train loss =   44.349\n",
            "Epoch:  200, Train loss =   60.680\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =  148.721\n",
            "Epoch:   50, Train loss =  294.100\n",
            "Epoch:  100, Train loss =  430.687\n",
            "Epoch:  150, Train loss =   64.078\n",
            "Epoch:  200, Train loss =  260.575\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =  167.848\n",
            "Epoch:   50, Train loss =   25.805\n",
            "Epoch:  100, Train loss =  -44.737\n",
            "Epoch:  150, Train loss =  162.412\n",
            "Epoch:  200, Train loss =   13.456\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =  156.826\n",
            "Epoch:   50, Train loss =  115.051\n",
            "Epoch:  100, Train loss =  193.592\n",
            "Epoch:  150, Train loss =  105.535\n",
            "Epoch:  200, Train loss =   57.134\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =  120.745\n",
            "Epoch:   50, Train loss =  -28.480\n",
            "Epoch:  100, Train loss =  267.719\n",
            "Epoch:  150, Train loss =   67.663\n",
            "Epoch:  200, Train loss =   83.294\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =  117.330\n",
            "Epoch:   50, Train loss =   85.282\n",
            "Epoch:  100, Train loss =   26.045\n",
            "Epoch:  150, Train loss =  216.380\n",
            "Epoch:  200, Train loss =   81.584\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =  100.169\n",
            "Epoch:   50, Train loss =  152.870\n",
            "Epoch:  100, Train loss =   77.591\n",
            "Epoch:  150, Train loss =  170.030\n",
            "Epoch:  200, Train loss =  165.796\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =   98.186\n",
            "Epoch:   50, Train loss =  176.815\n",
            "Epoch:  100, Train loss =   22.511\n",
            "Epoch:  150, Train loss =  533.579\n",
            "Epoch:  200, Train loss =  -22.970\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =  105.150\n",
            "Epoch:   50, Train loss =   85.702\n",
            "Epoch:  100, Train loss =  107.630\n",
            "Epoch:  150, Train loss =   45.773\n",
            "Epoch:  200, Train loss =   87.892\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =  100.925\n",
            "Epoch:   50, Train loss =  149.584\n",
            "Epoch:  100, Train loss =   55.350\n",
            "Epoch:  150, Train loss =  127.717\n",
            "Epoch:  200, Train loss =   49.019\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =   97.045\n",
            "Epoch:   50, Train loss =  502.182\n",
            "Epoch:  100, Train loss =   49.919\n",
            "Epoch:  150, Train loss =  160.623\n",
            "Epoch:  200, Train loss =   93.277\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =  109.137\n",
            "Epoch:   50, Train loss =  -49.926\n",
            "Epoch:  100, Train loss =   52.022\n",
            "Epoch:  150, Train loss =   92.863\n",
            "Epoch:  200, Train loss =   -5.414\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =  111.889\n",
            "Epoch:   50, Train loss =   37.983\n",
            "Epoch:  100, Train loss =  103.199\n",
            "Epoch:  150, Train loss =  128.565\n",
            "Epoch:  200, Train loss =   88.555\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =   79.735\n",
            "Epoch:   50, Train loss =  187.384\n",
            "Epoch:  100, Train loss =  158.093\n",
            "Epoch:  150, Train loss =  160.939\n",
            "Epoch:  200, Train loss =  578.069\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning:A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            " /usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning:Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            " /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning:Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0, Train loss =  103.033\n",
            "Epoch:   50, Train loss =   23.315\n",
            "Epoch:  100, Train loss =   17.348\n",
            "Epoch:  150, Train loss =   11.662\n",
            "Epoch:  200, Train loss =  188.187\n",
            "[0.7098755736131469, 0.6106938775519146, 0.45725374406675473, 0.503049929223701, 0.8613716491948362, 0.5832311617042192, 0.6746940409702975, 0.4123022810302746, 0.5080119718179843, 0.35774874778982074, 0.48818289042023594, 0.4490981458731713, 0.5138526950964792, 0.5795283605983869, 0.6866188610684733, 0.6760814049884148, 0.445814275568945, 0.4881252197008813, 0.7897166294559346, 0.491894318689955] 0.5109323334572318\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}